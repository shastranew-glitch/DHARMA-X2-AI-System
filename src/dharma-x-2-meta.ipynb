{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13066251,"sourceType":"datasetVersion","datasetId":8267434},{"sourceId":13122511,"sourceType":"datasetVersion","datasetId":8225736}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-19T10:13:20.087132Z","iopub.execute_input":"2025-09-19T10:13:20.087403Z","iopub.status.idle":"2025-09-19T10:13:20.524678Z","shell.execute_reply.started":"2025-09-19T10:13:20.087375Z","shell.execute_reply":"2025-09-19T10:13:20.523703Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --quiet pytorch-forecasting pytorch-lightning --extra-index-url https://download.pytorch.org/whl/cpu\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-19T10:13:20.52697Z","iopub.execute_input":"2025-09-19T10:13:20.527354Z","iopub.status.idle":"2025-09-19T10:14:59.15581Z","shell.execute_reply.started":"2025-09-19T10:13:20.527323Z","shell.execute_reply":"2025-09-19T10:14:59.154185Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install networkx\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-19T10:14:59.157798Z","iopub.execute_input":"2025-09-19T10:14:59.158241Z","iopub.status.idle":"2025-09-19T10:15:03.127015Z","shell.execute_reply.started":"2025-09-19T10:14:59.158187Z","shell.execute_reply":"2025-09-19T10:15:03.126059Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n# â”‚  APEX Expert System - Phase 1 + Phase 2 Co-Existence Implementation    â”‚\n# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nimport secrets                                     # CSPRNG - replaces random / np.random\nfrom random import SystemRandom                    # convenient alias\n_secure = SystemRandom()\n\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport pytorch_lightning as pl\nfrom pytorch_forecasting import TemporalFusionTransformer, TimeSeriesDataSet\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.cluster import KMeans\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom collections import Counter\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” Utility: secure sampling helpers â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\ndef secure_randint(a: int, b: int) -> int:\n    \"\"\"Cryptographically secure randint inclusive [a, b].\"\"\"\n    return secrets.randbelow(b - a + 1) + a\n\ndef secure_choice(seq):\n    \"\"\"Secure single choice from non-empty seq.\"\"\"\n    idx = secrets.randbelow(len(seq))\n    return seq[idx]\n\ndef secure_sample(seq, k):\n    \"\"\"Secure k-sample without replacement (k â‰¤ len(seq)).\"\"\"\n    return _secure.sample(seq, k)\n\n# Data loading and preparation (unchanged)\ndata = pd.read_excel(\"/kaggle/input/draw-41to44/draw_Updated.xlsx\")\ndata = data.rename(columns={\n    'N1': 'Number1', 'N2': 'Number2', 'N3': 'Number3',\n    'N4': 'Number4', 'N5': 'Number5', 'N6': 'Number6'\n})\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# PHASE 1 EXPERTS (PROVEN & WORKING - KEEP ALL)\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nclass PerformanceTracker:\n    def __init__(self):\n        self.history = {}\n    def update(self, expert, error):\n        self.history.setdefault(expert, []).append(error)\n    def score(self, expert):\n        err = self.history.get(expert, [])\n        return max(0.5, 1 - np.mean(err)) if err else 0.5\n\nperf = PerformanceTracker()\n\ndef ensure_valid(pred) -> list:\n    \"\"\"Clipâ†’uniqueâ†’pad to six numbers using CSPRNG.\"\"\"\n    p = [int(np.clip(round(x), 1, 39)) for x in pred]\n    seen, out = set(), []\n    for x in p:\n        if x not in seen:\n            seen.add(x); out.append(x)\n    while len(out) < 6:\n        rem = list(set(range(1, 40)) - set(out))\n        out.append(secure_choice(rem))\n    return sorted(out)\n\ndef fallback(window) -> list:\n    \"\"\"Top-frequency fallback with secure padding.\"\"\"\n    nums = sum((window[f'Number{i}'].tolist() for i in range(1, 7)), [])\n    freq = Counter(nums)\n    top6 = [n for n, _ in freq.most_common(6)]\n    if len(top6) < 6:\n        remainder = list(set(range(1, 40)) - set(top6))\n        top6 += remainder[: 6 - len(top6)]\n    return sorted(top6)\n\n# Phase 1 Expert Functions (Your Current Working System)\ndef A1_ARIMA_expert(window):\n    \"\"\"Advanced ARIMA with Dynamic Order Selection and Fallback Strategies\"\"\"\n    window_size = len(window)\n    score = perf.score('A1_ARIMA')\n    \n    # Dynamic order selection based on available data\n    if window_size >= 20:\n        order = (5,1,2)  # Full model for large windows\n        confidence_base = 0.8\n    elif window_size >= 15:\n        order = (3,1,1)  # Reduced model for medium windows  \n        confidence_base = 0.7\n    elif window_size >= 10:\n        order = (2,1,1)  # Minimal ARIMA for small windows\n        confidence_base = 0.6\n    elif window_size >= 6:\n        order = (1,1,1)  # Simplest ARIMA that can converge\n        confidence_base = 0.5\n    else:\n        # For very small windows (2-5 draws), use exponential smoothing approach\n        preds = []\n        for i in range(1,7):\n            series = window[f'Number{i}'].values\n            if len(series) >= 2:\n                # Simple exponential smoothing with trend\n                alpha = 0.3\n                trend = (series[-1] - series[0]) / (len(series) - 1) if len(series) > 1 else 0\n                pred = series[-1] * alpha + (1-alpha) * np.mean(series) + trend\n                preds.append(pred)\n            else:\n                preds.append(np.mean(series))\n        return ensure_valid(preds), 0.4\n    \n    # ARIMA modeling with dynamic order\n    preds, confs = [], []\n    for i in range(1,7):\n        series = window[f'Number{i}'].values\n        try:\n            # Adaptive ARIMA with automatic order adjustment\n            model = ARIMA(series, order=order)\n            fitted_model = model.fit()\n            \n            # Forecast with trend awareness\n            forecast = fitted_model.forecast()[0]\n            trend_weight = 0.5 + 0.5 * abs(fitted_model.params.get('ar.L1', 0))\n            pred_value = forecast * trend_weight\n            \n            preds.append(pred_value)\n            # Confidence increases with window size and model stability\n            conf = confidence_base * (1 + score) * trend_weight\n            confs.append(min(0.95, conf))\n            \n        except Exception as e:\n            # Robust fallback for convergence issues\n            series_mean = np.mean(series)\n            series_trend = (series[-1] - series[0]) / len(series) if len(series) > 1 else 0\n            preds.append(series_mean + series_trend)\n            confs.append(0.3)\n    \n    return ensure_valid(preds), float(np.mean(confs))\n\n\ndef A2_MovingAverage_expert(window):\n    \"\"\"Phase 1 - Adaptive EWMA Expert\"\"\"\n    preds = []\n    for i in range(1,7):\n        series = window[f'Number{i}'].values[-5:]\n        if len(series) < 2:\n            return fallback(window), 0.3\n        alpha = 2/6 + np.std(series)/10\n        ewma = series[-1]*alpha + (1-alpha)*np.mean(series[:-1])\n        lr = LinearRegression().fit(np.arange(len(series)).reshape(-1,1), series)\n        preds.append(ewma + lr.coef_[0])\n    return ensure_valid(preds), 0.7\n\ndef B1_RandomForest_expert(window):\n    \"\"\"Phase 1 - Meta-Optimized Random Forest\"\"\"\n    if len(window) < 15:\n        return fallback(window), 0.3\n    score = perf.score('B1_RandomForest')\n    feats, tars = [], []\n    for idx in range(10, len(window)):\n        row = []\n        for lag in (1,2,3):\n            row += window.iloc[idx-lag][[f'Number{i}' for i in range(1,7)]].tolist()\n        row += [\n            window.iloc[idx-5:idx]['Number1'].mean(),\n            window.iloc[idx-5:idx]['Number2'].std(),\n            score * 10\n        ]\n        feats.append(row)\n        tars.append(window.iloc[idx][[f'Number{i}' for i in range(1,7)]].tolist())\n    rf = RandomForestRegressor(\n        n_estimators=100,\n        max_depth=15 if score > 0.7 else 10,\n        random_state=42, n_jobs=-1\n    )\n    rf.fit(feats, tars)\n    out = rf.predict([feats[-1]])[0]\n    return ensure_valid(out), 0.85\n\ndef E1_FrequencyAnalysis_expert(window):\n    nums = sum((window[f'Number{i}'].tolist() for i in range(1, 7)), [])\n    freq = Counter(nums)\n    vals = np.array(list(freq.values()))\n    z = (vals - vals.mean()) / (vals.std() or 1)\n    keys = list(freq.keys())\n\n    labels = KMeans(n_clusters=2, random_state=42).fit_predict(vals.reshape(-1, 1))\n    hot = [k for k, l in zip(keys, labels) if l == 0]\n    cold = list(set(range(1, 40)) - set(hot))\n\n    hr = 0.7 + (len(np.where(z > 2)[0]) / len(keys)) * 0.3\n    hot_pick = secure_sample(hot, min(int(6 * hr), len(hot))) if hot else []\n    cold_pick = secure_sample(cold, 6 - len(hot_pick))\n    return ensure_valid(hot_pick + cold_pick), 0.75\n\ndef D1_EnsembleStacking_expert(window):\n    \"\"\"Phase 1 - Attention-Stacking Meta-Expert\"\"\"\n    preds, ws = [], []\n    for name in ['A1_ARIMA','A2_MovingAverage','B1_RandomForest','E1_FrequencyAnalysis']:\n        p,c = globals()[f\"{name}_expert\"](window)\n        preds.append(p); ws.append(c * perf.score(name))\n    if not preds:\n        return fallback(window), 0.3\n    w = np.exp(ws) / np.sum(np.exp(ws))\n    final = [sum(p[i] * wi for p,wi in zip(preds, w)) for i in range(6)]\n    return ensure_valid(final), float(min(0.98, np.dot(w, ws)))\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# PHASE 2 ADVANCED EXPERTS (NEW CUTTING-EDGE IMPLEMENTATIONS)\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nclass HybridLSTMTransformer(pl.LightningModule):\n    \"\"\"Advanced LSTM-Transformer Hybrid Architecture\"\"\"\n    def __init__(self, input_size=6, hidden_size=64, num_heads=8, num_layers=3):\n        super().__init__()\n        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True, dropout=0.1)\n        \n        # Multi-head Self-Attention with positional encoding\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=hidden_size, \n            nhead=num_heads, \n            dim_feedforward=hidden_size*4,\n            dropout=0.1,\n            activation='gelu',\n            batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n        \n        # Advanced prediction head with residual connections\n        self.prediction_head = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size*2),\n            nn.LayerNorm(hidden_size*2),\n            nn.GELU(),\n            nn.Dropout(0.1),\n            nn.Linear(hidden_size*2, hidden_size),\n            nn.LayerNorm(hidden_size),\n            nn.GELU(),\n            nn.Linear(hidden_size, 6)\n        )\n        \n        # Attention visualization for interpretability\n        self.attention_weights = None\n    \n    def forward(self, x):\n        # LSTM processing for sequential patterns\n        lstm_out, _ = self.lstm(x)\n        \n        # Transformer with self-attention for long-range dependencies\n        transformer_out = self.transformer(lstm_out)\n        \n        # Extract last timestep and predict\n        prediction = self.prediction_head(transformer_out[:, -1, :])\n        return prediction\n    \n    def predict_numbers(self, window_data):\n        \"\"\"Convert window data to prediction\"\"\"\n        self.eval()\n        with torch.no_grad():\n            # Prepare tensor from window\n            x = torch.tensor(\n                window_data[[f'Number{i}' for i in range(1,7)]].values, \n                dtype=torch.float32\n            ).unsqueeze(0)\n            \n            # Get prediction\n            pred = self(x).squeeze().numpy()\n            \n            # Apply constraints and ensure validity\n            pred = np.clip(pred, 1, 39)\n            return ensure_valid(pred)\n\nclass TemporalFusionExpert:\n    \"\"\"Advanced Temporal Fusion Transformer Expert\"\"\"\n    def __init__(self):\n        self.tft = None\n        self.trained = False\n    \n    def prepare_dataset(self, window):\n        \"\"\"Prepare TimeSeriesDataSet for TFT\"\"\"\n        df = window.reset_index()\n        df['time_idx'] = range(len(df))\n        df['series'] = 0\n        \n        dataset = TimeSeriesDataSet(\n            df,\n            time_idx='time_idx',\n            target=['Number1', 'Number2', 'Number3', 'Number4', 'Number5', 'Number6'],\n            group_ids=['series'],\n            max_encoder_length=min(20, len(df)-1),\n            max_prediction_length=1,\n            time_varying_unknown_reals=['Number1', 'Number2', 'Number3', 'Number4', 'Number5', 'Number6'],\n            allow_missing_timesteps=True\n        )\n        return dataset\n    \n    def predict(self, window):\n        \"\"\"Generate TFT prediction\"\"\"\n        try:\n            if not self.trained or self.tft is None:\n                dataset = self.prepare_dataset(window)\n                dataloader = dataset.to_dataloader(train=True, batch_size=1)\n                \n                # Initialize TFT with optimized hyperparameters\n                self.tft = TemporalFusionTransformer.from_dataset(\n                    dataset,\n                    learning_rate=1e-3,\n                    hidden_size=64,\n                    attention_head_size=8,\n                    dropout=0.2,\n                    hidden_continuous_size=32,\n                    output_size=6,\n                    loss=nn.MSELoss(),\n                    reduce_on_plateau_patience=3\n                )\n                \n                # Quick training for demonstration\n                trainer = pl.Trainer(\n                    max_epochs=20, \n                    enable_checkpointing=False, \n                    logger=False,\n                    enable_progress_bar=False\n                )\n                trainer.fit(self.tft, dataloader)\n                self.trained = True\n            \n            # Generate prediction\n            test_dataset = self.prepare_dataset(window)\n            test_dataloader = test_dataset.to_dataloader(train=False, batch_size=1)\n            predictions = self.tft.predict(test_dataloader)\n            \n            # Extract and process prediction\n            if len(predictions) > 0:\n                pred = predictions[0][:6] if hasattr(predictions[0], '__len__') else predictions[:6]\n                return ensure_valid(pred), 0.92\n            else:\n                return fallback(window), 0.5\n                \n        except Exception as e:\n            return fallback(window), 0.4\n\nclass NeuralDecisionForest:\n    \"\"\"Neural-Boosted Decision Forest with Advanced Features\"\"\"\n    def __init__(self):\n        self.models = []\n        self.trained = False\n    \n    def create_advanced_features(self, window):\n        \"\"\"Generate advanced feature engineering\"\"\"\n        features = []\n        \n        # Statistical features across all numbers\n        all_nums = window[[f'Number{i}' for i in range(1,7)]].values\n        features.extend([\n            np.mean(all_nums), np.std(all_nums), np.median(all_nums),\n            np.percentile(all_nums, 25), np.percentile(all_nums, 75),\n            len(np.unique(all_nums)), np.min(all_nums), np.max(all_nums)\n        ])\n        \n        # Temporal patterns\n        if len(window) > 5:\n            recent_trend = np.polyfit(range(5), all_nums[-5:].mean(axis=1), 1)[0]\n            features.append(recent_trend)\n        else:\n            features.append(0)\n        \n        # Frequency-based features\n        flat_nums = all_nums.flatten()\n        freq_dist = Counter(flat_nums)\n        most_common_freq = freq_dist.most_common(1)[0][1] if freq_dist else 0\n        features.extend([len(freq_dist), most_common_freq])\n        \n        # Gap analysis\n        gaps = np.diff(np.sort(all_nums[-1]))\n        features.extend([np.mean(gaps), np.std(gaps)])\n        \n        return features\n    \n    def predict(self, window):\n        \"\"\"Generate neural forest prediction\"\"\"\n        try:\n            if len(window) < 15:\n                return fallback(window), 0.3\n            \n            # Prepare training data with advanced features\n            X, y = [], []\n            for i in range(10, len(window)):\n                features = self.create_advanced_features(window.iloc[i-10:i])\n                X.append(features)\n                y.append(window.iloc[i][[f'Number{i}' for i in range(1,7)]].values)\n            \n            if len(X) < 5:\n                return fallback(window), 0.3\n            \n            # Train ensemble of neural-boosted forests\n            if not self.trained:\n                from sklearn.ensemble import GradientBoostingRegressor, ExtraTreesRegressor\n                \n                self.models = [\n                    RandomForestRegressor(n_estimators=200, max_depth=20, random_state=42),\n                    GradientBoostingRegressor(n_estimators=100, max_depth=10, random_state=42),\n                    ExtraTreesRegressor(n_estimators=150, max_depth=15, random_state=42)\n                ]\n                \n                for model in self.models:\n                    model.fit(X, y)\n                self.trained = True\n            \n            # Generate prediction from ensemble\n            test_features = self.create_advanced_features(window.iloc[-10:])\n            predictions = [model.predict([test_features])[0] for model in self.models]\n            \n            # Ensemble averaging with confidence weighting\n            weights = [0.4, 0.35, 0.25]  # Random Forest gets highest weight\n            final_pred = np.average(predictions, weights=weights, axis=0)\n            \n            return ensure_valid(final_pred), 0.88\n            \n        except Exception as e:\n            return fallback(window), 0.3\n\nclass MultiScalePatternAnalyzer:\n    \"\"\"Advanced Multi-Scale Pattern Analysis Expert\"\"\"\n    def __init__(self):\n        self.pattern_memory = {}\n    \n    def extract_multiscale_patterns(self, window):\n        \"\"\"Extract patterns at different time scales\"\"\"\n        patterns = {}\n        data = window[[f'Number{i}' for i in range(1,7)]].values\n        \n        # Short-term patterns (last 3 draws)\n        if len(data) >= 3:\n            patterns['short_term'] = data[-3:].flatten()\n        \n        # Medium-term patterns (last 7 draws)\n        if len(data) >= 7:\n            patterns['medium_term'] = data[-7:].flatten()\n        \n        # Long-term patterns (all available data)\n        patterns['long_term'] = data.flatten()\n        \n        # Cyclical patterns\n        if len(data) >= 7:\n            weekly_pattern = [data[i::7] for i in range(min(7, len(data)))]\n            patterns['cyclical'] = np.array([np.mean(p) for p in weekly_pattern if len(p) > 0])\n        \n        return patterns\n    \n    def predict(self, window):\n        \"\"\"Generate multi-scale pattern prediction\"\"\"\n        try:\n            patterns = self.extract_multiscale_patterns(window)\n            \n            # Advanced anomaly detection using multiple scales\n            predictions = []\n            confidences = []\n            \n            # Short-term momentum\n            if 'short_term' in patterns:\n                short_pred = np.mean(patterns['short_term'].reshape(-1, 6), axis=0)\n                predictions.append(short_pred)\n                confidences.append(0.3)\n            \n            # Medium-term trend\n            if 'medium_term' in patterns:\n                med_data = patterns['medium_term'].reshape(-1, 6)\n                med_pred = med_data[-1] + (med_data[-1] - med_data[0]) / len(med_data)\n                predictions.append(med_pred)\n                confidences.append(0.4)\n            \n            # Long-term frequency analysis with clustering\n            if 'long_term' in patterns:\n                freq = Counter(patterns['long_term'])\n                # Advanced clustering for hot/cold number identification\n                vals = np.array(list(freq.values()))\n                if len(vals) > 2:\n                    kmeans = KMeans(n_clusters=3, random_state=42)\n                    clusters = kmeans.fit_predict(vals.reshape(-1, 1))\n                    hot_cluster = np.argmax(kmeans.cluster_centers_.flatten())\n                    hot_numbers = [num for num, cluster in zip(freq.keys(), clusters) \n                                 if cluster == hot_cluster]\n                else:\n                    hot_numbers = list(freq.keys())[:20]\n                \n                # Select top numbers with some randomness\n                if len(hot_numbers) >= 6:\n                    long_pred = np.random.choice(hot_numbers, 6, replace=False)\n                else:\n                    remaining = list(set(range(1, 40)) - set(hot_numbers))\n                    long_pred = list(hot_numbers) + list(np.random.choice(remaining, 6-len(hot_numbers), replace=False))\n                \n                predictions.append(long_pred)\n                confidences.append(0.3)\n            \n            # Ensemble the predictions\n            if predictions:\n                weights = np.array(confidences) / np.sum(confidences)\n                if len(predictions[0]) == 6:  # Numerical predictions\n                    final_pred = np.average(predictions, weights=weights, axis=0)\n                else:  # Mixed types, use weighted voting\n                    final_pred = predictions[np.argmax(confidences)]\n                \n                return ensure_valid(final_pred), float(np.mean(confidences) + 0.1)\n            else:\n                return fallback(window), 0.4\n                \n        except Exception as e:\n            return fallback(window), 0.3\n\n# Initialize Phase 2 experts\nlstm_transformer_expert = HybridLSTMTransformer()\ntft_expert = TemporalFusionExpert()\nneural_forest_expert = NeuralDecisionForest()\npattern_analyzer_expert = MultiScalePatternAnalyzer()\n\n# Phase 2 Expert Interface Functions\ndef A1_ARIMA_expert_v2(window):\n    \"\"\"Phase 2 - Hybrid LSTM-Transformer Expert\"\"\"\n    pred = lstm_transformer_expert.predict_numbers(window)\n    return pred, 0.91\n\ndef A2_MovingAverage_expert_v2(window):\n    \"\"\"Phase 2 - Temporal Fusion Transformer Expert\"\"\"\n    return tft_expert.predict(window)\n\ndef B1_RandomForest_expert_v2(window):\n    \"\"\"Phase 2 - Neural Decision Forest Expert\"\"\"\n    return neural_forest_expert.predict(window)\n\ndef E1_FrequencyAnalysis_expert_v2(window):\n    \"\"\"Phase 2 - Multi-Scale Pattern Analyzer\"\"\"\n    return pattern_analyzer_expert.predict(window)\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# ADVANCED META-SUPERVISOR FOR CO-EXISTENCE\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# CORRECTED ADVANCED META-SUPERVISOR FOR CO-EXISTENCE  \n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nclass APEXMetaSupervisor:\n    \"\"\"Fixed Meta-Learning Supervisor for Phase 1 + Phase 2 Co-existence\"\"\"\n    \n    def __init__(self, data, perf_tracker):\n        self.data = data\n        self.perf = perf_tracker\n        \n        # Define all expert registry with CORRECT function mappings\n        self.expert_registry = {\n            # Phase 1 Experts (Proven) - Standard naming\n            'A1_ARIMA': {'status': 'active', 'min_window': 10, 'phase': 1, 'func_name': 'A1_ARIMA_expert'},\n            'A2_MovingAverage': {'status': 'active', 'min_window': 5, 'phase': 1, 'func_name': 'A2_MovingAverage_expert'},\n            'B1_RandomForest': {'status': 'active', 'min_window': 15, 'phase': 1, 'func_name': 'B1_RandomForest_expert'},\n            'E1_FrequencyAnalysis': {'status': 'active', 'min_window': 10, 'phase': 1, 'func_name': 'E1_FrequencyAnalysis_expert'},\n            'D1_EnsembleStacking': {'status': 'active', 'min_window': 20, 'phase': 1, 'func_name': 'D1_EnsembleStacking_expert'},\n            \n            # Phase 2 Experts (Advanced) - v2 naming  \n            'A1_ARIMA_v2': {'status': 'active', 'min_window': 15, 'phase': 2, 'func_name': 'A1_ARIMA_expert_v2'},\n            'A2_MovingAverage_v2': {'status': 'active', 'min_window': 20, 'phase': 2, 'func_name': 'A2_MovingAverage_expert_v2'},\n            'B1_RandomForest_v2': {'status': 'active', 'min_window': 15, 'phase': 2, 'func_name': 'B1_RandomForest_expert_v2'},\n            'E1_FrequencyAnalysis_v2': {'status': 'active', 'min_window': 10, 'phase': 2, 'func_name': 'E1_FrequencyAnalysis_expert_v2'}\n        }\n        \n        # Meta-learning weights (start equal, learn over time)\n        self.phase_weights = {'phase_1': 0.5, 'phase_2': 0.5}\n        \n    def segment_window(self, end_idx, window_size):\n        \"\"\"Create data window for expert processing\"\"\"\n        start = max(0, end_idx - window_size + 1)\n        return self.data.iloc[start:end_idx+1]\n    \n    def advanced_attention_mechanism(self, expert_predictions, expert_confidences, phases):\n        \"\"\"Advanced attention-based weighting with cross-phase learning\"\"\"\n        \n        # Separate Phase 1 and Phase 2 experts\n        phase_1_indices = [i for i, p in enumerate(phases) if p == 1]\n        phase_2_indices = [i for i, p in enumerate(phases) if p == 2]\n        \n        # Base confidence weighting\n        conf_weights = np.array(expert_confidences)\n        \n        # Performance-based weighting  \n        expert_names = list(expert_predictions.keys())\n        perf_weights = np.array([self.perf.score(name) for name in expert_names])\n        \n        # Phase-based weighting (meta-learning component)\n        phase_weight_array = np.array([\n            self.phase_weights['phase_1'] if p == 1 else self.phase_weights['phase_2'] \n            for p in phases\n        ])\n        \n        # Cross-attention mechanism: let phases inform each other\n        if len(phase_1_indices) > 0 and len(phase_2_indices) > 0:\n            p1_avg_conf = np.mean([expert_confidences[i] for i in phase_1_indices])\n            p2_avg_conf = np.mean([expert_confidences[i] for i in phase_2_indices])\n            \n            # Adaptive phase weighting based on recent performance\n            if p2_avg_conf > p1_avg_conf * 1.1:  # Phase 2 significantly better\n                self.phase_weights['phase_2'] = min(0.7, self.phase_weights['phase_2'] + 0.05)\n                self.phase_weights['phase_1'] = 1 - self.phase_weights['phase_2']\n            elif p1_avg_conf > p2_avg_conf * 1.1:  # Phase 1 significantly better\n                self.phase_weights['phase_1'] = min(0.7, self.phase_weights['phase_1'] + 0.05)\n                self.phase_weights['phase_2'] = 1 - self.phase_weights['phase_1']\n        \n        # Combine all weighting factors\n        combined_weights = conf_weights * perf_weights * phase_weight_array\n        \n        # Softmax normalization for attention\n        exp_weights = np.exp(combined_weights - np.max(combined_weights))\n        final_weights = exp_weights / np.sum(exp_weights)\n        \n        return final_weights\n    \n    def run_meta_cycle(self, window_size=25):\n        \"\"\"Run complete meta-learning prediction cycle with FIXED function calling\"\"\"\n        \n        expert_predictions = {}\n        expert_confidences = {}\n        expert_phases = {}\n        \n        end_idx = len(self.data) - 1\n        window = self.segment_window(end_idx, window_size)\n        \n        # Phase 1: Run all active experts with CORRECT function names\n        for expert_name, config in self.expert_registry.items():\n            if config['status'] != 'active' or len(window) < config['min_window']:\n                continue\n                \n            try:\n                # FIXED: Use the correct function name from registry\n                func_name = config['func_name']\n                expert_func = globals()[func_name]\n                pred, conf = expert_func(window)\n                \n                expert_predictions[expert_name] = pred\n                expert_confidences[expert_name] = conf\n                expert_phases[expert_name] = config['phase']\n                \n                print(f\"âœ… {expert_name}: {pred} (conf={conf:.3f})\")\n                \n            except KeyError as e:\n                print(f\"âŒ Expert {expert_name} failed: Function '{config['func_name']}' not found\")\n                continue\n            except Exception as e:\n                print(f\"âŒ Expert {expert_name} failed: {str(e)}\")\n                continue\n        \n        if not expert_predictions:\n            print(\"âš ï¸ No experts succeeded, using fallback\")\n            return fallback(window), {}\n        \n        # Phase 2: Advanced attention-based aggregation\n        names = list(expert_predictions.keys())\n        confs = [expert_confidences[name] for name in names]\n        phases = [expert_phases[name] for name in names]\n        \n        weights = self.advanced_attention_mechanism(expert_predictions, confs, phases)\n        \n        # Phase 3: Generate final prediction with cross-position attention\n        final_prediction = []\n        for position in range(6):\n            # Weight predictions for this position across all experts\n            position_values = [expert_predictions[name][position] for name in names]\n            weighted_value = np.sum([val * w for val, w in zip(position_values, weights)])\n            final_prediction.append(weighted_value)\n        \n        final_prediction = ensure_valid(final_prediction)\n        \n        # Phase 4: Update performance tracking\n        avg_confidence = np.average(confs, weights=weights)\n        for name in names:\n            error = max(0.0, 1 - expert_confidences[name])\n            self.perf.update(name, error)\n        \n        # Phase 5: Package detailed results\n        expert_details = {\n            name: {\n                'pred': expert_predictions[name],\n                'conf': expert_confidences[name],\n                'weight': float(weights[i]),\n                'phase': expert_phases[name]\n            } for i, name in enumerate(names)\n        }\n        \n        # Add meta-learning info\n        meta_info = {\n            'phase_1_weight': self.phase_weights['phase_1'],\n            'phase_2_weight': self.phase_weights['phase_2'],\n            'total_experts': len(names),\n            'phase_1_experts': len([p for p in phases if p == 1]),\n            'phase_2_experts': len([p for p in phases if p == 2]),\n            'ensemble_confidence': float(avg_confidence)\n        }\n        \n        return final_prediction, {'experts': expert_details, 'meta': meta_info}\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# CORRECTED EXECUTION\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n# Initialize the FIXED meta-supervisor\nprint(\"ðŸ”§ Initialising secure APEX Meta-Supervisor ...\")\nmeta_supervisor = APEXMetaSupervisor(data, perf)\n\nprint(\"\\nðŸš€ APEX Phase-1 + Phase-2 Co-Existence System (Secure RNG)\")\nprint(\"=\" * 80)\nfinal_prediction, details = meta_supervisor.run_meta_cycle(window_size=25)\n\nprint(f\"\\nðŸŽ¯ FINAL ENSEMBLE PREDICTION: {final_prediction}\")\nprint(f\"ðŸ“Š Ensemble confidence: {details['meta']['ensemble_confidence']:.3f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-19T10:15:03.129668Z","iopub.execute_input":"2025-09-19T10:15:03.130029Z","iopub.status.idle":"2025-09-19T10:15:28.423926Z","shell.execute_reply.started":"2025-09-19T10:15:03.129992Z","shell.execute_reply":"2025-09-19T10:15:28.422981Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# Phase-4 Expert Modules (Secure RNG Edition) â€“ Category E, F, G\n# Each expert returns (pred List[int], conf: float)\n# All stochastic operations use CSPRNG via `secrets` / `SystemRandom`\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nimport secrets\nfrom random import SystemRandom\n_secure = SystemRandom()\n\nimport numpy as np\nfrom collections import Counter\n\n# â”€â”€â”€ Secure helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndef s_randint(a: int, b: int) -> int:\n    \"\"\"Secure randint in [a, b].\"\"\"\n    return secrets.randbelow(b - a + 1) + a\n\ndef s_choice(seq):\n    \"\"\"Secure single choice from non-empty sequence.\"\"\"\n    return seq[secrets.randbelow(len(seq))]\n\ndef s_sample(seq, k: int):\n    \"\"\"Secure k-sample without replacement.\"\"\"\n    return _secure.sample(seq, k)\n\n# unique6 is defined in the Phase-3 cell; reused here\n# ---------------------------------------------------------------------------\n\ndef E2_Number_Theory_expert(window):\n    nums = sum((window[f'Number{i}'].tolist() for i in range(1, 7)), [])\n    mod_counts = Counter(n % 7 for n in nums)\n    top_res = [r for r, _ in mod_counts.most_common(2)]\n    cand = {((res + k * 7) % 39) + 1 for res in top_res for k in range(6)}\n    pred = sorted(cand)[:6]\n    freq = mod_counts[top_res[0]] / len(nums)\n    return pred, float(min(0.9, 0.5 + freq))\n\ndef E6_Sum_Range_Analysis_expert(window):\n    sums = window[[f'Number{i}' for i in range(1, 7)]].sum(axis=1)\n    base = int(sums.mean() // 6)\n    pred = sorted({base + i for i in range(6)})[:6]\n    conf = float(0.6 + 0.4 * (1 - np.std(sums) / (6 * 39)))\n    return pred, conf\n\ndef E9_Gap_Analysis_expert(window):\n    latest = window.iloc[-2:][[f'Number{i}' for i in range(1, 7)]].values\n    gaps = np.diff(np.sort(latest.flatten()))\n    med_gap = int(np.median(gaps)) if len(gaps) else 0\n    last = sorted(window.iloc[-1][[f'Number{i}' for i in range(1, 7)]])\n    pred = {((n + med_gap - 1) % 39) + 1 for n in last}\n    while len(pred) < 6:\n        pred.add(s_randint(1, 39))\n    conf = float(0.5 + 0.5 * (np.unique(gaps).size / max(len(gaps), 1)))\n    return sorted(pred), conf\n\ndef E16_Chaos_Theory_expert(window):\n    last = window.iloc[-1][[f'Number{i}' for i in range(1, 7)]].values / 39\n    r = 3.9\n    next_vals = r * last * (1 - last)\n    pred = {int(v * 39) + 1 for v in next_vals}\n    while len(pred) < 6:\n        pred.add(s_randint(1, 39))\n    conf = float(max(0.1, 1 - np.std(next_vals)))\n    return sorted(pred), conf\n\ndef F3_Causal_Inference_expert(window):\n    df = window[[f'Number{i}' for i in range(1, 7)]]\n    corr = df.corr().abs().mean().sort_values(ascending=False)\n    top_cols = corr.index[:6]\n    pred = sorted(int(window[col].iloc[-1]) for col in top_cols)\n    conf = float(min(0.9, corr[top_cols].mean()))\n    return pred, conf\n\ndef F16_Multi_Armed_Bandits_expert(window):\n    \"\"\"\n    EXP3-style bandit expert that securely samples experts\n    and builds a six-number prediction without index errors.\n    \"\"\"\n    if not state['hist_match']:\n        return fallback(window), 0.3\n\n    keys, vals = zip(*state['hist_match'].items())\n    weights = np.array(vals) + 1e-3\n    weights /= weights.sum()\n\n    sampled = _secure.choices(keys, weights, k=6)\n    preds = []\n    for idx, ex in enumerate(sampled):\n        raw, _ = globals().get(f\"{ex}_expert\", _stub)(window)\n        safe = unique6(raw)\n        preds.append(safe[idx % 6])\n\n    final = ensure_valid(preds)\n    conf  = float(0.6 + 0.4 * (1 - np.std(vals)))\n    return final, conf\n\ndef F2_Quantum_ML_expert(window):\n    X = window[[f'Number{i}' for i in range(1, 7)]].values.astype(float)\n    rng = np.random.default_rng(secrets.randbits(128))\n    Q, _ = np.linalg.qr(rng.standard_normal((6, 6)))\n    proj = (X[-1] @ Q) % 39 + 1\n    pred = {int(v) for v in proj}\n    while len(pred) < 6:\n        pred.add(s_randint(1, 39))\n    return sorted(pred), 0.70\n\ndef G1_Auto_ML_expert(window):\n    from sklearn.tree import DecisionTreeRegressor\n    df = window[[f'Number{i}' for i in range(1, 7)]]\n    X = np.arange(len(df)).reshape(-1, 1)\n    preds, confs = [], []\n    for i in range(6):\n        y = df[f'Number{i+1}']\n        mdl = DecisionTreeRegressor(max_depth=3, random_state=s_randint(1, 10_000))\n        mdl.fit(X, y)\n        preds.append(int(np.clip(round(mdl.predict([[len(df)]])[0]), 1, 39)))\n        confs.append(1 - mdl.tree_.max_depth / 10)\n    return ensure_valid(preds), float(np.mean(confs))\n\ndef G3_Genetic_Programming_expert(window):\n    try:\n        from deap import creator, base, gp, tools\n    except ImportError:\n        # Fallback if DEAP not installed\n        seq = window[[f'Number{i}' for i in range(1, 7)]].iloc[-2:].mean().round().tolist()\n        return ensure_valid(seq), 0.35\n    seqs = [window[f'Number{i+1}'].tolist() for i in range(6)]\n    preds = []\n    for seq in seqs:\n        diff = seq[-1] - seq[-2] if len(seq) > 1 else 0\n        preds.append(((seq[-1] + diff - 1) % 39) + 1)\n    return ensure_valid(preds), 0.60\n\ndef G19_Catastrophic_Forgetting_expert(window):\n    nums = sum((window[f'Number{i}'].tolist() for i in range(1, 7)), [])\n    freq = Counter(nums)\n    if not freq:\n        return fallback(window), 0.3\n    min_cnt = min(freq.values())\n    rare = [n for n, c in freq.items() if c == min_cnt]\n    if len(rare) < 6:\n        others = [n for n in range(1, 40) if n not in rare]\n        rare += s_sample(others, 6 - len(rare))\n    pred = sorted(s_sample(rare, 6))\n    conf = float(0.4 + 0.6 * (len(rare) / 39))\n    return pred, conf\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-19T10:15:28.424994Z","iopub.execute_input":"2025-09-19T10:15:28.425568Z","iopub.status.idle":"2025-09-19T10:15:28.453858Z","shell.execute_reply.started":"2025-09-19T10:15:28.425544Z","shell.execute_reply":"2025-09-19T10:15:28.452865Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# Phase-5 Expert Modules â€“ Advanced Cryptanalysis Agents\n# Each returns (pred: List[int], conf: float)\n# Use aescrypt-like primitives, CRT, differential analysis, and secure RNG\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nimport secrets\nfrom random import SystemRandom\n_secure = SystemRandom()\n\nimport numpy as np\nfrom collections import Counter\nfrom math import gcd\nfrom sympy.ntheory.modular import crt  # Chinese remainder theorem\n\n# Secure RNG helpers (reuse from orchestrator)\ndef s_randint(a, b): return secrets.randbelow(b - a + 1) + a\ndef s_choice(seq):   return seq[secrets.randbelow(len(seq))]\ndef s_sample(seq, k): return _secure.sample(seq, k)\n\n# Expert 1: E20_Fourier_Residue_Expert\ndef E20_Fourier_Residue_expert(window):\n    \"\"\"\n    Applies DFT on flattened draws mod small primes to detect frequency peaks.\n    \"\"\"\n    nums = sum((window[f'Number{i}'].tolist() for i in range(1,7)), [])\n    # transform sequence into complex vector\n    vec = np.array(nums) - np.mean(nums)\n    fft = np.fft.fft(vec)\n    # Find top two frequency bins\n    mags = np.abs(fft[:len(fft)//2])\n    peaks = mags.argsort()[-2:]\n    # reconstruct six numbers from inverse peaks\n    preds = []\n    for pk in peaks:\n        phase = np.angle(fft[pk])\n        val = int((mags[pk] * np.cos(phase)) % 39) + 1\n        preds.append(val)\n    # pad using secure RNG\n    while len(preds) < 6:\n        preds.append(s_randint(1,39))\n    # confidence from peak sharpness\n    conf = float(min(0.9, (mags[peaks[0]] / (np.mean(mags)+1e-6))))\n    return sorted(set(preds))[:6], conf\n\n# Expert 2: E21_CRT_Reconstructor\ndef E21_CRT_Reconstructor_expert(window):\n    \"\"\"\n    Uses Chinese Remainder Theorem on residues mod 5,7,9 to reconstruct patterns.\n    \"\"\"\n    nums = sum((window[f'Number{i}'].tolist() for i in range(1,7)), [])\n    res5 = [n % 5 for n in nums]\n    res7 = [n % 7 for n in nums]\n    res9 = [n % 9 for n in nums]\n    # solve for k in last residue triple\n    r5, r7, r9 = res5[-1], res7[-1], res9[-1]\n    candidates = []\n    for x in range(1,40):\n        if x%5==r5 and x%7==r7 and x%9==r9:\n            candidates.append(x)\n    # pick up to six candidates\n    pred = sorted(candidates)[:6]\n    while len(pred)<6:\n        pred.append(s_randint(1,39))\n    # confidence by count of valid solutions\n    conf = float(min(0.9, len(candidates)/6))\n    return pred, conf\n\n# Expert 3: E22_Differential_Entropy\ndef E22_Differential_Entropy_expert(window):\n    \"\"\"\n    Estimates entropy change across sliding pairs; low change indicates pattern.\n    \"\"\"\n    arr = np.array(sum((window[f'Number{i}'].tolist() for i in range(1,7)), []))\n    # compute histograms for last two draws\n    h1, _ = np.histogram(arr[-12:-6], bins=39, range=(1,40), density=True)\n    h2, _ = np.histogram(arr[-6:], bins=39, range=(1,40), density=True)\n    # KL divergence proxy\n    kl = np.sum(h1 * np.log((h1+1e-6)/(h2+1e-6)))\n    # next draw prediction: repeat most recent pattern\n    recent = window.iloc[-1][[f'Number{i}' for i in range(1,7)]].tolist()\n    pred = sorted(unique6(recent))\n    conf = float(max(0.1, 1 - kl))\n    return pred, conf\n\n# Expert 4: F23_BlockCipher_LFSR_Hybrid\ndef F23_BlockCipher_LFSR_expert(window):\n    \"\"\"\n    Uses LFSR seeded from AES-like S-box outputs on window bytes.\n    \"\"\"\n    # simple S-box mimic: nonlinear mapping\n    sbox = lambda x: ((x*45) ^ 0x1F) & 0xFF\n    seed = sum(sbox(n) for n in sum((window[f'Number{i}'].tolist() for i in range(1,7)), []))\n    # LFSR: x_n+1 = feedback of taps at bits [0,2,3,5]\n    state_val = seed & 0x7F\n    pred=[]\n    for _ in range(6):\n        bit = ((state_val>>0) ^ (state_val>>2) ^ (state_val>>3) ^ (state_val>>5)) & 1\n        state_val = ((state_val>>1) | (bit<<6)) & 0x7F\n        pred.append((state_val % 39)+1)\n    conf = 0.5\n    return sorted(unique6(pred)), conf\n\n# Expert 5: F24_Timing_Attack_Simulator\ndef F24_Timing_Attack_expert(window):\n    \"\"\"\n    Simulates timing channel by measuring model runtime variances (proxy).\n    \"\"\"\n    import time\n    times = []\n    for func in [A1_ARIMA_expert, A2_MovingAverage_expert]:\n        start = time.perf_counter()\n        func(window)\n        times.append(time.perf_counter()-start)\n    # slower function output appended\n    slow_idx = int(np.argmax(times)) +1\n    pred = [slow_idx + i for i in range(6)]\n    conf = float(0.4 + 0.6*(max(times)/sum(times)))\n    return sorted(unique6(pred)), conf\n\n# Expert 6: E25_Resampling_Chaos\ndef E25_Resampling_Chaos_expert(window):\n    \"\"\"\n    Bootstrap resampling + chaos measure: uses Lyapunov exponent proxy.\n    \"\"\"\n    nums = sum((window[f'Number{i}'].tolist() for i in range(1,7)), [])\n    # bootstrap sample\n    bs = s_sample(nums, len(nums))\n    # proxy Lyapunov: average log ratio\n    ratios = [abs(bs[i+1]-bs[i]) / (abs(bs[i]-bs[i-1])+1e-6) for i in range(1,len(bs)-1)]\n    lyap = np.mean(np.log(np.abs(ratios)+1e-6))\n    # predict by sampling high-ratio points\n    high = [bs[i] for i,r in enumerate(ratios) if r>np.median(ratios)]\n    pred = sorted(s_sample(high if len(high)>=6 else nums, 6))\n    conf = float(max(0.1, min(0.9, 1 - abs(lyap))))\n    return pred, conf\n\n# Expert 7: G26_Pruned_Network_Attack\ndef G26_Pruned_Network_expert(window):\n    \"\"\"\n    Uses a tiny pruned neural net (PyTorch) to fit last draw pattern.\n    \"\"\"\n    import torch\n    # simple one-layer MLP\n    X = torch.tensor(window[[f'Number{i}' for i in range(1,7)]].values[:-1], dtype=torch.float32)\n    y = torch.tensor(window[[f'Number{i}' for i in range(1,7)]].values[1:], dtype=torch.float32)\n    mdl = torch.nn.Sequential(\n        torch.nn.Linear(6, 6),\n        torch.nn.ReLU(),\n        torch.nn.Linear(6, 6)\n    )\n    opt = torch.optim.Adam(mdl.parameters(), lr=0.01)\n    for _ in range(20):\n        opt.zero_grad()\n        loss = torch.nn.MSELoss()(mdl(X), y)\n        loss.backward(); opt.step()\n    out = mdl(torch.tensor(window[[f'Number{i}' for i in range(1,7)]].values[-1:],dtype=torch.float32))\n    pred = sorted(unique6(out.detach().numpy().flatten()))\n    conf = float(max(0.2, 1 - loss.item()))\n    return pred, conf\n\n# Expert 8: E27_Fractal_Pattern_Expert\ndef E27_Fractal_Pattern_expert(window):\n    \"\"\"\n    Uses Mandelbrot membership iteration counts as pattern scores.\n    \"\"\"\n    def mandel_count(c, maxit=20):\n        z=0; count=0\n        while abs(z)<=2 and count<maxit:\n            z=z*z+c; count+=1\n        return count\n    nums = window.iloc[-1][[f'Number{i}' for i in range(1,7)]].values\n    counts = [mandel_count(complex(n/39-1,n/39-1)) for n in nums]\n    pred = sorted(unique6(counts))\n    conf = float(min(0.8, np.mean(counts)/20))\n    return pred, conf\n\n# Expert 9: F28_Kolmogorov_Smirnov_Test\ndef F28_KSTest_expert(window):\n    \"\"\"\n    Applies KS-test against uniform distribution on history; low p-value indicates bias.\n    \"\"\"\n    nums = sum((window[f'Number{i}'].tolist() for i in range(1,7)), [])\n    from scipy.stats import kstest\n    stat,p = kstest(nums, 'uniform', args=(1,39))\n    # if bias detected, repeat most frequent numbers\n    freq = Counter(nums).most_common(6)\n    pred = [n for n,_ in freq]\n    conf = float(max(0.1, min(0.9, 1 - p)))\n    return sorted(unique6(pred)), conf\n\n# Expert 10: G29_Entropy_Gradient_Boost\ndef G29_Entropy_Gradient_Boost_expert(window):\n    \"\"\"\n    Trains a LightGBM regressor on entropy features to predict next draw.\n    Uses MultiOutputRegressor to handle 6-dimensional targets.\n    \"\"\"\n    from sklearn.multioutput import MultiOutputRegressor\n    import lightgbm as lgb\n    import numpy as np\n\n    # Prepare features and targets\n    feats, targ = [], []\n    arr = window[[f'Number{i}' for i in range(1,7)]].values\n    for i in range(10, len(window)):\n        win = arr[i-10:i]\n        # compute per-column Shannon entropy over the 10-draw window\n        ent_feats = []\n        for col in range(6):\n            counts = np.unique(win[:, col], return_counts=True)[1]\n            probs  = counts / counts.sum()\n            ent    = -np.sum(probs * np.log2(probs + 1e-6))\n            ent_feats.append(ent)\n        feats.append(ent_feats)\n        targ.append(arr[i])  # shape (6,)\n\n    if not feats:\n        return fallback(window), 0.4\n\n    # Wrap LightGBM in MultiOutputRegressor to support 6-dimensional targets\n    base = lgb.LGBMRegressor(n_estimators=50, random_state=42)\n    model = MultiOutputRegressor(base, n_jobs=-1)\n    model.fit(feats, targ)\n\n    # Compute features for the prediction step (last 10 draws)\n    last_feats = []\n    win = arr[-10:]\n    for col in range(6):\n        counts = np.unique(win[:, col], return_counts=True)[1]\n        probs  = counts / counts.sum()\n        ent    = -np.sum(probs * np.log2(probs + 1e-6))\n        last_feats.append(ent)\n\n    pred = model.predict([last_feats])[0]\n    return sorted(unique6(pred)), 0.88\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-19T10:15:28.454853Z","iopub.execute_input":"2025-09-19T10:15:28.45521Z","iopub.status.idle":"2025-09-19T10:15:28.491493Z","shell.execute_reply.started":"2025-09-19T10:15:28.45517Z","shell.execute_reply":"2025-09-19T10:15:28.490346Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# Phase-5 Counter-AI-RNG Expert Modules â€“ Advanced Cryptanalysis Agents\n# Each returns (pred: List[int], conf: float)\n# Uses secure RNG (secrets/SystemRandom) and pandasâ†’list conversions for robustness.\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nimport secrets\nfrom random import SystemRandom\n_secure = SystemRandom()\n\nimport numpy as np\nimport pandas as pd\nfrom collections import Counter\nfrom sympy.ntheory.modular import crt\nimport torch\nfrom sklearn.multioutput import MultiOutputRegressor\nimport lightgbm as lgb\n\n# Secure RNG helpers\ndef s_randint(a: int, b: int) -> int:\n    return secrets.randbelow(b - a + 1) + a\n\ndef s_uniform(a: float, b: float) -> float:\n    return _secure.uniform(a, b)\n\ndef s_sample(seq, k: int):\n    return _secure.sample(seq, k)\n\ndef unique6(vec):\n    nums = [int(np.clip(round(x), 1, 39)) for x in vec]\n    seen, out = set(), []\n    for x in nums:\n        if x not in seen:\n            seen.add(x); out.append(x)\n        if len(out)==6: break\n    while len(out)<6:\n        r = s_randint(1,39)\n        if r not in seen:\n            seen.add(r); out.append(r)\n    return sorted(out)\n\ndef fallback(window):\n    # most frequent fallback\n    nums = sum((window[f'Number{i}'].tolist() for i in range(1,7)), [])\n    top = [n for n,_ in Counter(nums).most_common(6)]\n    return unique6(top)\n\n# Expert 1: E30_MarkovChain_Residuals_expert\ndef E30_MarkovChain_Residuals_expert(window):\n    \"\"\"\n    Builds transition matrix of last draws and picks six highest-probability next states.\n    \"\"\"\n    seq = sum((window[f'Number{i}'].tolist() for i in range(1,7)), [])\n    # build 39Ã—39 transition counts\n    M = np.zeros((39,39))\n    for a,b in zip(seq, seq[1:]):\n        M[a-1,b-1] += 1\n    # normalize to probabilities\n    P = M / (M.sum(axis=1, keepdims=True) + 1e-9)\n    last = seq[-1]-1\n    probs = P[last]\n    # pick top 6 next numbers\n    top6 = np.argsort(probs)[-6:][::-1] + 1\n    conf = float(min(0.9, probs[top6-1].mean() * 10))\n    return unique6(top6), conf\n\n# Expert 2: E31_Fractal_Dimension_expert\ndef E31_Fractal_Dimension_expert(window):\n    \"\"\"\n    Computes box-counting dimension of the flattened sequence and predicts high-dimension cells.\n    \"\"\"\n    seq = sum((window[f'Number{i}'].tolist() for i in range(1,7)), [])\n    arr = np.array(seq)  # Convert to numpy array for numerical ops\n    dims = []\n    for n in range(1,40):\n        # count boxes of size k= cell intervals\n        k = np.mean(arr==n)\n        dims.append(-np.log2(k+1e-9))  # Use 1e-9 to avoid log(0); correct formula\n    dims = np.array(dims)  # Ensure dims is numpy array for std/max\n    # pick six largest dims\n    idx = np.argsort(dims)[-6:] + 1\n    conf = float(min(0.9, np.std(dims)/np.max(dims+1e-6)))\n    return unique6(idx), conf\n\n# Expert 3: E32_Poisson_Process_expert\ndef E32_Poisson_Process_expert(window):\n    \"\"\"\n    Fits Poisson Î» to each numberâ€™s historical count; predicts six highest Î».\n    \"\"\"\n    seq = sum((window[f'Number{i}'].tolist() for i in range(1,7)), [])\n    counts = Counter(seq)\n    lam = {n: counts[n]/len(window) for n in range(1,40)}\n    top6 = sorted(lam, key=lambda n: lam[n], reverse=True)[:6]\n    conf = float(min(0.9, np.mean(list(lam.values()))))\n    return unique6(top6), conf\n\n# Expert 4: F33_Ghost_Factorization_expert\ndef F33_Ghost_Factorization_expert(window):\n    \"\"\"\n    Uses Pollardâ€™s rho to factor pairwise differences mod small primes, revealing cycles.\n    \"\"\"\n    import random\n    from sympy import factorint\n    seq = sum((window[f'Number{i}'].tolist() for i in range(1,7)), [])\n    diffs = [abs(a-b) or 1 for a,b in zip(seq, seq[1:])]\n    facs = [factorint(d) for d in diffs]\n    # collect small factors\n    flats = [p for f in facs for p in f if p<40]\n    top6 = Counter(flats).most_common(6)\n    top = [n for n,_ in top6]\n    conf = float(min(0.9, len(top)/6))\n    return unique6(top), conf\n\n# Expert 5: F34_Lattice_Basis_expert\ndef F34_Lattice_Basis_expert(window):\n    \"\"\"\n    Applies LLL reduction to linear recurrence coefficients from sequence embedding.\n    Uses a pure-Python LLL implementation for Kaggle compatibility (no fpylll needed).\n    \"\"\"\n    seq = sum((window[f'Number{i}'].tolist() for i in range(1,7)), [])\n    # Simple Python LLL (basic version for 6x6 matrix)\n    def lll_basis(M):\n        # Gram-Schmidt orthogonalization with reduction\n        for i in range(6):\n            for j in range(i):\n                mu = np.dot(M[i], M[j]) / np.dot(M[j], M[j])\n                M[i] = M[i] - mu * M[j]\n        return M\n\n    # Embed as 6x6 matrix from seq\n    if len(seq) < 36:\n        return fallback(window), 0.4  # Too short for 6x6\n    mat = np.array(seq[-36:]).reshape(6,6) % 39 + 1\n    reduced = lll_basis(mat)\n    # Extract first row as prediction\n    pred = reduced[0]\n    conf = float(min(0.9, np.std(pred) / np.mean(pred + 1e-6)))\n    return unique6(pred), conf\n\n# Expert 6: G35_Meta_Reinforcement_expert\ndef G35_Meta_Reinforcement_expert(window):\n    \"\"\"\n    Trains a tiny REINFORCE policy network to select numbers maximizing past hits.\n    \"\"\"\n    import torch.nn as nn, torch.optim as optim, torch\n    seq = sum((window[f'Number{i}'].tolist() for i in range(1,7)), [])\n    # policy: 39 logits\n    policy = nn.Linear(39,39)\n    opt = optim.Adam(policy.parameters(), lr=1e-2)\n    # dummy training on one-hot last state\n    state_vec = torch.zeros(39); state_vec[seq[-1]-1]=1\n    for _ in range(5):\n        logits = policy(state_vec)\n        dist = torch.distributions.Categorical(logits=logits)\n        action = dist.sample((6,))\n        loss = -dist.log_prob(action).mean()\n        opt.zero_grad(); loss.backward(); opt.step()\n    preds = [(int(a)+1) for a in action]\n    return unique6(preds), float(0.6)\n\n# Expert 7: G36_Evolutionary_Ensemble_expert\ndef G36_Evolutionary_Ensemble_expert(window):\n    \"\"\"\n    Genetic algorithm that evolves small expert subsets and uses their average prediction.\n    Ensures all sub-predictions are fixed to length 6 before averaging to avoid shape errors.\n    \"\"\"\n    experts = list(expert_registry.keys())\n    # start population: random subsets\n    pop = [s_choice(experts) for _ in range(10)]\n    # pick 6 from pop\n    chosen = pop[:6]\n    preds = []\n    for e in chosen:\n        raw_p, _ = globals().get(f\"{e}_expert\", lambda w: (fallback(w), 0))(window)\n        # Fix to exactly 6 unique ints\n        safe_p = unique6(raw_p)\n        preds.append(safe_p)\n    # average by position (now safe, all shape (6,))\n    agg = np.mean(np.array(preds), axis=0)\n    return unique6(agg), 0.5\n\n# Expert 8: G37_Autoencoder_Anomaly_expert\ndef G37_Autoencoder_Anomaly_expert(window):\n    \"\"\"\n    Uses a small PyTorch autoencoder on past draws; anomalous reconstruction errors flagged.\n    \"\"\"\n    import torch.nn as nn, torch\n    data = torch.tensor(window[[f'Number{i}' for i in range(1,7)]].values, dtype=torch.float32)\n    enc = nn.Linear(6,3); dec=nn.Linear(3,6)\n    for _ in range(10):\n        # training skipped for brevity\n        pass\n    rec = dec(enc(data))\n    errs = torch.mean((rec - data)**2, dim=0).detach().numpy()\n    top6 = np.argsort(errs)[-6:]+1\n    return unique6(top6), float(min(0.9, np.mean(errs)))\n\n# Expert 9: G38_Hypernet_Bias_expert\ndef G38_Hypernet_Bias_expert(window):\n    \"\"\"\n    Hypernetwork generates weights for sub-ensembles conditioned on window stats.\n    \"\"\"\n    stats = np.mean(window[[f'Number{i}' for i in range(1,7)]].values, axis=0)\n    # hypernetwork: simple scaled softmax\n    w = np.exp(stats) / np.sum(np.exp(stats))\n    # weight top6 positions\n    idx = np.argsort(w)[-6:]+1\n    return unique6(idx), float(min(0.9, np.max(w)))\n\n# Expert 10: E33_MonteCarlo_Simulation_expert\ndef E33_MonteCarlo_Simulation_expert(window):\n    \"\"\"\n    Bootstrap Monte Carlo of past sequences to sample top co-occurring numbers.\n    \"\"\"\n    seq = sum((window[f'Number{i}'].tolist() for i in range(1,7)), [])\n    samples = []\n    for _ in range(100):\n        bs = s_sample(seq, len(seq))\n        samples.extend(bs[:6])\n    top6 = [n for n,_ in Counter(samples).most_common(6)]\n    return unique6(top6), 0.6\n\n\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-19T10:15:28.492686Z","iopub.execute_input":"2025-09-19T10:15:28.493017Z","iopub.status.idle":"2025-09-19T10:15:32.933381Z","shell.execute_reply.started":"2025-09-19T10:15:28.492987Z","shell.execute_reply":"2025-09-19T10:15:32.932607Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def E40_NIST_80022_Battery_expert(window):\n    import numpy as np\n    seq = sum((window[f'Number{i}'].tolist() for i in range(1,7)), [])\n    # Frequency, Runs, and Serial Correlations\n    freq = np.mean([abs(n-20) for n in seq]) / 19\n    changes = np.sum(np.diff(np.sign(np.diff(seq))))\n    runs = changes / len(seq)\n    serial_corr = np.corrcoef(seq[:-1], seq[1:])[0,1]\n    pvals = [1-freq, 1-runs, 1-abs(serial_corr)]\n    # Lower pval = likely pattern, so invert\n    score = 1 - np.mean([abs(p) for p in pvals])\n    pred = np.argsort([freq, runs, serial_corr])[-6:] + 1\n    pred = [int(x) for x in pred]\n    while len(pred) < 6:\n        pred.append(s_randint(1,39))\n    return unique6(pred), float(np.clip(score, 0.3, 0.95))\n\n\ndef G50_Neural_Discriminator_expert(window):\n    import torch, torch.nn as nn\n    seq = window[[f'Number{i}' for i in range(1,7)]].values.astype(float)\n    # Label last 20 draws as either \"AI\" (pattern matches expert) or \"RNG\" (uniform)\n    y = np.zeros(len(seq)-1)\n    for i in range(1, len(seq)):\n        diffs = np.abs(seq[i] - seq[i-1])\n        y[i-1] = 1 if np.mean(diffs) < 5 else 0\n    X = torch.tensor(seq[:-1], dtype=torch.float32)\n    y = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n    # Tiny net\n    model = nn.Sequential(nn.Linear(6,12), nn.ReLU(), nn.Linear(12,2))\n    loss_fn = nn.CrossEntropyLoss()\n    opt = torch.optim.Adam(model.parameters(), lr=0.01)\n    for _ in range(15):  # quick, overfit for illustration\n        opt.zero_grad()\n        o = model(X)\n        loss = loss_fn(o, y.squeeze().long())\n        loss.backward(); opt.step()\n    # Use logits to pick the \"AI\"most likely next draw from recent history\n    last = torch.tensor(seq[-1:], dtype=torch.float32)\n    res = model(last).detach().numpy().flatten()\n    idx = int(np.argmax(res))\n    pred = [int(np.mean(seq[-3:], axis=0)[i]) for i in range(6)] if idx==1 else [s_randint(1,39) for _ in range(6)]\n    return unique6(pred), float(np.min([0.94, np.max(res)/2 + 0.5]))\n\n\ndef X10_GraphEntropy_SubGraph_expert(window):\n    import networkx as nx, numpy as np\n    seq = sum((window[f'Number{i}'].tolist() for i in range(1,7)), [])\n    G = nx.DiGraph()\n    for i in range(len(seq)-1):\n        G.add_edge(seq[i], seq[i+1])\n    # Compute node entropy (Shannon)\n    degrees = np.array([G.degree(n) for n in G.nodes()])\n    probs = degrees / (degrees.sum() or 1)\n    entropy = -np.sum(probs*np.log2(probs+1e-9))\n    # Pick top-entropy nodes as prediction\n    ent_nodes = np.argsort(-degrees)[:6]\n    pred = [list(G.nodes())[e] for e in ent_nodes]\n    while len(pred)<6: pred.append(s_randint(1,39))\n    return unique6(pred), float(min(0.9, entropy/6))\n\n\ndef advanced_meta_aggregate(window, actual=None):\n    # 1. Run all experts in parallel\n    results = run_all_experts_parallel(window, expert_registry)\n    preds, confs = {}, {}\n    for name, (pred, conf) in results.items():\n        preds[name] = pred\n        confs[name] = conf\n    # 2. Calculate diversity\n    diversity = np.mean([\n        len(set(preds[e1]) ^ set(preds[e2])) for e1 in preds for e2 in preds if e1 != e2\n    ]) / 6\n    # 3. Weighted softmax with bonus for diversity\n    scores = np.array(list(confs.values()), dtype=np.float32)\n    scores *= (1+0.1*diversity)\n    weights = np.exp(scores - np.max(scores))\n    weights /= weights.sum()\n    # 4. Aggregation\n    out = np.zeros(6)\n    names = list(preds)\n    for i in range(len(names)):\n        out += weights[i]*np.array(unique6(preds[names[i]]))\n    out_final = unique6(out)\n    return out_final, dict(diversity=diversity, confs=confs, weights=weights, step_preds=preds)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-19T10:15:32.936158Z","iopub.execute_input":"2025-09-19T10:15:32.936869Z","iopub.status.idle":"2025-09-19T10:15:32.957047Z","shell.execute_reply.started":"2025-09-19T10:15:32.936843Z","shell.execute_reply":"2025-09-19T10:15:32.956044Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def Z1_Hyperchaos_Lyapunov_expert(window):\n    nums = sum((window[f'Number{i}'].tolist() for i in range(1,7)), [])\n    arr = np.array(nums[-60:])\n    # Estimate local Lyapunov exponent (sliding window)\n    diffs = np.diff(arr) + 1e-9\n    lyap = np.mean(np.log(np.abs(diffs[1:] / diffs[:-1])))\n    # Predict future state using robust tent/logistic map emulation\n    r = 3.99 if lyap > 0.95 else 3.66\n    last = arr[-6:]/40\n    chaos_pred = r*last*(1-last)\n    pred = [int(x*39)+1 for x in chaos_pred]\n    while len(pred) < 6:\n        pred.append(s_randint(1,39))\n    conf = float(min(0.97, abs(lyap)/1.2))\n    return unique6(pred), conf\n\n\ndef Z2_GrammaticalEvo_ML_expert(window):\n    import torch, torch.nn as nn\n    from hashlib import sha256\n    # Use n-gram hash patterns as features\n    nums = sum((window[f'Number{i}'].tolist() for i in range(1,7)), [])\n    feats = []\n    for i in range(len(nums)-5):\n        chunk = nums[i:i+6]\n        hval = int(sha256(str(chunk).encode()).hexdigest(), 16)\n        feats.append([hval%39+1])\n    feats = torch.tensor(feats[-20:],dtype=torch.float32) if len(feats)>=20 else torch.zeros((20,1))\n    y = torch.abs(feats[:-1] - feats[1:])\n    model = nn.Sequential(nn.Linear(1,4), nn.ReLU(), nn.Linear(4,1))\n    opt = torch.optim.Adam(model.parameters(),lr=0.05)\n    for _ in range(6):\n        opt.zero_grad(); o = model(feats[:-1]); loss = ((o-y)**2).mean(); loss.backward(); opt.step()\n    pred = [int(torch.mean(model(torch.tensor([[i+1]],dtype=torch.float32))).item()) for i in range(6)]\n    return unique6(pred), float(min(0.93, 1-loss.item()))\n\n\ndef Z3_XORMixnet_Reverse_expert(window):\n    nums = sum((window[f'Number{i}'].tolist() for i in range(1,7)), [])\n    arr = np.array(nums)\n    xored = arr.copy()\n    for i in range(1,len(arr)):\n        xored[i] = xored[i]^xored[i-1]\n    # Try de-xoring\n    rev = [xored[0]]\n    for val in xored[1:]:\n        rev.append(rev[-1]^val)\n    pred = [int((rev[-1]+i*7)%39 + 1) for i in range(6)]\n    conf = 0.80 + 0.1*(np.std(xored)/np.max(arr))\n    return unique6(pred), float(min(0.96, conf))\n\n\ndef Z4_GenieAI_MetaEnsemble_expert(window, registry=None, diversity_boost=0.15):\n    import numpy as np\n    if registry is None:\n        registry = expert_registry\n    valid_experts = [name for name, cfg in registry.items() if len(window) >= cfg['min_window']]\n    preds, confs = [], []\n    for name in valid_experts[:8]:\n        fn = globals().get(registry[name]['func'], fallback)\n        p, c = fn(window)\n        preds.append(unique6(p)); confs.append(c)\n    # Use evolutionary logic: batch generation, diversity scoring\n    preds_arr = np.array(preds)\n    disagreement = np.mean([\n        len(set(preds_arr[i]) ^ set(preds_arr[j])) for i in range(len(preds_arr)) for j in range(i+1, len(preds_arr))\n    ]) / 6.0 if len(preds_arr)>1 else 1.0\n    # Weighted sum, bias to most diverse combos\n    weights = np.array(confs) * (1 + diversity_boost*disagreement)\n    weights = weights / weights.sum()\n    agg = np.zeros(6)\n    for i in range(len(preds)):\n        agg += weights[i]*np.array(preds[i])\n    return unique6(agg), float(np.clip(np.mean(confs)* (1+0.2*disagreement), 0.5, 0.99))\n\n\ndef Z5_ProbabilisticNeural_Circuit_expert(window):\n    import torch\n    from torch import nn\n    # Flatten and normalize draw\n    seq = sum((window[f'Number{i}'].tolist() for i in range(1,7)), [])\n    seq = (np.array(seq[-48:])-20)/20 if len(seq)>=48 else (np.array(seq)-20)/20\n    X = torch.tensor(seq, dtype=torch.float32).view(-1,6)\n    # Simple neural circuit: skip connections, multiplicative attention\n    class NeuralCircuit(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.in1 = nn.Linear(6, 16)\n            self.in2 = nn.Linear(16, 16)\n            self.attn = nn.Linear(16, 6)\n        def forward(self, x):\n            x = torch.relu(self.in1(x))\n            skip = x\n            x = torch.relu(self.in2(x))\n            x = x * torch.sigmoid(self.attn(skip))\n            out = x.mean(dim=0) * 20 + 20\n            return out\n    model = NeuralCircuit()\n    yhat = model(X)\n    pred = [int(np.clip(val.item(), 1, 39)) for val in yhat]\n    conf = float(min(0.97, 1-np.std(pred)/33))\n    return unique6(pred), conf\n\n\n\ndef Z6_Adversarial_PhaseWatcher_expert(window):\n    seq = sum((window[f'Number{i}'].tolist() for i in range(1,7)), [])\n    arr = np.array(seq)\n    # Sliding window FFT on windowed statistics to catch phase shifts\n    swsz = 24\n    phases = []\n    for i in range(len(arr)-swsz):\n        win = arr[i:i+swsz]\n        fft = np.abs(np.fft.fft(win-np.mean(win)))\n        phases.append(np.max(fft))\n    # If a big change in max-fft recently, AI-RNG may have shifted strategy\n    diffs = np.diff(phases)\n    sudden = np.argmax(np.abs(diffs[-6:])) if len(diffs) >= 6 else 0\n    shift_base = arr[-6+sudden:][-6:] if len(arr) >= 12 else arr[-6:]\n    pred = [int(x) for x in shift_base]\n    return unique6(pred), float(0.75 + 0.2*np.std(diffs)/max(1,np.mean(phases)))\n\n\n\ndef Z7_SparseDict_Explain_expert(window):\n    # Use sklearn for dictionary learning\n    from sklearn.decomposition import DictionaryLearning\n    seq = sum((window[f'Number{i}'].tolist() for i in range(1,7)), [])\n    arr = np.array(seq[-60:]).reshape(-1,6)\n    model = DictionaryLearning(n_components=6, alpha=1.0, max_iter=30, random_state=42)\n    try:\n        codes = model.fit_transform(arr)\n        rec = model.inverse_transform(codes[-1:])\n        pred = [int(np.clip(x,1,39)) for x in rec.flatten()]\n        err = np.mean(np.abs(arr[-1]-rec))\n        conf = float(0.9-np.clip(err/20,0,0.7))\n    except Exception:\n        pred, conf = fallback(window), 0.45\n    return unique6(pred), conf\n\n\ndef Z8_ExplorationPolicy_AI_RNG_expert(window):\n    # Policy net explores unique combo distributions\n    import torch\n    import torch.nn as nn\n    seq = sum((window[f'Number{i}'].tolist() for i in range(1,7)), [])\n    seq = np.array(seq[-60:])\n    X = torch.tensor(seq[:-6].reshape(-1,6), dtype=torch.float32)\n    y = torch.tensor(seq[6:].reshape(-1,6), dtype=torch.float32)\n    class Policy(nn.Module):\n        def __init__(self): super().__init__()\n        def forward(self,x): return x.mean(dim=0)\n    p = Policy(); # Simple for robustness\n    out = p(X)\n    reward = 1-np.var(out.detach().numpy())/39\n    pred = [int(np.clip(x,1,39)) for x in out.detach().numpy()]\n    return unique6(pred), float(0.65+reward*0.3)\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-19T10:15:32.958092Z","iopub.execute_input":"2025-09-19T10:15:32.958339Z","iopub.status.idle":"2025-09-19T10:15:32.994142Z","shell.execute_reply.started":"2025-09-19T10:15:32.95831Z","shell.execute_reply":"2025-09-19T10:15:32.993066Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def Z9_SideChannel_Leakage_expert(window):\n    import time, numpy as np\n    nums = sum((window[f'Number{i}'].tolist() for i in range(1,7)), [])\n    variants = []\n    for jitter in np.linspace(-0.25, 0.25, 6):\n        start = time.time()\n        base = np.array(nums) + jitter\n        np.fft.fft(base)\n        variants.append(time.time() - start)\n    stddev = np.std(variants)\n    pos = int((stddev*1000) % 39) + 1\n    pred = [(pos + i*3) % 39 + 1 for i in range(6)]\n    return unique6(pred), float(min(0.97, stddev * 11))\n\n\ndef Z10_QuantumEigen_Drift_expert(window):\n    nums = sum((window[f'Number{i}'].tolist() for i in range(1,7)), [])\n    arr = np.array(nums[-78:])  # Use last 13 draws\n    if len(arr) < 18: return fallback(window), 0.4\n    mat = np.zeros((39,39))\n    for i in range(len(arr)-1):\n        mat[arr[i]-1, arr[i+1]-1] += 1\n    vals = np.linalg.eigvalsh(mat)\n    pred = [int(np.abs(vals[-i]) % 39)+1 for i in range(1,7)]\n    conf = float(min(0.95, np.ptp(vals)/np.std(arr)))\n    return unique6(pred), conf\n\n\ndef advanced_self_attention_gate(window, registry, last_meta_conf=None):\n    import numpy as np\n    # Parallel, as before\n    expert_results = run_all_experts_parallel(window, registry)\n    preds, confs = {}, {}\n    for name, (pred, conf) in expert_results.items():\n        preds[name] = pred\n        confs[name] = conf\n    # Calculate consensus and diversity\n    consensus = np.mean([len(set(preds[e1]) & set(preds[e2]))/6\n                         for e1 in preds for e2 in preds if e1 != e2])\n    entropy = -np.sum([(c/(sum(confs.values()) or 1)) * np.log2((c/(sum(confs.values()) or 1))+1e-9)\n                      for c in confs.values()])\n    # Drift detection: boost experts if consensus drops or entropy dips\n    weights = np.array([c for c in confs.values()])\n    if last_meta_conf and consensus < last_meta_conf['consensus']:\n        weights = weights * 1.20\n    # Softmax\n    w = np.exp(weights-np.max(weights))\n    w /= w.sum()\n    out = np.zeros(6)\n    names = list(preds)\n    for i in range(len(names)):\n        out += w[i]*np.array(unique6(preds[names[i]]))\n    return unique6(out), dict(consensus=consensus, entropy=entropy, weights=dict(zip(names,w)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-19T10:15:32.995222Z","iopub.execute_input":"2025-09-19T10:15:32.995493Z","iopub.status.idle":"2025-09-19T10:15:33.018476Z","shell.execute_reply.started":"2025-09-19T10:15:32.995471Z","shell.execute_reply":"2025-09-19T10:15:33.01757Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Swarm Meta-Orchestrator (Insert above usual driver/orchestrator) ---\nimport numpy as np\nfrom random import SystemRandom\n_secure = SystemRandom()\n\n\ndef meta_swarm_evolution(window, registry, population=10, generations=4):\n    # (1) Generate initial population: Each member a subset of expert names\n    pop = [random.sample(list(registry), k=min(8, len(registry))) for _ in range(population)]\n    weights_hist = []\n    best_pred = None\n    best_score = -np.inf\n    for gen in range(generations):\n        scores = []\n        preds = []\n        # (2) Evaluate each population member in parallel\n        for genes in pop:\n            local_preds, local_confs = [], []\n            for name in genes:\n                fn = globals().get(registry[name]['func'], fallback)\n                pred, conf = fn(window)\n                local_preds.append(unique6(pred))\n                local_confs.append(conf)\n            # Fusion: Confidence-weighted\n            c = np.array(local_confs, dtype=np.float32)\n            w = np.exp(c-np.max(c)); w /= w.sum() or 1\n            agg = sum(w[i]*np.array(local_preds[i]) for i in range(len(w))) / (w.sum() or 1)\n            final = unique6(agg)\n            score = float(np.mean(local_confs)) + 0.1 * len(set.union(*[set(p) for p in local_preds]))/6\n            preds.append(final)\n            scores.append(score)\n            if score > best_score:\n                best_score = score\n                best_pred = final\n        # (3) Selection: keep top half\n        idx = np.argsort(scores)[::-1][:population // 2]\n        selected = [pop[i] for i in idx]\n        # (4) Crossover/mutation: spawn new generation\n        next_gen = []\n        for _ in range(population - len(selected)):\n            p1, p2 = random.sample(selected, 2)\n            cross = list(set(random.choices(p1, k=4) + random.choices(p2, k=4)))\n            # Mutate randomly\n            if random.random() < 0.45:\n                all_names = list(registry) + [name for name in registry if registry[name]['min_window'] < len(window)]\n                cross[random.randrange(len(cross))] = random.choice(all_names)\n            next_gen.append(cross[:min(len(cross), 8)])\n        pop = selected + next_gen\n        weights_hist.append(scores)\n    return best_pred, dict(\n        gen_score=best_score,\n        diversity=np.std(weights_hist),\n        route=\"GA+RL hybrid\"\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-19T10:15:33.019428Z","iopub.execute_input":"2025-09-19T10:15:33.019737Z","iopub.status.idle":"2025-09-19T10:15:33.043036Z","shell.execute_reply.started":"2025-09-19T10:15:33.019717Z","shell.execute_reply":"2025-09-19T10:15:33.041965Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def Z13_NeuralMI_Estimator_expert(window):\n    import torch, torch.nn as nn\n    # Prepare features: flatten the last N draws\n    seq = sum((window[f'Number{i}'].tolist() for i in range(1,7)), [])\n    N = min(len(seq)//6, 32)\n    if N < 6:\n        return fallback(window), 0.4\n    data = np.array(seq)[-N*6:].reshape(N,6)\n    X = torch.tensor(data[:-1], dtype=torch.float32)\n    y = torch.tensor(data[1:], dtype=torch.float32)\n    class SmallNN(nn.Module):\n        def __init__(self): super().__init__()\n        self.fc1 = nn.Linear(6, 32)\n        self.fc2 = nn.Linear(32, 32)\n        self.out = nn.Linear(32, 6)\n        def forward(self, z):\n            z = torch.relu(self.fc1(z))\n            z = torch.relu(self.fc2(z))\n            return self.out(z)\n    net = SmallNN()\n    loss_fn = nn.MSELoss()\n    opt = torch.optim.Adam(net.parameters(), lr=0.02)\n    for i in range(10):\n        out = net(X)\n        loss = loss_fn(out, y)\n        opt.zero_grad(); loss.backward(); opt.step()\n    mi = float(torch.mean((out - y)**2).item())\n    pred = [int(np.clip(v,1,39)) for v in out[-1].detach().numpy()]\n    # Confidence is inverse of error (high MI means more leakage detected)\n    conf = float(np.clip(0.99-mi/50, 0.3, 0.99))\n    return unique6(pred), conf\n\n\nfrom concurrent.futures import ProcessPoolExecutor, as_completed\n\ndef run_all_experts_max_parallel(window, registry):\n    import copy\n    results = {}\n    # NOTE: Use ProcessPoolExecutor where allowed, for heavy compute. For Kaggle, ThreadPoolExecutor if ProcessPool is restricted.\n    with ProcessPoolExecutor(max_workers=16) as executor:\n        futures = {}\n        for name, cfg in registry.items():\n            if len(window) >= cfg['min_window']:\n                # Deep copy window to avoid shared memory issues\n                fn = globals().get(cfg['func'], fallback)\n                futures[executor.submit(fn, copy.deepcopy(window))] = name\n        for future in as_completed(futures):\n            name = futures[future]\n            try:\n                pred, conf = future.result(timeout=60)\n            except Exception as e:\n                pred, conf = fallback(window), 0.3\n            results[name] = (pred, conf)\n    return results\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-19T10:15:33.044153Z","iopub.execute_input":"2025-09-19T10:15:33.044501Z","iopub.status.idle":"2025-09-19T10:15:33.06794Z","shell.execute_reply.started":"2025-09-19T10:15:33.044471Z","shell.execute_reply":"2025-09-19T10:15:33.067059Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def Z20_DeterministicMomentum_Ensemble(window, registry, prev_weights=None, momentum=0.9):\n    # Generate matrix of expert predictions/confidences\n    valid = [(name, cfg) for name, cfg in registry.items() if len(window) >= cfg['min_window']]\n    preds, confs = [], []\n    for name, cfg in valid:\n        fn = globals().get(cfg['func'], fallback)\n        pred, conf = fn(window)\n        preds.append(unique6(pred))\n        confs.append(conf)\n    preds = np.array(preds)\n    confs = np.array(confs)\n    # Initialize or update deterministic ensemble weights\n    if prev_weights is None:\n        weights = np.ones(len(preds)) / len(preds)\n        velocity = np.zeros(len(preds))\n    else:\n        weights, velocity = prev_weights\n    # Deterministic gradient: bias towards higher-confidence, high-diversity\n    grad = confs + np.std(preds, axis=1)\n    velocity = momentum * velocity + grad\n    weights = weights + velocity\n    weights = np.clip(weights, 0, 1)\n    weights = weights / (weights.sum() or 1)\n    # Weighted average ensemble prediction\n    final = np.sum([w * p for w, p in zip(weights, preds)], axis=0) / (weights.sum() or 1)\n    pred = unique6(final)\n    conf = float(np.clip(np.dot(weights, confs), 0.5, 0.99))\n    return pred, (weights, velocity), conf\n\n\ndef Z21_AlgebraicCubeAttack_expert(window, registry):\n    # Extract all expert predictions and flatten\n    valid = [(name, cfg) for name, cfg in registry.items() if len(window) >= cfg['min_window']]\n    cubes = []\n    for name, cfg in valid:\n        fn = globals().get(cfg['func'], fallback)\n        pred, _ = fn(window)\n        cubes.extend(unique6(pred))\n    cubes = np.array(list(set(cubes)))\n    # Apply deterministic extractor: modulo-lattice projection\n    basis = np.array([17, 23, 29, 31, 37, 39])\n    combined = np.mod(cubes.reshape(-1,1) * basis, 39) + 1\n    pred = [int(np.median(col)) for col in combined.T]\n    conf = float(min(0.98, cubes.std() / 39 + 0.18))\n    return unique6(pred), conf\n\n\ndef Z30_SymbolicRegression_expert(window):\n    import numpy as np\n    try:\n        import pysr  # PySR is a fast symbolic regression library for Python\n    except ImportError:\n        return fallback(window), 0.45\n    # Prepare data: flatten window\n    seq = sum((window[f'Number{i}'].tolist() for i in range(1,7)), [])\n    X = np.arange(len(seq)).reshape(-1,1)\n    y = np.array(seq)\n    # Symbolic regression (limited runs for speed)\n    model = pysr.PySRRegressor(niterations=15, binary_operators=[\"+\", \"-\", \"*\", \"/\"], unary_operators=[\"sin\", \"cos\", \"exp\"])\n    try:\n        model.fit(X, y)\n        expr = model.get_best()\n        # Predict next 6 numbers\n        x_pred = np.arange(len(seq), len(seq)+6).reshape(-1,1)\n        yhat = expr(x_pred)\n        pred = [int(np.clip(round(v),1,39)) for v in yhat]\n        conf = float(min(0.98, 1/(1+np.std(yhat))))\n    except Exception:\n        pred, conf = fallback(window), 0.45\n    return unique6(pred), conf\n\n\n\ndef Z31_Disentangled_decomposer_expert(window):\n    import torch, torch.nn as nn, numpy as np\n    seq = sum((window[f'Number{i}'].tolist() for i in range(1,7)), [])\n    N = min(36, len(seq)//6)\n    if N < 6: return fallback(window), 0.4\n    data = np.array(seq)[-N*6:].reshape(N,6)\n    X = torch.tensor(data, dtype=torch.float32)\n    class VAE(nn.Module):\n        def __init__(self): super().__init__()\n        self.enc = nn.Linear(6, 4)\n        self.mu = nn.Linear(4,2)\n        self.logvar = nn.Linear(4,2)\n        self.dec = nn.Linear(2,6)\n        def encode(self, x):\n            h = torch.relu(self.enc(x))\n            return self.mu(h), self.logvar(h)\n        def reparam(self, mu, logvar):\n            std = torch.exp(0.5*logvar)\n            eps = torch.zeros_like(std)\n            return mu + std * eps\n        def decode(self, z): return self.dec(z)\n        def forward(self, x):\n            mu, logvar = self.encode(x)\n            z = self.reparam(mu, logvar)\n            return self.decode(z)\n    vae = VAE()\n    opt = torch.optim.Adam(vae.parameters(), lr=0.04)\n    for _ in range(7):\n        opt.zero_grad()\n        recon = vae(X)\n        loss = ((recon-X)**2).mean()\n        loss.backward(); opt.step()\n    z, _ = vae.encode(X[-1])\n    out = vae.decode(z)\n    pred = [int(np.clip(v.item(),1,39)) for v in out]\n    conf = float(min(0.98, 0.7+0.3/(1+loss.item())))\n    return unique6(pred), conf\n\n\ndef Z32_Physics_Inspired_NN_expert(window):\n    import torch, torch.nn as nn, numpy as np\n    seq = sum((window[f'Number{i}'].tolist() for i in range(1,7)), [])\n    N = min(36, len(seq)//6)\n    if N < 7: return fallback(window), 0.4\n    X = np.linspace(0,1,N).reshape(-1,1).astype(np.float32)\n    y = np.array(seq)[-N*6:].reshape(N,6).astype(np.float32)\n    X_t = torch.from_numpy(X)\n    y_t = torch.from_numpy(y)\n    class PINN(nn.Module):\n        def __init__(self): super().__init__()\n        self.linear1 = nn.Linear(1,16)\n        self.linear2 = nn.Linear(16,32)\n        self.linear3 = nn.Linear(32,6)\n        def forward(self, x):\n            x1 = torch.sin(self.linear1(x))\n            x2 = torch.relu(self.linear2(x1))\n            return self.linear3(x2)\n    net = PINN()\n    optimizer = torch.optim.Adam(net.parameters(), lr=0.03)\n    for _ in range(8):\n        optimizer.zero_grad()\n        y_pred = net(X_t)\n        loss = ((y_pred-y_t)**2).mean()\n        loss.backward(); optimizer.step()\n    next_X = torch.from_numpy(np.linspace(1,1.05,6).reshape(-1,1).astype(np.float32))\n    out = net(next_X).detach().numpy().flatten()\n    pred = [int(np.clip(v,1,39)) for v in out]\n    conf = float(min(0.97, 0.65+0.3/(1+loss.item())))\n    return unique6(pred), conf\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-19T10:15:33.068871Z","iopub.execute_input":"2025-09-19T10:15:33.069134Z","iopub.status.idle":"2025-09-19T10:15:33.097357Z","shell.execute_reply.started":"2025-09-19T10:15:33.069116Z","shell.execute_reply":"2025-09-19T10:15:33.096487Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def Z40_VonNeumannExtractor_expert(window):\n    nums = sum((window[f'Number{i}'].tolist() for i in range(1,7)), [])\n    bits = \"\".join(f\"{n:06b}\" for n in nums)\n    pairs = [bits[i:i+2] for i in range(0,len(bits)-1,2)]\n    unbiased = [p[0] for p in pairs if len(p)==2 and p[0]!=p[1]]\n    if not unbiased: return fallback(window), 0.3\n    vals = [int(\"\".join(unbiased[i:i+6]),2)%39+1 for i in range(0, len(unbiased)-5, 6)] or [s_randint(1,39) for _ in range(6)]\n    pred = unique6(vals)\n    conf = float(min(0.99, len(unbiased)/len(bits)))\n    return pred, conf\n\n\ndef Z41_MultiBatteryStatTestExpert(window):\n    import numpy as np\n    seq = sum((window[f'Number{i}'].tolist() for i in range(1,7)), [])\n    # frequency test\n    freq = np.mean(seq)\n    runs = np.sum(np.diff(seq)!=0)\n    acorr = np.corrcoef(seq[:-1], seq[1:])[0,1] if len(seq)>1 else 0\n    # combine into joint anomaly score\n    score = ((abs(freq-20)/19 + abs(runs-len(seq)/2)/(len(seq)/2) + abs(acorr))/3)\n    pred = [1+int(38*score + i)%39 for i in range(6)]\n    return unique6(pred), float(np.clip(1-score,0.35,0.97))\n\n\ndef Z42_MT19937_FingerprintExpert(window):\n    # Detect regularity/missing high/low bits in output typically present in AI or MT19937 PRNGs\n    nums = sum((window[f'Number{i}'].tolist() for i in range(1,7)), [])\n    vals = np.array(nums)\n    lsb = vals & 1\n    msb = (vals >> 5) & 1\n    bias = abs(lsb.mean() - msb.mean())\n    pred = [int(v) for v in np.clip(vals[-6:]+int(10*bias),1,39)]\n    return unique6(pred), float(min(0.99, 1/(1+bias)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-19T10:15:33.09855Z","iopub.execute_input":"2025-09-19T10:15:33.098911Z","iopub.status.idle":"2025-09-19T10:15:33.119871Z","shell.execute_reply.started":"2025-09-19T10:15:33.098883Z","shell.execute_reply":"2025-09-19T10:15:33.118907Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef to_device(tensor):\n    return tensor.to(DEVICE) if isinstance(tensor, torch.Tensor) else tensor\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-19T10:15:33.120722Z","iopub.execute_input":"2025-09-19T10:15:33.120984Z","iopub.status.idle":"2025-09-19T10:15:33.142959Z","shell.execute_reply.started":"2025-09-19T10:15:33.120965Z","shell.execute_reply":"2025-09-19T10:15:33.14206Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from concurrent.futures import ThreadPoolExecutor, as_completed\n\ndef run_all_experts_parallel(window, registry):\n    results = {}\n    with ThreadPoolExecutor(max_workers=min(32, len(registry))) as executor:\n        futures = {}\n        for name, cfg in registry.items():\n            if len(window) >= cfg['min_window']:\n                fn = globals().get(cfg['func'], fallback)\n                futures[executor.submit(fn, window)] = name\n        for future in as_completed(futures):\n            name = futures[future]\n            try:\n                pred, conf = future.result(timeout=22)\n            except Exception as e:\n                pred, conf = fallback(window), 0.3\n            results[name] = (pred, conf)\n    return results\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-19T10:15:33.144039Z","iopub.execute_input":"2025-09-19T10:15:33.144377Z","iopub.status.idle":"2025-09-19T10:15:33.1619Z","shell.execute_reply.started":"2025-09-19T10:15:33.144351Z","shell.execute_reply":"2025-09-19T10:15:33.16091Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Quick cleanup - run this before each new orchestrator run\nimport os, glob\n[os.remove(f) for pattern in [\"/kaggle/working/*.json\", \"/kaggle/working/*.pkl\", \"/kaggle/working/*.csv\"] \n for f in glob.glob(pattern) \n if any(x in f for x in [\"dharma_\", \"apex_\", \"breakthrough\", \"comprehensive\", \"validation\", \"insights\", \"analytics\", \"ultimate\", \"state\"]) \n and os.path.exists(f)]\nprint(\"ðŸ§¹ All DHARMA_X orchestrator data cleared!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-19T10:15:33.162786Z","iopub.execute_input":"2025-09-19T10:15:33.163059Z","iopub.status.idle":"2025-09-19T10:15:33.186211Z","shell.execute_reply.started":"2025-09-19T10:15:33.163039Z","shell.execute_reply":"2025-09-19T10:15:33.185151Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 0. ABSOLUTE WARNING / LOG SUPPRESSION  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nimport os, sys, warnings, logging\nfrom contextlib import contextmanager\nfrom io import StringIO\nimport datetime as dt\n\nos.environ.update({\n    \"PYTHONWARNINGS\": \"ignore\",\n    \"TF_CPP_MIN_LOG_LEVEL\": \"3\",\n    \"LIGHTGBM_VERBOSITY\": \"-1\",\n    \"LIGHTGBM_LOG_LEVEL\": \"OFF\",\n})\nwarnings.filterwarnings(\"ignore\")\nwarnings.simplefilter(\"ignore\")\nlogging.getLogger().setLevel(logging.CRITICAL)\n\nfor _lg in [\"lightgbm\", \"statsmodels\", \"sklearn\", \"matplotlib\", \"seaborn\"]:\n    l = logging.getLogger(_lg)\n    l.setLevel(logging.CRITICAL)\n    l.propagate = False\n    l.disabled = True\n\n@contextmanager\ndef absolute_silence():\n    \"\"\"Suppress C / Fortran stdout+stderr (e.g., sklearn, lightgbm, etc.).\"\"\"\n    old_stdout, old_stderr = os.dup(1), os.dup(2)\n    devnull = os.open(os.devnull, os.O_WRONLY)\n    try:\n        os.dup2(devnull, 1)\n        os.dup2(devnull, 2)\n        yield\n    finally:\n        os.dup2(old_stdout, 1)\n        os.dup2(old_stderr, 2)\n        os.close(old_stdout); os.close(old_stderr); os.close(devnull)\n\n# 1. CORE IMPORTS  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nimport math, json, pickle, secrets, traceback, numpy as np, pandas as pd\nfrom datetime import datetime\nfrom random import SystemRandom\nfrom collections import Counter, defaultdict, deque\nfrom scipy.stats import entropy as scipy_entropy\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.ensemble import RandomForestRegressor\nfrom scipy import stats\nimport glob\n\n\n# â”€â”€ LOAD 872 ELITE EXPERT COMBINATIONS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nimport pandas as pd\nimport ast\n\ndef load_elite_combinations():\n    \"\"\"Load 872 elite expert combinations from analysis\"\"\"\n    df = pd.read_csv('/kaggle/input/dharma-synergy/dharma_expert_synergy_analysis.csv')\n    elite_combos = []\n    \n    for _, row in df.iterrows():\n        if float(row['success_rate_percent']) >= 20.0:\n            combo = ast.literal_eval(row['expert_combination'])\n            elite_combos.append(combo)\n    \n    print(f\"ðŸŽ¯ Loaded {len(elite_combos)} elite combinations (â‰¥20% success rate)\")\n    return elite_combos\n\n# Load at startup\nELITE_COMBOS = load_elite_combinations()\nRESOURCE_ALLOCATION = {'elite': 0.85, 'explore': 0.15}\n\n# === BEGIN: SEGMENT SUMMARY COLLECTOR (SCREEN ONLY) ===\nfrom collections import Counter\nfrom typing import List, Optional\n\n# === BEGIN: SEGMENT SUMMARY COLLECTOR (Topâ€‘6 vs Target directly under frequency) ===\nfrom typing import List, Optional\n\nclass SegmentSummaryCollector:\n    \"\"\"\n    Per-segment summaries (screen only):\n      - Frequency of predicted numbers (1..39) across all iterations\n      - Distribution of exact hit counts (0..6)\n      - Matrix by actual drawn numbers: rows = the six actuals, cols = 0/6..6/6 + Total\n      - Topâ€‘6 by frequency vs target match score (printed immediately under frequency)\n    \"\"\"\n    __slots__ = (\n        \"_num_counts\", \"_hit_bucket\", \"_total_predictions\",\n        \"_actual_rows\", \"_per_actual\", \"_min_k\"\n    )\n\n    def __init__(self, min_k_include: int = 0) -> None:\n        self._num_counts = [0] * 40           # 1..39 frequency\n        self._hit_bucket = [0] * 7            # 0..6 exact-hit histogram\n        self._total_predictions = 0\n        self._actual_rows: List[int] = []     # target numbers for this segment\n        self._per_actual: dict[int, List[int]] = {}\n        self._min_k = int(min_k_include)      # only tally iterations with k >= min_k\n\n    def set_actuals(self, actual_numbers: List[int]) -> None:\n        self._actual_rows = [int(x) for x in actual_numbers if 1 <= int(x) <= 39][:6]\n        self._per_actual = {a: [0] * 7 for a in self._actual_rows}\n\n    def update(self, prediction_6: List[int], hit_count: int, target_numbers: Optional[List[int]] = None) -> None:\n        if target_numbers is not None and not self._actual_rows:\n            self.set_actuals(target_numbers)\n\n        k = int(hit_count)\n        if not (0 <= k <= 6):\n            return\n        if k < self._min_k:\n            return\n\n        # 1..39 frequency\n        for n in prediction_6:\n            n_int = int(n)\n            if 1 <= n_int <= 39:\n                self._num_counts[n_int] += 1\n\n        # Segment distribution\n        self._hit_bucket[k] += 1\n\n        # Per-actual matrix\n        if self._actual_rows:\n            pred_set = {int(x) for x in prediction_6}\n            for a in self._actual_rows:\n                if a in pred_set:\n                    self._per_actual[a][k] += 1\n\n        self._total_predictions += 1\n\n    def _compute_frequency_pairs(self) -> List[tuple]:\n        pairs = [(n, self._num_counts[n]) for n in range(1, 40)]\n        pairs.sort(key=lambda x: (-x[1], x[0]))  # desc by count, then asc by number\n        return pairs\n\n    def _print_number_frequency(self, pairs: List[tuple]) -> None:\n        print(\"\\nPredicted number frequency (1..39) â€“ descending by count:\")\n        print(\"Number | Count\")\n        print(\"------ | -----\")\n        for n, c in pairs:\n            print(f\"{n:>6} | {c:>5}\")\n\n    def _print_top6_vs_target(self, pairs: List[tuple]) -> None:\n        \"\"\"\n        Take topâ€‘6 by frequency and compare with target; print match score and overlap\n        directly under the frequency table.\n        \"\"\"\n        # Extract topâ€‘6 numbers (ties broken by smaller number first)\n        top6 = [n for (n, c) in pairs[:6]]\n        if not self._actual_rows or len(top6) < 6:\n            return\n        top6_set = set(top6)\n        target_set = set(self._actual_rows)\n        overlap = sorted(top6_set & target_set)\n        k = len(overlap)\n        print(\"\\nTopâ€‘6 by frequency vs Target:\")\n        print(f\"  Topâ€‘6: {sorted(top6)}\")\n        print(f\"  Target: {sorted(self._actual_rows)}\")\n        print(f\"  Match score: {k}/6\")\n        print(f\"  Overlap: {overlap if overlap else 'None'}\")\n\n    def _print_actual_positions(self, pairs: list[tuple[int, int]]) -> None:\n        \"\"\"\n        Print the position (1-based rank) and count of each target number\n        in the descending frequency list (pairs), preserving draw order.\n        pairs: list of (number, count) sorted by (-count, number)\n        \"\"\"\n        if not self._actual_rows:\n            return\n\n        # Build rank map from pairs\n        rank_map: dict[int, int] = {}\n        count_map: dict[int, int] = {}\n        for idx, (n, c) in enumerate(pairs, start=1):\n            rank_map[n] = idx\n            count_map[n] = c\n\n        print(\"\\nPositions of target numbers in frequency list (1 = highest):\")\n        print(\"Number | Rank | Count\")\n        print(\"------ | ---- | -----\")\n        # Keep the target draw order\n        for a in self._actual_rows:\n            r = rank_map.get(a)\n            c = count_map.get(a, 0)\n            if r is None:\n                print(f\"{a:>6} |  N/A | {c:>5}\")\n            else:\n                print(f\"{a:>6} | {r:>4} | {c:>5}\")\n\n\n    def _print_segment_distribution(self) -> None:\n        print(\"\\nExact hit distribution (k/6) for this segment:\")\n        cols = [f\"{k}/6\" for k in range(0, 7)]\n        counts = [self._hit_bucket[k] for k in range(0, 7)]\n        total = sum(counts)\n        print(\" | \".join([f\"{col:>4}\" for col in cols] + [\"Total\"]))\n        print(\"-+-\".join([\"----\"] * (len(cols) + 1)))\n        print(\" | \".join([f\"{v:>4}\" for v in counts] + [f\"{total}\"]))\n\n    def _print_per_actual_matrix(self) -> None:\n        if not self._actual_rows:\n            return\n        print(\"\\nActual Numbers â€“ exact hit distribution by contained predictions:\")\n        header_cols = [f\"{k}/6\" for k in range(0, 7)] + [\"Total\"]\n        print(f\"{'Actual Numbers':>14} | \" + \" | \".join([f\"{h:>4}\" for h in header_cols]))\n        print(\"-\" * 14 + \"-+-\" + \"-+-\".join([\"----\"] * len(header_cols)))\n        for a in self._actual_rows:\n            row = self._per_actual.get(a, [0] * 7)\n            total = sum(row)\n            cells = \" | \".join([f\"{v:>4}\" for v in row] + [f\"{total}\"])\n            print(f\"{str(a):>14} | {cells}\")\n\n    def print_summary(self, segment_label: Optional[str] = None, period_label: Optional[str] = None) -> None:\n        \"\"\"\n        Print, in order:\n          1) 1..39 number frequency (descending)\n          2) Topâ€‘6 vs Target match (directly under frequency)\n          3) Segment exact-hit distribution (single row)\n          4) Per-actual 6Ã—8 matrix\n        \"\"\"\n        header = \"SEGMENT SUMMARY\"\n        if period_label is not None:\n            header += f\" | Period: {period_label}\"\n        if segment_label is not None:\n            header += f\" | Segment: {segment_label}\"\n        print(\"\\n\" + \"=\" * 80)\n        print(header)\n        print(\"=\" * 80)\n\n        pairs = self._compute_frequency_pairs()\n        self._print_number_frequency(pairs)\n        self._print_top6_vs_target(pairs)      # moved directly under frequency\n        self._print_actual_positions(pairs)\n        self._print_segment_distribution()\n        self._print_per_actual_matrix()\n\n        print(\"\\nIterations (predictions) in this segment:\", self._total_predictions)\n        print(\"=\" * 80)\n# === END: SEGMENT SUMMARY COLLECTOR ===\n\n\n\n\n# 2. SECURE RNG HELPERS  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n_rand = SystemRandom()\ns_int = lambda a, b: secrets.randbelow(b - a + 1) + a\ns_unif = lambda a, b: _rand.uniform(a, b)\ns_pick = lambda seq: seq[secrets.randbelow(len(seq))]\ns_smpl = lambda seq, k: _rand.sample(seq, k)\n\ndef extract_breakthrough_intelligence(unified_analytics_file):\n    \"\"\"Extract >=4/6 breakthrough intelligence for orchestrator optimization\"\"\"\n    import json\n    import pandas as pd\n    from collections import defaultdict, Counter\n    import numpy as np\n    \n    with open(unified_analytics_file, 'r') as f:\n        data = json.load(f)\n    \n    breakthrough_records = []\n    \n    # Extract all >=4/6 records across ALL periods\n    for period_name, period_data in data['periods'].items():\n        match_counts = period_data.get('match_counts', {})\n        \n        # *** CHANGE: Include 4/6, 5/6, and 6/6 hits ***\n        for hit_level in ['4', '5', '6']:  # <<<--- CHANGED FROM ['5', '6']\n            if hit_level in match_counts:\n                for record in match_counts[hit_level]:\n                    breakthrough_records.append({\n                        'period': period_name,\n                        'hit_level': int(hit_level),\n                        'beta': record['beta'],\n                        'entropy': record['entropy'],\n                        'cycle_position': record['cycle_position'],\n                        'phase': record['phase'],\n                        'iteration': record['iteration'],\n                        'expert_weights': record['expert_weights'],\n                        'active_experts': record['active_experts'],\n                        'weight_concentration': record['weight_concentration'],\n                        'optimal_convergence': record['optimal_convergence'],\n                        'performance_weighted_score': record['performance_weighted_score'],\n                        'prediction': record['prediction'],\n                        'target': record['target']\n                    })\n    \n    if not breakthrough_records:\n        print(\"âŒ No â‰¥4/6 breakthrough records found\")\n        return None\n    \n    print(f\"ðŸŽ¯ Found {len(breakthrough_records)} breakthrough records (â‰¥4/6)\")\n    return analyze_breakthrough_patterns(breakthrough_records)\n\ndef analyze_breakthrough_patterns(breakthrough_records):\n    \"\"\"Analyze breakthrough records to identify optimization targets\"\"\"\n    import numpy as np\n    from collections import defaultdict, Counter\n    \n    analysis = {}\n    \n    # 1. PARAMETER SWEET SPOTS\n    betas = [r['beta'] for r in breakthrough_records]\n    entropies = [r['entropy'] for r in breakthrough_records]\n    convergences = [r['optimal_convergence'] for r in breakthrough_records]\n    \n    analysis['parameter_sweet_spots'] = {\n        'beta_min': min(betas),\n        'beta_max': max(betas),\n        'beta_mean': np.mean(betas),\n        'beta_std': np.std(betas),\n        'entropy_min': min(entropies),\n        'entropy_max': max(entropies),\n        'entropy_mean': np.mean(entropies),\n        'entropy_std': np.std(entropies),\n        'convergence_min': min(convergences),\n        'convergence_max': max(convergences),\n        'convergence_mean': np.mean(convergences)\n    }\n    \n    # 2. EXPERT COMBINATION FREQUENCY\n    expert_combos = Counter()\n    for record in breakthrough_records:\n        combo = tuple(sorted(record['active_experts']))\n        expert_combos[combo] += 1\n    \n    analysis['top_expert_combinations'] = expert_combos.most_common(10)\n    \n    # 3. CYCLE POSITION HOTSPOTS\n    cycle_positions = Counter(r['cycle_position'] for r in breakthrough_records)\n    analysis['cycle_hotspots'] = cycle_positions.most_common(10)\n    \n    # 4. PHASE DISTRIBUTION\n    phases = Counter(r['phase'] for r in breakthrough_records)\n    analysis['phase_distribution'] = dict(phases)\n    \n    # 5. WEIGHT CONCENTRATION ANALYSIS\n    weight_concentrations = [r['weight_concentration'] for r in breakthrough_records]\n    analysis['weight_concentration'] = {\n        'min': min(weight_concentrations),\n        'max': max(weight_concentrations),\n        'mean': np.mean(weight_concentrations),\n        'optimal_threshold': np.percentile(weight_concentrations, 80)\n    }\n    \n    return analysis\n\ndef generate_orchestrator_optimization_recommendations(analysis):\n    \"\"\"Generate specific orchestrator modifications based on breakthrough analysis\"\"\"\n    recommendations = []\n    \n    params = analysis['parameter_sweet_spots']\n    \n    # Beta optimization\n    beta_target = params['beta_mean']\n    beta_tolerance = params['beta_std'] * 0.5\n    recommendations.append({\n        'type': 'parameter_lock',\n        'parameter': 'BETA_TARGET',\n        'value': beta_target,\n        'tolerance': beta_tolerance,\n        'reason': f'â‰¥4/6 success concentrated at Î²={beta_target:.4f}Â±{beta_tolerance:.4f}'\n    })\n    \n    # Entropy optimization  \n    entropy_target = params['entropy_mean']\n    entropy_tolerance = params['entropy_std'] * 0.5\n    recommendations.append({\n        'type': 'parameter_lock',\n        'parameter': 'ENTROPY_TARGET', \n        'value': entropy_target,\n        'tolerance': entropy_tolerance,\n        'reason': f'â‰¥4/6 success concentrated at H={entropy_target:.4f}Â±{entropy_tolerance:.4f}'\n    })\n    \n    # Expert combination forcing\n    top_combos = analysis['top_expert_combinations'][:7]\n    recommendations.append({\n        'type': 'expert_forcing',\n        'parameter': 'BREAKTHROUGH_COMBINATIONS',\n        'value': [list(combo) for combo, _ in top_combos],\n        'reason': f'Top 3 combinations account for {sum(count for _, count in top_combos)} breakthrough successes'\n    })\n    \n    return recommendations\n\n\n# 3. BREAKTHROUGH HYPER-PARAMETERS (â˜… DISCOVERY-BASED â˜…)  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# â•â•â•â•â•â•â•â•â• BREAKTHROUGH INTELLIGENCE CONSTANTS (GLOBAL) â•â•â•â•â•â•â•â•â•\nBETA_TARGET = 0.5075      # Perfect 6/6 beta (was 0.5058)\n#ENTROPY_TARGET = 5.0003   # Perfect 6/6 entropy (was 5.1385)\nBETA_TOLERANCE = 0.0020   # Tighter tolerance (was 0.0141)  \nENTROPY_TOLERANCE = 0.0060 # Tighter tolerance (was 0.0055)\nMIN_HIT_LEVEL = 4\nENTROPY_4_5_TARGET = 5.1442   # Universal for 4/6 and 5/6 (80% of attempts)\nENTROPY_6_TARGET = 5.1444     # Specialized for 6/6 attempts (20% of attempts)\nENTROPY_TARGET = ENTROPY_4_5_TARGET\nBETA_RANGE = [0.5400, 0.5552]       # Universal range present in ALL periods\nENTROPY_RANGE = [5.1301, 5.1442]    # Universal range present in ALL periods\n\n# Strategy mode: 'conservative' (â‰¥4/6 focus), 'balanced' (default), 'aggressive' (â‰¥5/6â€“6/6 push)\n# UNIVERSAL STRATEGY MODE (based on 321,130 record analysis)\nSTRATEGY_MODE = 'universal'  # New mode based on cross-period intelligence\n\n# Balanced allocation based on trend analysis\nFORCE_UNIVERSAL_SHARE = 0.60   # 60% universal patterns (multi-period validated)\nFORCE_EXPLORATION_SHARE = 0.25 # 25% exploration (preserve 4/6-5/6 breadth)\nFORCE_6OF6_SHARE = 0.15        # 15% breakthrough attempts (6/6 specialized)\n\n# Share of iterations locked to 6/6 beta corridor (balanced=0.80, conservative=0.50, aggressive=0.95)\nBETA_LOCK_SHARE = 0.80 if STRATEGY_MODE == 'balanced' else (0.50 if STRATEGY_MODE == 'conservative' else 0.95)\n\n# Hottest cycle positions for â‰¥6/6 (from intelligence)\n# UNIVERSAL CYCLE POSITIONS (29/29 period coverage)\nUNIVERSAL_CYCLE_POSITIONS = [10, 18, 25, 39, 60, 67, 72, 41, 32, 55, 74]  # Perfect coverage\nSECONDARY_CYCLE_POSITIONS = [1, 21, 24]                   # 28/29 period coverage\n\n# Keep legacy 6/6 positions for breakthrough attempts\nHOTTEST_6OF6_POSITIONS  = [10,18,25,39,60]\n\n# Top 6/6 combinations (from intelligence)\n# UNIVERSAL EXPERT COMBINATIONS (23/29 period validated)\nUNIVERSAL_EXPERT_COMBOS = [\n    ['B1_RandomForest_v2','E21_CRT_Reconstructor','G19_Catastrophic_Forgetting'],\n    ['A1_ARIMA','E16_Chaos_Theory','E30_MarkovChain_Residuals'],\n    ['A1_ARIMA_v2','E16_Chaos_Theory','E30_MarkovChain_Residuals'],\n    ['E16_Chaos_Theory','E1_FrequencyAnalysis','E30_MarkovChain_Residuals','E9_Gap_Analysis'],\n    ['E16_Chaos_Theory','E1_FrequencyAnalysis_v2','E30_MarkovChain_Residuals'],\n    ['D1_EnsembleStacking','E16_Chaos_Theory','E1_FrequencyAnalysis']\n]\n\n# Keep 6/6 combos for breakthrough attempts (legacy)\nBREAKTHROUGH_6OF6_COMBOS = [\n    ['D1_EnsembleStacking', 'E16_Chaos_Theory', 'E1_FrequencyAnalysis'],  # 6/6 specific\n    ['B1_RandomForest_v2', 'E21_CRT_Reconstructor', 'G19_Catastrophic_Forgetting'],\n    ['E16_Chaos_Theory', 'E1_FrequencyAnalysis', 'E30_MarkovChain_Residuals']\n]\n\n# Universal cross-hit-level experts (boost weights)\nUNIVERSAL_EXPERT_BOOST = 2.5\nUNIVERSAL_EXPERTS = [\n    'A1_ARIMA','A1_ARIMA_v2','A2_MovingAverage','A2_MovingAverage_v2',\n    'B1_RandomForest','B1_RandomForest_v2','D1_EnsembleStacking',\n    'E16_Chaos_Theory','E1_FrequencyAnalysis','E1_FrequencyAnalysis_v2'\n]\n\n# Signature enforcement (Î²:50 | H:50 dominates 6/6)\nTARGET_SIGNATURE = {'beta_bucket': 50, 'entropy_bucket': 50}\n\n# â”€â”€ Helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndef enforce_breakthrough_signature(beta, entropy):\n    # Nudges beta/entropy toward signature buckets without hard locking\n    if abs(beta * 100 - TARGET_SIGNATURE['beta_bucket']) > 2.5:\n        beta = TARGET_SIGNATURE['beta_bucket'] / 100.0 + s_unif(-0.025, 0.025)\n    if abs(entropy * 10 - TARGET_SIGNATURE['entropy_bucket']) > 2.5:\n        entropy = TARGET_SIGNATURE['entropy_bucket'] / 10.0 + s_unif(-0.025, 0.025)\n    return beta, entropy\n\ndef adaptive_beta_with_6of6_lock(iteration, max_it):\n    # Early: explore within the 6/6 corridor; Late: pin exactly to center\n    if iteration < max_it * BETA_LOCK_SHARE:\n        return BETA_TARGET + s_unif(-BETA_TOLERANCE, BETA_TOLERANCE)\n    return BETA_TARGET\n\ndef force_universal_cycle_positions(iteration, default_cycle_pos):\n    # Use universal positions with 29/29 period coverage\n    if iteration % 4 == 0:  # Every 4th iteration\n        return s_pick(UNIVERSAL_CYCLE_POSITIONS)  # Perfect coverage positions\n    elif iteration % 8 == 0:  # Every 8th iteration  \n        return s_pick(SECONDARY_CYCLE_POSITIONS)  # 28/29 coverage positions\n    elif iteration % 12 == 0:  # Breakthrough attempts\n        return s_pick(HOTTEST_6OF6_POSITIONS)    # Legacy 6/6 positions\n    else:\n        return default_cycle_pos\n\ndef build_6of6_breakthrough_set(preds, scores, st):\n    # 70% chance to use the perfect D1+E16+E1 combo\n    if s_unif(0, 1) < 0.7:\n        combo = ['D1_EnsembleStacking', 'E16_Chaos_Theory', 'E1_FrequencyAnalysis']\n    else:\n        combo = s_pick(BREAKTHROUGH_6OF6_COMBOS)\n    subw = {}\n    for expert in combo:\n        if expert in scores:\n            subw[expert] = scores[expert] * 3.0\n    if not subw:\n        return None, None\n    normalize(subw)\n    agg = np.zeros(6)\n    for expert, w in subw.items():\n        if expert in preds:\n            agg += w * np.array(preds[expert])\n    return unique6(agg), subw\n\ndef build_universal_set(preds, scores, st):\n    combo = s_pick(UNIVERSAL_EXPERT_COMBOS)\n    subw = {e: scores[e]*2.5 for e in combo if e in scores}\n    if not subw: return None, None\n    normalize(subw)\n    agg = sum(subw[e]*np.array(preds[e]) for e in subw)\n    return unique6(agg), subw\n\ndef force_universal_cycle_positions(it, default):\n    if it % 4 == 0:  # universal positions more frequently\n        return s_pick(UNIVERSAL_CYCLE_POSITIONS)\n    if it % 8 == 0:  # secondaries less frequently\n        return s_pick(SECONDARY_CYCLE_POSITIONS)\n    return default\n\ndef adaptive_entropy_targeting(it, max_it):\n    return ENTROPY_6_TARGET if it > max_it*0.9 else ENTROPY_4_5_TARGET\n\ndef beta_dual_anchor(it):\n    # 80% of iterations: center at 0.5075; 20%: bump to 0.5084 to hit the second universal bin\n    if (it % 5) == 0:\n        return 0.5084 + s_unif(-0.0010, 0.0010)\n    return BETA_TARGET + s_unif(-BETA_TOLERANCE, BETA_TOLERANCE)\n\n\n\n# â”€â”€ SUCCESS SIGNATURE WEIGHTS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nSIGNATURE_WEIGHTS = {\n    'signature_1': 0.40,    # Most frequent breakthrough pattern\n    'signature_2': 0.35,    # Highest performance signature  \n    'signature_3': 0.25     # Specialized breakthrough type\n}\n\ndef matches_pattern(signature, prediction):\n    \"\"\"Simple pattern matching for signatures\"\"\"\n    # For now, return True for any prediction to apply boost\n    # You can refine this logic later based on your signature analysis\n    return True\n\n\n# EXPERT HIERARCHY MULTIPLIERS (Based on 5/6 analysis)\nORACLE_EXPERTS = {\n    \"E30_MarkovChain_Residuals\": 6.00,  # The Oracle - 29 appearances\n    \"E16_Chaos_Theory\": 5.00,  # Nonlinear Master - 16 appearances  \n    \"E9_Gap_Analysis\": 4.50,  # Pattern Filler - 13 appearances\n    \"Z3_XORMixnet_Reverse\": 4.00,  # Encryption Breaker - 13 appearances\n}\nBREAKTHROUGH_EXPERTS = {\n    \"E1_FrequencyAnalysis\": 2.80,  # 20 appearances\n    \"Z6_Adversarial_PhaseWatcher\": 2.70,  # 19 appearances\n    \"B1_RandomForest\": 2.65,  # 19 appearances  \n    \"F2_Quantum_ML\": 2.60,  # 19 appearances\n    \"X4_GrangerCausality\": 2.55,  # 19 appearances\n    \"E31_Fractal_Dimension\": 2.50,  # 18 appearances\n}\nSPECIALIST_EXPERTS = {\n    \"E1_FrequencyAnalysis_v2\": 2.25,  # 17 appearances\n    \"E27_Fractal_Pattern\": 2.20,  # 16 appearances\n    \"A1_ARIMA\": 2.10,  # 15 appearances\n    \"E25_Resampling_Chaos\": 2.05,  # 15 appearances\n    \"A1_ARIMA_v2\": 2.00,  # 13 appearances\n}\nSUPPRESSED_EXPERTS = {\n    \"E40_NIST_80022_Battery\": 0.25,\n    \"Z2_GrammaticalEvo_ML\": 0.30,  # Often had 0.000 weights\n}\n\n# ADVANCED PARAMETERS\nCONVERGENCE_BOOST = 3.50    # Boost for entropy convergence\nPATTERN_REINFORCEMENT = 2.75    # Boost successful patterns\nENTROPY_LOCK_STRENGTH = 0.98    # How strongly to enforce entropy target\nBETA_CYCLE_LENGTH = 75      # Longer cycles for convergence\nENTROPY_FORCING_RATIO = 0.95    # % of predictions forced to target entropy\n\n# 4. VALIDATION PERIODS CONFIGURATION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# MONTHLY PREDICTION STRATEGY - 3 MONTHS TO PREDICT 1 MONTH\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\ndef generate_segmented_periods(df):\n    \"\"\"Generate segmented validation periods from data\"\"\"\n    df['Date'] = pd.to_datetime(df['Date'])\n    df_sorted = df.sort_values('Date')\n    month_groups = df_sorted.groupby(df_sorted['Date'].dt.strftime('%Y-%m'))\n    \n    segmented_periods = []\n    \n    for month_str, group in month_groups:\n        dates = list(group['Date'].dt.strftime('%Y-%m-%d'))\n        \n        # Skip months with less than 2 draws\n        if len(dates) < 2:\n            continue\n            \n        # Predict the LAST draw in the month\n        predict_date = dates[-1]\n        \n        # Generate segments: from 1 draw up to (total_draws - 1)\n        for seg_idx in range(1, len(dates)):\n            training_end_idx = len(dates) - 2  # penultimate draw index\n            training_start_idx = training_end_idx - (seg_idx - 1)  # expand backward\n            \n            if training_start_idx < 0:\n                training_start_idx = 0\n            \n            # Get training window\n            training_dates = dates[training_start_idx:training_end_idx + 1]\n            \n            period_config = {\n                \"name\": f\"{month_str}_Seg{seg_idx}\",\n                \"month\": month_str,\n                \"segment\": seg_idx,\n                \"training_dates\": training_dates,\n                \"predict_date\": predict_date,\n                \"train_end\": training_dates[-1]  # last training date\n            }\n            \n            segmented_periods.append(period_config)\n    \n    return segmented_periods\n\n# Initialize empty - will be populated when data is loaded\nVALIDATION_PERIODS = []\n\n\n# 5. EXPERT REGISTRY WITH BREAKTHROUGH OPTIMIZATION  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nexpert_registry = {\n    # ORACLE TIER - The proven winners\n    'E30_MarkovChain_Residuals': dict(min_window=20, func='E30_MarkovChain_Residuals_expert'),\n    'E16_Chaos_Theory': dict(min_window=20, func='E16_Chaos_Theory_expert'),\n    'E9_Gap_Analysis': dict(min_window=10, func='E9_Gap_Analysis_expert'),\n    'Z3_XORMixnet_Reverse': dict(min_window=40, func='Z3_XORMixnet_Reverse_expert'),\n    \n    # BREAKTHROUGH TIER - High frequency winners\n    'E1_FrequencyAnalysis': dict(min_window=10, func='E1_FrequencyAnalysis_expert'),\n    'Z6_Adversarial_PhaseWatcher': dict(min_window=30, func='Z6_Adversarial_PhaseWatcher_expert'),\n    'B1_RandomForest': dict(min_window=15, func='B1_RandomForest_expert'),\n    'F2_Quantum_ML': dict(min_window=10, func='F2_Quantum_ML_expert'),\n    'X4_GrangerCausality': dict(min_window=15, func='X4_GrangerCausality_expert'),\n    'E31_Fractal_Dimension': dict(min_window=25, func='E31_Fractal_Dimension_expert'),\n    \n    # SPECIALIST TIER - Consistent contributors\n    'E1_FrequencyAnalysis_v2': dict(min_window=6, func='E1_FrequencyAnalysis_expert_v2'),\n    'E27_Fractal_Pattern': dict(min_window=10, func='E27_Fractal_Pattern_expert'),\n    'A1_ARIMA': dict(min_window=10, func='A1_ARIMA_expert'),\n    'E25_Resampling_Chaos': dict(min_window=15, func='E25_Resampling_Chaos_expert'),\n    'A1_ARIMA_v2': dict(min_window=6, func='A1_ARIMA_expert_v2'),\n    \n    # ENHANCED SPECIALISTS\n    'G19_Catastrophic_Forgetting': dict(min_window=12, func='G19_Catastrophic_Forgetting_expert'),\n    'Z32_Physics_Inspired_NN': dict(min_window=36, func='Z32_Physics_Inspired_NN_expert'),\n    'B1_RandomForest_v2': dict(min_window=6, func='B1_RandomForest_expert_v2'),\n    \n    # CORE CONTRIBUTORS\n    'X1_TakensEmbedding': dict(min_window=20, func='X1_TakensEmbedding_expert'),\n    'X2_EntropyBoundary': dict(min_window=15, func='X2_EntropyBoundary_expert'),\n    'X5_AdvTraining': dict(min_window=20, func='X5_AdversarialTraining_expert'),\n    'F16_Multi_Armed_Bandits': dict(min_window=8, func='F16_Multi_Armed_Bandits_expert'),\n    'D1_EnsembleStacking': dict(min_window=20, func='D1_EnsembleStacking_expert'),\n    'E20_Fourier_Residue': dict(min_window=12, func='E20_Fourier_Residue_expert'),\n    'E22_Differential_Entropy': dict(min_window=12, func='E22_Differential_Entropy_expert'),\n    'E32_Poisson_Process': dict(min_window=15, func='E32_Poisson_Process_expert'),\n    'F33_Ghost_Factorization': dict(min_window=30, func='F33_Ghost_Factorization_expert'),\n    'G35_Meta_Reinforcement': dict(min_window=20, func='G35_Meta_Reinforcement_expert'),\n    'G36_Evolutionary_Ensemble': dict(min_window=20, func='G36_Evolutionary_Ensemble_expert'),\n    'Z1_Hyperchaos_Lyapunov': dict(min_window=40, func='Z1_Hyperchaos_Lyapunov_expert'),\n    'Z5_ProbabilisticNeural_Circuit': dict(min_window=24, func='Z5_ProbabilisticNeural_Circuit_expert'),\n    'Z10_QuantumEigen_Drift': dict(min_window=24, func='Z10_QuantumEigen_Drift_expert'),\n    'Z30_SymbolicRegression': dict(min_window=36, func='Z30_SymbolicRegression_expert'),\n    'Z31_Disentangled_decomposer': dict(min_window=36, func='Z31_Disentangled_decomposer_expert'),\n    'Z40_VonNeumannExtractor': dict(min_window=24, func='Z40_VonNeumannExtractor_expert'),\n    'Z42_MT19937_Fingerprint': dict(min_window=24, func='Z42_MT19937_FingerprintExpert'),\n    \n    # SUPPORTING TIER\n    'A2_MovingAverage': dict(min_window=5, func='A2_MovingAverage_expert'),\n    'A2_MovingAverage_v2': dict(min_window=6, func='A2_MovingAverage_expert_v2'),\n    'F28_KSTest': dict(min_window=12, func='F28_KSTest_expert'),\n    'E21_CRT_Reconstructor': dict(min_window=15, func='E21_CRT_Reconstructor_expert'),\n    'G50_Neural_Discriminator': dict(min_window=20, func='G50_Neural_Discriminator_expert'),\n    'X10_GraphEntropy_SubGraph': dict(min_window=20, func='X10_GraphEntropy_SubGraph_expert'),\n    \n    # CONTROLLED SUPPRESSED TIER\n    'E40_NIST_80022_Battery': dict(min_window=20, func='E40_NIST_80022_Battery_expert'),\n    'Z2_GrammaticalEvo_ML': dict(min_window=40, func='Z2_GrammaticalEvo_ML_expert'),\n}\n\n# â”€â”€ ADJUST MIN_WINDOW TO FIT SMALL TRAINING SETS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# Cap every expertâ€™s min_window to 6 for 40â€“50 draws\nfor cfg in expert_registry.values():\n    cfg['min_window'] = min(cfg['min_window'], 1)  # SUPPORT 1-DRAW SEGMENTS\n\n\n# 6. COMPREHENSIVE ANALYTICS COLLECTOR â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nclass UnifiedAnalyticsCollector:\n    \"\"\"Complete unified collector with all AdvancedAnalyticsCollector functionality\"\"\"\n    \n    def __init__(self):\n        self.unified_data = {\n            \"metadata\": {\n                \"collection_start\": datetime.now().isoformat(),\n                \"total_periods\": len(VALIDATION_PERIODS),\n                \"threshold\": 3,\n                \"system_version\": \"DHARMA_X_PHASE_6_COMPLETE\"\n            },\n            \"periods\": {}\n        }\n    \n    def initialize_period(self, period_name, train_end, predict_date, actual_target):\n        \"\"\"Initialize period structure\"\"\"\n        self.unified_data[\"periods\"][period_name] = {\n            \"metadata\": {\n                \"train_end\": train_end,\n                \"predict_date\": predict_date,\n                \"actual_target\": actual_target,\n                \"collection_start\": datetime.now().isoformat()\n            },\n            \"match_counts\": {\n                #\"2\": [], \n                #\"3\": [], \n                \"4\": [], \"5\": [], \"6\": []\n            },\n            \"statistics\": {\n                \"total_predictions\": 0,\n                \"collected_records\": 0,\n                \"hit_distribution\": Counter()\n            }\n        }\n    \n    def collect_record(self, period_name, hit_count, iteration, prediction, target,\n                      template, entropy, beta, phase, cycle_position,\n                      expert_weights, expert_performances):\n        \"\"\"Collect complete record with ALL original AdvancedAnalyticsCollector fields\"\"\"\n        \n        if hit_count < 3:\n            return\n        \n        matches = list(set(prediction) & set(target))\n        misses = list(set(target) - set(prediction))\n        extras = list(set(prediction) - set(target))\n        \n        # Complete record with ALL 78+ fields (identical to original)\n        complete_record = {\n            'validation_period': period_name,\n            'iteration': iteration,\n            'hit_count': hit_count,\n            'prediction': prediction,\n            'target': target,\n            'matches': matches,\n            'misses': misses,\n            'extras': extras,\n            'template': template,\n            'entropy': entropy,\n            'beta': beta,\n            'phase': phase,\n            'cycle_position': cycle_position,\n            'timestamp': datetime.now().isoformat(),\n            \n            # Beta-Entropy Analysis (RESTORED)\n            'beta_entropy_ratio': beta / entropy if entropy > 0 else 0,\n            'beta_deviation': abs(beta - BETA_TARGET),\n            'entropy_deviation': abs(entropy - ENTROPY_TARGET),\n            'optimal_convergence': self._calculate_convergence_score(beta, entropy),\n            \n            # Expert Weight Analysis (RESTORED)\n            'active_experts': list(template.keys()),\n            'expert_weights': dict(template),\n            'top_3_experts': self._get_top_experts(template, 3),\n            'weight_entropy': self._calculate_weight_entropy(template),\n            'weight_concentration': max(template.values()) if template else 0,\n            \n            # Pattern Analysis (RESTORED)\n            'number_gaps': self._analyze_number_gaps(prediction),\n            'number_clusters': self._analyze_number_clusters(prediction),\n            'number_distribution': self._analyze_number_distribution(prediction),\n            'sum_total': sum(prediction),\n            'sum_deviation': abs(sum(prediction) - 117),\n            \n            # Performance Context (RESTORED)\n            'expert_performances': dict(expert_performances),\n            'performance_weighted_score': self._calculate_performance_score(template, expert_performances),\n            \n            # Collection metadata\n            \"collection_timestamp\": datetime.now().isoformat()\n        }\n        \n        # Store under period â†’ hit_count structure\n        hit_str = str(hit_count)\n        self.unified_data[\"periods\"][period_name][\"match_counts\"][hit_str].append(complete_record)\n        \n        # Update statistics\n        stats = self.unified_data[\"periods\"][period_name][\"statistics\"]\n        stats[\"collected_records\"] += 1\n        stats[\"hit_distribution\"][hit_count] += 1\n    \n    # ===== RESTORED HELPER METHODS FROM ORIGINAL =====\n    \n    def _calculate_convergence_score(self, beta, entropy):\n        \"\"\"Calculate convergence to optimal beta-entropy combination\"\"\"\n        beta_score = max(0, 1 - abs(beta - BETA_TARGET) * 10)\n        entropy_score = max(0, 1 - abs(entropy - ENTROPY_TARGET) * 2)\n        return (beta_score + entropy_score) / 2\n    \n    def _get_top_experts(self, template, n):\n        \"\"\"Get top N experts by weight\"\"\"\n        return sorted(template.items(), key=lambda x: x[1], reverse=True)[:n]\n    \n    def _calculate_weight_entropy(self, template):\n        \"\"\"Calculate entropy of expert weight distribution\"\"\"\n        if not template:\n            return 0\n        weights = np.array(list(template.values()))\n        weights = weights / weights.sum()\n        return float(-(weights * np.log2(weights)).sum())\n    \n    def _analyze_number_gaps(self, prediction):\n        \"\"\"Analyze gaps between consecutive numbers\"\"\"\n        sorted_nums = sorted(prediction)\n        return [sorted_nums[i+1] - sorted_nums[i] for i in range(len(sorted_nums)-1)]\n    \n    def _analyze_number_clusters(self, prediction):\n        \"\"\"Analyze number clustering patterns\"\"\"\n        sorted_nums = sorted(prediction)\n        clusters = []\n        current_cluster = [sorted_nums[0]]\n        \n        for i in range(1, len(sorted_nums)):\n            if sorted_nums[i] - sorted_nums[i-1] <= 3:  # Close numbers\n                current_cluster.append(sorted_nums[i])\n            else:\n                clusters.append(current_cluster)\n                current_cluster = [sorted_nums[i]]\n        clusters.append(current_cluster)\n        \n        return {'cluster_count': len(clusters), 'cluster_sizes': [len(c) for c in clusters]}\n    \n    def _analyze_number_distribution(self, prediction):\n        \"\"\"Analyze number distribution across ranges\"\"\"\n        ranges = {'1-10': 0, '11-20': 0, '21-30': 0, '31-39': 0}\n        for num in prediction:\n            if 1 <= num <= 10:\n                ranges['1-10'] += 1\n            elif 11 <= num <= 20:\n                ranges['11-20'] += 1\n            elif 21 <= num <= 30:\n                ranges['21-30'] += 1\n            elif 31 <= num <= 39:\n                ranges['31-39'] += 1\n        return ranges\n    \n    def _calculate_performance_score(self, template, performances):\n        \"\"\"Calculate weighted performance score\"\"\"\n        if not template or not performances:\n            return 0\n        \n        total_score = 0\n        total_weight = 0\n        \n        for expert, weight in template.items():\n            if expert in performances:\n                total_score += weight * performances[expert]\n                total_weight += weight\n        \n        return total_score / total_weight if total_weight > 0 else 0\n    \n    # ===== REST OF UNIFIED COLLECTOR METHODS =====\n    \n    def finalize_period(self, period_name, total_predictions):\n        \"\"\"Finalize period statistics\"\"\"\n        stats = self.unified_data[\"periods\"][period_name][\"statistics\"]\n        stats[\"total_predictions\"] = total_predictions\n        stats[\"collection_end\"] = datetime.now().isoformat()\n        stats[\"collection_rate\"] = stats[\"collected_records\"] / total_predictions * 100\n    \n    def save_unified_file(self, filename=\"dharma_unified_analytics_complete.json\"):\n        \"\"\"Save all data to single structured file\"\"\"\n        self.unified_data[\"metadata\"][\"collection_end\"] = datetime.now().isoformat()\n        \n        with open(filename, 'w') as f:\n            json.dump(self.unified_data, f, indent=2, default=str)\n        \n        return filename\n\n\n# 7. ADVANCED UTILS: ENTROPY-LOCKED, PATTERN-AWARE  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# DHARMA_X BUG FIX - UNIQUE6 FUNCTION\n# Replace the unique6 function with this corrected version\n\ndef unique6(vec):\n    \"\"\"Coerce iterable to six unique ints 1-39 with substitution error prevention.\"\"\"\n    \n    # Fix the numpy array boolean check issue\n    try:\n        if vec is None:\n            vec = [s_int(1, 39) for _ in range(6)]\n        elif hasattr(vec, '__len__') and len(vec) == 0:\n            vec = [s_int(1, 39) for _ in range(6)]\n        elif isinstance(vec, np.ndarray) and vec.size == 0:\n            vec = [s_int(1, 39) for _ in range(6)]\n    except:\n        vec = [s_int(1, 39) for _ in range(6)]\n    \n    # Convert to list if numpy array\n    if isinstance(vec, np.ndarray):\n        vec = vec.tolist()\n    \n    # Handle non-iterable input\n    if not hasattr(vec, '__iter__'):\n        vec = [vec] if vec else [s_int(1, 39) for _ in range(6)]\n    \n    # Convert to numbers and filter\n    nums = []\n    for v in vec:\n        try:\n            if isinstance(v, (int, float, np.integer, np.floating)) and not np.isnan(v):\n                nums.append(max(1, min(39, int(round(float(v))))))\n        except:\n            continue\n    \n    # Remove duplicates while preserving order\n    out, seen = [], set()\n    for n in nums:\n        if n not in seen:\n            seen.add(n)\n            out.append(n)\n            if len(out) == 6:\n                return sorted(out)\n    \n    # Fill remaining slots\n    while len(out) < 6:\n        r = s_int(1, 39)\n        if r not in seen:\n            seen.add(r)\n            out.append(r)\n    \n    return sorted(out[:6])\n\n# ALSO FIX THE NORMALIZE FUNCTION\ndef normalize(d):\n    \"\"\"Normalize weights with minimum threshold.\"\"\"\n    if not d or len(d) == 0:  # Fixed boolean check\n        return d\n        \n    try:\n        s = sum(d.values()) or 1\n        for k in d: \n            d[k] = max(1e-8, d[k] / s)\n    except:\n        # Fallback for edge cases\n        for k in d:\n            d[k] = 1.0 / len(d) if len(d) > 0 else 1.0\n    \n    return d\n\n# ENTROPY FUNCTION FIX\ndef entropy(wdict):\n    \"\"\"Calculate entropy with breakthrough targeting.\"\"\"\n    if not wdict or len(wdict) == 0:  # Fixed boolean check\n        return 0.0\n        \n    try:\n        w = np.array(list(wdict.values()))\n        if w.size == 0:  # Handle empty array\n            return 0.0\n            \n        w = w / (w.sum() or 1)\n        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n            # Filter out zero and negative values\n            w_filtered = w[w > 1e-9]\n            if len(w_filtered) == 0:\n                return 0.0\n            ent = float(-(w_filtered * np.log2(w_filtered)).sum())\n            return ent if not np.isnan(ent) else 0.0\n    except:\n        return 0.0\n\ndef beta_convergence_reward(beta):\n    \"\"\"Reward function for beta convergence to target.\"\"\"\n    distance = abs(beta - BETA_TARGET)\n    if distance < BETA_TOLERANCE:\n        return 1.0 + CONVERGENCE_BOOST\n    return max(0.5, 1.0 - (distance * 2))\n\n# 8. STATE MANAGEMENT  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndef _blank_state():\n    return dict(\n        weights=defaultdict(float),\n        performance=defaultdict(float),\n        entropy_history=[],\n        beta_history=[],\n        success_patterns=[],\n        expert_synergy=defaultdict(float),\n        convergence_rate=0.0,\n        iteration=0,\n        breakthrough_count=0,\n        pattern_memory=deque(maxlen=100),\n        \n        # Enhanced analytics\n        successful_beta_range=[],\n        optimal_beta_mean=0.0,\n        optimal_beta_std=0.0,\n        success_beta_histogram=[],\n        total_beta_range=[],\n        success_rate_by_beta=defaultdict(float),\n        \n        successful_entropy_range=[],\n        optimal_entropy_mean=0.0,\n        optimal_entropy_std=0.0,\n        success_entropy_histogram=[],\n        \n        cycle_position_performance=defaultdict(int),\n        hottest_positions=[],\n        position_success_rate=defaultdict(float),\n        position_clusters=[],\n        \n        expert_optimal_weights=defaultdict(dict),\n        correlation_analysis=defaultdict(float),\n        success_beta_entropy_ratio=[]\n    )\n\ndef load_state(validation_period):\n    state_file = f\"apex_state_{validation_period}.pkl\"\n    \n    if not os.path.exists(state_file):\n        st = _blank_state()\n    else:\n        try:\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\")\n                st = pickle.load(open(state_file, \"rb\"))\n        except: \n            st = _blank_state()\n    \n    # Initialize expert weights with breakthrough bias\n    for ex in expert_registry:\n        if ex not in st[\"weights\"]:\n            if ex in ORACLE_EXPERTS:\n                st[\"weights\"][ex] = ORACLE_EXPERTS[ex] / 10.0\n            elif ex in BREAKTHROUGH_EXPERTS:\n                st[\"weights\"][ex] = BREAKTHROUGH_EXPERTS[ex] / 10.0  \n            elif ex in SPECIALIST_EXPERTS:\n                st[\"weights\"][ex] = SPECIALIST_EXPERTS[ex] / 10.0\n            elif ex in SUPPRESSED_EXPERTS:\n                st[\"weights\"][ex] = SUPPRESSED_EXPERTS[ex] / 10.0\n            else:\n                st[\"weights\"][ex] = 1.0 / len(expert_registry)\n        \n        st[\"performance\"][ex] = st.get(\"performance\", {}).get(ex, 0.5)\n    \n    normalize(st[\"weights\"])\n    return st\n\ndef save_state(st, validation_period):\n    state_file = f\"apex_state_{validation_period}.pkl\"\n    try: \n        pickle.dump(st, open(state_file, \"wb\"))\n    except Exception as e:\n        print(f\"Warning: Could not save state - {e}\")\n\n# 9. EXPERT EXECUTION  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndef run_experts(df, beta, st):\n    \"\"\"Execute experts with breakthrough optimization.\"\"\"\n    preds, confs = {}, {}\n    \n    for ex, cfg in expert_registry.items():\n        if len(df) < cfg[\"min_window\"]:\n            continue\n            \n        fn = globals().get(cfg[\"func\"], _stub)\n        try:\n            with absolute_silence():\n                p, c = fn(df)\n            preds[ex], confs[ex] = unique6(p), c\n        except Exception as e:\n            preds[ex], confs[ex] = unique6([s_int(1, 39) for _ in range(6)]), 0.4\n    \n    # Advanced scoring with breakthrough weighting\n    scores = {}\n    for ex in preds:\n        base_conf = confs[ex]\n        \n        # Apply expert hierarchy multipliers\n        if ex in ORACLE_EXPERTS:\n            multiplier = ORACLE_EXPERTS[ex]\n        elif ex in BREAKTHROUGH_EXPERTS:\n            multiplier = BREAKTHROUGH_EXPERTS[ex]\n        elif ex in SPECIALIST_EXPERTS:\n            multiplier = SPECIALIST_EXPERTS[ex]\n        elif ex in SUPPRESSED_EXPERTS:\n            multiplier = SUPPRESSED_EXPERTS[ex]\n        else:\n            multiplier = 1.0\n        \n        # Performance-based adjustment\n        perf_mult = 1.0 + st[\"performance\"][ex]\n        \n        # Beta convergence bonus\n        beta_bonus = beta_convergence_reward(beta)\n        \n        scores[ex] = max(1e-6, base_conf * multiplier * perf_mult * beta_bonus)\n    \n    normalize(scores)\n    return preds, scores\n\n# 10. ENSEMBLE BUILDING  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndef build_set(preds, scores, st, forced_entropy=False):\n    \"\"\"Build ensemble with entropy locking and pattern optimization.\"\"\"\n    pool = list(scores)\n    if not pool:\n        return unique6([s_int(1, 39) for _ in range(6)]), {}\n    \n    # Expert selection with synergy consideration\n    if forced_entropy and len(pool) >= 4:\n        # Force high-entropy configuration for target hitting\n        chosen = s_smpl(pool, min(len(pool), s_int(6, 10)))\n        sub_w = {e: 1.0 for e in chosen}  # Equal weights for high entropy\n    else:\n        # Weighted selection favoring successful combinations\n        chosen = []\n        remaining = pool.copy()\n        \n        # Always include oracle experts if available\n        for oracle in ORACLE_EXPERTS:\n            if oracle in remaining:\n                chosen.append(oracle)\n                remaining.remove(oracle)\n        \n        TARGET_TEAM_SIZE = 8\n        # replace `target_n = min(8, len(pool))` with:\n        target_n = TARGET_TEAM_SIZE\n        while len(chosen) < target_n and remaining:\n            if len(chosen) < 4:\n                breakthrough_remaining = [e for e in remaining if e in BREAKTHROUGH_EXPERTS]\n                candidates = breakthrough_remaining if breakthrough_remaining else remaining\n            else:\n                candidates = remaining\n            \n            if not candidates:\n                break\n            \n            # Weighted random selection\n            weights = [scores[e] for e in candidates]\n            total_weight = sum(weights)\n            if total_weight == 0:\n                next_expert = s_pick(candidates)\n            else:\n                r = s_unif(0, total_weight)\n                cumsum = 0\n                next_expert = candidates[-1]  # fallback\n                for i, candidate in enumerate(candidates):\n                    cumsum += weights[i]\n                    if r <= cumsum:\n                        next_expert = candidate\n                        break\n            \n            if next_expert in remaining:\n                chosen.append(next_expert)\n                remaining.remove(next_expert)\n            else:\n                break  # Prevent infinite loops\n        \n        sub_w = {e: scores[e] for e in chosen}\n    \n    normalize(sub_w)\n    \n    # Entropy adjustment for targeting\n    current_entropy = entropy(sub_w)\n    entropy_error = abs(current_entropy - ENTROPY_TARGET)\n    \n    if entropy_error > ENTROPY_TOLERANCE:\n        # Adjust weights to approach target entropy\n        if current_entropy < ENTROPY_TARGET:\n            # Flatten weights to increase entropy\n            for k in sub_w:\n                sub_w[k] = sub_w[k] ** 0.8\n        else:\n            # Sharpen weights to decrease entropy  \n            for k in sub_w:\n                sub_w[k] = sub_w[k] ** 1.2\n        normalize(sub_w)\n    \n    # Weighted aggregation\n    agg = np.zeros(6)\n    for e, w in sub_w.items():\n        if e in preds:\n            agg += w * np.array(preds[e])\n    \n    final_pred = unique6(agg)\n    return final_pred, sub_w\n\n\n\n\n# 11. TEMPORAL VALIDATION ORCHESTRATOR  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndef validate_period_unified(df, validation_config, unified_collector, n_sets=100, max_it=10000, thr=3):\n    \"\"\"Execute validation for a single period with comprehensive analytics.\"\"\"\n    \n    period_name = validation_config[\"name\"]\n    train_end = validation_config[\"train_end\"]\n    predict_date = validation_config[\"predict_date\"]\n    \n    \n    \n    print(f\"\\n{'='*80}\")\n    print(f\"ðŸš€ DHARMA_X SEGMENTED VALIDATION - {period_name.upper()}\")\n    if 'segment' in validation_config:\n        print(f\"ðŸ“Š SEGMENT {validation_config['segment']} - TRAINING SIZE: {len(validation_config.get('training_dates', []))} DRAWS\")\n    print(f\"{'='*80}\")\n    seg_summary = SegmentSummaryCollector()\n   \n    \n    # Filter training data up to train_end\n    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n    # SAFE DYNAMIC TRAINING WINDOW WITH ERROR HANDLING\n    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n    \n    df['Date'] = pd.to_datetime(df['Date'])\n    train_end_dt = pd.to_datetime(train_end)\n    \n    # Get available data up to train_end\n    available_data = df[df['Date'] <= train_end_dt].copy()\n    available_data = available_data.sort_values('Date', ascending=True)  # Ensure ascending\n    \n    # Safety check for empty data\n    if len(available_data) < 1:\n        print(f\"âŒ No data available before {train_end}\")\n        return 0  # Return 0 total_predictions to indicate failure\n    \n    # Get last 12 draws for 3-month window\n    # SEGMENTED TRAINING WINDOW\n    if 'training_dates' in validation_config:\n        # Use specific training dates for this segment\n        training_date_strs = validation_config['training_dates']\n        training_dates = pd.to_datetime(training_date_strs)\n        train_df = df[df['Date'].isin(training_dates)].copy()\n        train_df = train_df.sort_values('Date', ascending=True)\n    else:\n        # Fallback to original logic\n        train_df = available_data.tail(3)\n\n    print(f\"ðŸ” Available data range: {available_data['Date'].min()} to {available_data['Date'].max()}\")\n    print(f\"ðŸ” Selected window: {train_df['Date'].min()} to {train_df['Date'].max()}\")\n    \n    # Safety check for empty training set\n    if train_df.empty:\n        print(f\"âŒ Training window is empty for {period_name}\")\n        return 0  # Return 0 total_predictions to indicate failure\n    \n    # Calculate dynamic training range for display\n    dynamic_start = train_df['Date'].min().strftime('%Y-%m-%d')\n    dynamic_end = train_df['Date'].max().strftime('%Y-%m-%d')\n    \n    print(f\"ðŸ“… Dynamic Training: {dynamic_start} â†’ {dynamic_end}\")\n    print(f\"ðŸ“Š Training draws: {len(train_df)} (target: 3)\")\n    if 'training_dates' in validation_config:\n        print(f\"ðŸ“… Segmented Training: {validation_config['training_dates']}\")\n        print(f\"ðŸ“Š Training Window Size: {len(validation_config['training_dates'])} draws\")\n    else:\n        print(f\"ðŸ“… Training: {train_df['Date'].min()} â†’ {train_end}\")\n    print(f\"ðŸŽ¯ Prediction Target: {predict_date}\")\n\n\n    \n    print(f\"ðŸ“Š Training samples: {len(train_df)}\")\n\n    # Final safety check before proceeding\n    if len(train_df) < 1:\n        print(f\"âŒ Insufficient training data: {len(train_df)} draws (minimum: 1)\")\n        return 0  # Return 0 total_predictions to indicate failure\n\n    \n    # Get actual result for the prediction date\n    predict_date_dt = pd.to_datetime(predict_date)\n    actual_results = df[df['Date'] == predict_date_dt]\n    \n    if actual_results.empty:\n        print(f\"âŒ No actual result found for {predict_date}\")\n        return None, None, None\n    \n    actual = actual_results.iloc[0]\n    actual_numbers = [actual[f'Number{i}'] for i in range(1, 7)]\n    \n    print(f\"âœ… Target result: {actual_numbers}\")\n    seg_summary = SegmentSummaryCollector(min_k_include=0)  # Accept all 0..6\n    seg_summary.set_actuals(actual_numbers)\n    \n    # Load state for this validation period\n    st = load_state(period_name)\n    # Boost universal experts once per period (intelligence-driven)\n    for _exp in UNIVERSAL_EXPERTS:\n        if _exp in st[\"weights\"]:\n            st[\"weights\"][_exp] *= UNIVERSAL_EXPERT_BOOST\n    normalize(st[\"weights\"])\n\n    \n    # Run orchestration\n    best_predictions = []\n    total_hit_counts = Counter()\n    \n    for it in range(1, max_it + 1):\n        st[\"iteration\"] = it\n        \n        default_cycle_pos = (it-1)%BETA_CYCLE_LENGTH\n        cycle_pos = force_universal_cycle_positions(it, default_cycle_pos)\n        \n        beta = beta_dual_anchor(it)\n        current_entropy_target = adaptive_entropy_targeting(it, max_it)\n        # Mixed corridor to preserve balanced attack (favor 5/6 periodically)\n        if STRATEGY_MODE != 'aggressive' and (it % 5 == 1):\n            beta = 0.5212 + s_unif(-0.0219, 0.0219)  # 5/6 envelope\n\n        \n            \n        # Convergence toward target over time\n        convergence_factor = min(1.0, it / (max_it * 0.7))\n        beta = beta * (1 - convergence_factor) + BETA_TARGET * convergence_factor\n        \n        # Execute expert predictions\n        preds, scores = run_experts(train_df, beta, st)\n        current_entropy = entropy(scores)\n        # Nudge toward current target entropy (4/5 or 6/6)\n        if abs(current_entropy - current_entropy_target) > ENTROPY_TOLERANCE:\n            beta, current_entropy = enforce_breakthrough_signature(beta, current_entropy)\n\n        \n        st[\"entropy_history\"].append(current_entropy)\n        st[\"beta_history\"].append(beta)\n        \n        # Determine phase\n        #if beta < 0.45:\n            #phase = \"EXPLOIT\"\n        #elif beta > 0.60:\n            #phase = \"EXPLORE\"\n        #else:\n            #phase = \"BALANCED\"\n        PHASE_WEIGHTS = {'BALANCED':1.00,'EXPLOIT':0.00,'EXPLORE':0.00}\n        phase = max(PHASE_WEIGHTS, key=lambda p: PHASE_WEIGHTS[p])  # always BALANCED by weight\n\n        # Build prediction sets\n        passed, hit_dist = 0, Counter()\n        breakthrough_this_iter = 0\n\n                # Calculate elite vs exploration sets\n        elite_sets = int(n_sets * RESOURCE_ALLOCATION['elite'])\n        explore_sets = n_sets - elite_sets\n\n        for i in range(n_sets):\n            # Tier 0: Proven 6/6 forcing for a controlled share of sets\n            # Tier 0: Universal patterns (60% - multi-period validated)\n            if i < int(n_sets*FORCE_UNIVERSAL_SHARE):\n                u_pred,u_subw = build_universal_set(preds,scores,st)\n                if u_pred: pred,subw = u_pred,u_subw\n                else:     pred,subw = build_set(preds,scores,st,forced_entropy=False)\n            \n            # Tier 1: Exploration (25%)\n            elif i < int(n_sets*(FORCE_UNIVERSAL_SHARE+FORCE_EXPLORATION_SHARE)):\n                pred,subw = build_set(preds,scores,st,forced_entropy=True)\n            \n            # Tier 2: 6/6 Breakthrough (15%)\n            elif i < int(n_sets*(FORCE_UNIVERSAL_SHARE+FORCE_EXPLORATION_SHARE+FORCE_6OF6_SHARE)):\n                b_pred,b_subw = build_6of6_breakthrough_set(preds,scores,st)\n                if b_pred: pred,subw = b_pred,b_subw\n                else:      pred,subw = build_set(preds,scores,st,forced_entropy=True)\n\n            # Tier 1: Your original elite allocation\n            elif i < elite_sets:\n                combo = ELITE_COMBOS[i % len(ELITE_COMBOS)]\n                subw = {}\n                for expert in combo:\n                    if expert in scores:\n                        subw[expert] = scores[expert]\n                if subw:\n                    normalize(subw)\n                    agg = np.zeros(6)\n                    for expert, weight in subw.items():\n                        if expert in preds:\n                            agg += weight * np.array(preds[expert])\n                    pred = unique6(agg)\n                else:\n                    pred, subw = build_set(preds, scores, st, forced_entropy=False)\n            # Tier 2: Exploration (unchanged)\n            else:\n                forced = ((i - elite_sets) < explore_sets * ENTROPY_FORCING_RATIO)\n                pred, subw = build_set(preds, scores, st, forced_entropy=forced)\n\n            \n            # Continue with existing hit calculation and analytics\n            hit = len(set(pred) & set(actual_numbers))\n            hit_dist[hit] += 1\n            total_hit_counts[hit] += 1\n            seg_summary.update(pred, hit)\n            # Collect analytics for â‰¥4/6 matches (your updated threshold)\n            if hit >= MIN_HIT_LEVEL:\n                unified_collector.collect_record(\n                    period_name=period_name,\n                    hit_count=hit,\n                    iteration=it,\n                    prediction=pred,\n                    target=actual_numbers,\n                    template=subw,\n                    entropy=current_entropy,\n                    beta=beta,\n                    phase=phase,\n                    cycle_position=cycle_pos,\n                    expert_weights=st['weights'],\n                    expert_performances=st['performance']\n                )\n                seg_summary.update(pred, hit)\n\n            \n            # Breakthrough detection\n            \n            if hit >= 5:\n                breakthrough_this_iter += 1\n                st[\"breakthrough_count\"] += 1\n                st[\"success_patterns\"].append((pred, subw, current_entropy, beta))\n                \n                # Update expert performance\n                for exp in subw:\n                    st[\"performance\"][exp] = min(2.0, st[\"performance\"][exp] + 0.1)\n            \n            if hit >= thr:\n                passed += 1\n                best_predictions.append((it, hit, pred, current_entropy, beta))\n\n        \n        #for i in range(n_sets):\n            # Force entropy targeting for a portion of predictions\n            #forced = (i < n_sets * ENTROPY_FORCING_RATIO)\n            #pred, subw = build_set(preds, scores, st, forced_entropy=forced)\n            #hit = len(set(pred) & set(actual_numbers))\n            #hit_dist[hit] += 1\n            #total_hit_counts[hit] += 1\n            \n            # Collect analytics for â‰¥4/6 matches\n            #if hit >= 2:\n                          \n                #unified_collector.collect_record(  # âœ… NEW METHOD\n                    #period_name=period_name,\n                    #hit_count=hit,\n                    #iteration=it,\n                    #prediction=pred,\n                    #target=actual_numbers,\n                    #template=subw,\n                    #entropy=current_entropy,\n                    #beta=beta,\n                    #phase=phase,\n                    #cycle_position=cycle_pos,\n                    #expert_weights=st['weights'],\n                    #expert_performances=st['performance']\n                #)\n                \n                \n            \n            # Breakthrough detection\n            #if hit >= 5:\n                #breakthrough_this_iter += 1\n                #st[\"breakthrough_count\"] += 1\n                #st[\"success_patterns\"].append((pred, subw, current_entropy, beta))\n                \n                # Update expert performance\n                #for exp in subw:\n                    #st[\"performance\"][exp] = min(2.0, st[\"performance\"][exp] + 0.1)\n            \n            #if hit >= thr:\n                #passed += 1\n                #best_predictions.append((it, hit, pred, current_entropy, beta))\n        \n        # Progress reporting\n        if it % 50 == 0 or breakthrough_this_iter > 0:\n            hits_str = \", \".join(f\"{h}/6:{c}\" for h, c in sorted(hit_dist.items()) if c > 0)\n            target_mode = \"6/6\" if current_entropy_target == ENTROPY_6_TARGET else \"4/5\"\n            print(f\"ðŸ”„ Iter {it:4d} | Î²={beta:.4f} | ent={current_entropy:.4f} | \"\n                  f\"mode={target_mode} | pos={cycle_pos} | Pass:{passed:2d}/{n_sets} | Hits: {hits_str}\")\n            \n            if breakthrough_this_iter > 0:\n                print(f\"   ðŸŽ¯ BREAKTHROUGHS: {breakthrough_this_iter} | Total: {st['breakthrough_count']}\")\n        \n        # Dynamic expert weight adjustment\n        if breakthrough_this_iter > 0:\n            #for exp in st[\"weights\"]:\n                #if exp in ORACLE_EXPERTS:\n                    #st[\"weights\"][exp] *= 1.02\n                #elif exp in BREAKTHROUGH_EXPERTS:\n                    #t[\"weights\"][exp] *= 1.01\n            for sig, w in SIGNATURE_WEIGHTS.items():\n                if matches_pattern(sig, pred):\n                    for exp in st[\"weights\"]:\n                        st[\"weights\"][exp] *= (1 + w)\n\n        \n        normalize(st[\"weights\"])\n        \n        # Early convergence check\n        entropy_convergence = 1.0 - abs(current_entropy - ENTROPY_TARGET)\n        beta_convergence = 1.0 - abs(beta - BETA_TARGET)\n        overall_convergence = (entropy_convergence + beta_convergence) / 2.0\n        \n        #if it > 200 and overall_convergence > 0.95 and breakthrough_this_iter >= 3:\n            #print(f\"ðŸŽŠ EARLY CONVERGENCE at iteration {it}!\")\n            #break\n    \n    # Save state and analytics\n    save_state(st, period_name)\n    \n    seg_summary.print_summary(\n        segment_label=str(validation_config.get('segment', '1')),\n        period_label=period_name\n    )\n    # Final summary\n    print(f\"\\n{'='*60}\")\n    print(f\"ðŸ“Š {period_name.upper()} VALIDATION COMPLETE\")\n    print(f\"{'='*60}\")\n    \n    total_predictions = sum(total_hit_counts.values())\n    success_4plus = sum(count for hit, count in total_hit_counts.items() if hit >= 4)\n    success_5plus = sum(count for hit, count in total_hit_counts.items() if hit >= 5)\n    \n    print(f\"Total predictions: {total_predictions}\")\n    print(f\"â‰¥4/6 successes: {success_4plus} ({success_4plus/total_predictions*100:.2f}%)\")\n    print(f\"â‰¥5/6 successes: {success_5plus} ({success_5plus/total_predictions*100:.2f}%)\")\n    \n    \n    print(\"\\nHit distribution:\")\n    for hit in range(7):\n        count = total_hit_counts[hit]\n        pct = count / total_predictions * 100 if total_predictions > 0 else 0\n        print(f\" {hit}/6: {count:5d} ({pct:5.2f}%)\")\n    \n    # Show top predictions\n    if best_predictions:\n        print(\"\\nTop predictions (â‰¥4/6):\")\n        for it, hit, pred, ent, beta in sorted(best_predictions, key=lambda x: (-x[1], x[0]))[:10]:\n            print(f\" Iter {it:4d} | {hit}/6 | {pred} | E:{ent:.4f} B:{beta:.4f}\")\n    \n    return total_predictions\n\n# 12. MULTI-PERIOD VALIDATION CONTROLLER  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndef run_comprehensive_validation(df_path, n_sets=100, max_it_per_period=10000):\n    \"\"\"Run validation across all configured periods.\"\"\"\n    unified_collector = UnifiedAnalyticsCollector()\n    try:\n        # Load data\n        print(\"ðŸ“ Loading data file...\")\n        df = pd.read_excel(df_path)\n        # GENERATE SEGMENTED PERIODS FROM DATA\n        global VALIDATION_PERIODS\n        VALIDATION_PERIODS = generate_segmented_periods(df)\n        print(f\"ðŸŽ¯ Generated {len(VALIDATION_PERIODS)} segmented validation periods\")\n        \n        # Display segment breakdown\n        segment_summary = {}\n        for period in VALIDATION_PERIODS:\n            month = period['month']\n            if month not in segment_summary:\n                segment_summary[month] = 0\n            segment_summary[month] += 1\n        \n        print(\"ðŸ“Š SEGMENTED PERIOD BREAKDOWN:\")\n        for month, seg_count in segment_summary.items():\n            print(f\"  {month}: {seg_count} segments\")\n\n        \n        # Standardize column names\n        if 'Date' not in df.columns:\n            date_cols = [col for col in df.columns if 'date' in col.lower()]\n            if date_cols:\n                df.rename(columns={date_cols[0]: 'Date'}, inplace=True)\n        \n        # Standardize number columns\n        for i in range(1, 7):\n            old_cols = [col for col in df.columns if f'n{i}' in col.lower() or f'number{i}' in col.lower()]\n            if old_cols:\n                df.rename(columns={old_cols[0]: f'Number{i}'}, inplace=True)\n        \n        print(f\"âœ… Data loaded: {len(df)} records\")\n        print(f\"ðŸ“… Date range: {df['Date'].min()} to {df['Date'].max()}\")\n        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n        # DEBUG: CHECK DATA AVAILABILITY FOR EACH MONTHLY PERIOD\n        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n        print(\"\\nðŸ” DEBUGGING DATA AVAILABILITY:\")\n        for period in VALIDATION_PERIODS:\n            train_end_dt = pd.to_datetime(period[\"train_end\"])\n            available = df[df['Date'] <= train_end_dt]\n            print(f\"{period['name']}: {len(available)} draws available ending {period['train_end']}\")\n            if len(available) > 0:\n                last_3 = available.tail(3)\n                print(f\"  â†’ Last 3: {last_3['Date'].min().strftime('%Y-%m-%d')} to {last_3['Date'].max().strftime('%Y-%m-%d')}\")\n            else:\n                print(f\"  â†’ âŒ NO DATA AVAILABLE\")\n            print()\n\n        # â”€â”€ EXPERT LOADING DIAGNOSTIC â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        print(\"\\nðŸ” Expert Function Diagnostic:\")\n        missing = []\n        found = 0\n        for ex, cfg in expert_registry.items():\n            fn = globals().get(cfg['func'])\n            if fn is None or fn == _stub:\n                missing.append(cfg['func'])\n            else:\n                found += 1\n        \n        print(f\"âœ… Experts found: {found}/{len(expert_registry)}\")\n        if missing:\n            print(f\"âŒ Missing experts: {len(missing)}\")\n            for m in missing[:5]:  # Show first 5 missing\n                print(f\"   - {m}\")\n            if len(missing) > 5:\n                print(f\"   ... and {len(missing)-5} more\")\n        else:\n            print(\"ðŸŽ¯ All expert functions loaded successfully!\")\n        print()\n\n\n        \n        # Run validation for each period\n        all_results = {}\n        all_analytics = {}\n        \n        for period_config in VALIDATION_PERIODS:\n            try:\n                period_name = period_config[\"name\"]\n                train_end = period_config[\"train_end\"]\n                predict_date = period_config[\"predict_date\"]\n                \n                # Get actual target\n                predict_date_dt = pd.to_datetime(predict_date)\n                actual_results = df[df['Date'] == predict_date_dt]\n                actual_numbers = [actual_results.iloc[0][f'Number{i}'] for i in range(1, 7)]\n                \n                # Initialize period in unified collector\n                unified_collector.initialize_period(period_name, train_end, predict_date, actual_numbers)\n                print(f\"\\nðŸ”„ Starting {period_name}...\")\n\n                # âœ… ADD THIS - ACTUAL VALIDATION EXECUTION\n                total_predictions = validate_period_unified(\n                    df=df, \n                    validation_config=period_config,\n                    unified_collector=unified_collector,\n                    n_sets=n_sets, \n                    max_it=max_it_per_period, \n                    thr=3\n                )\n                \n                # Finalize period statistics  \n                unified_collector.finalize_period(period_name, total_predictions)\n                print(f\"âœ… {period_name} completed successfully\")\n                \n                # Store results for cross-period analysis\n                all_results[period_name] = {\n                    'hit_distribution': dict(unified_collector.unified_data[\"periods\"][period_name][\"statistics\"][\"hit_distribution\"]),\n                    'total_predictions': total_predictions,\n                    'collected_records': unified_collector.unified_data[\"periods\"][period_name][\"statistics\"][\"collected_records\"]\n                }\n                \n            except Exception as e:  # âœ… ADD THIS MISSING EXCEPT BLOCK\n                print(f\"âŒ Error in {period_config.get('name', 'unknown_period')}: {e}\")\n                continue  # Skip this period and continue with next one\n\n        def display_segmented_results(all_results):\n            \"\"\"Display segmented prediction results with counts and percentages on-screen\"\"\"\n            \n            # Group results by month and segment\n            month_segment_data = {}\n            \n            for period_name, results in all_results.items():\n                if '_Seg' in period_name:\n                    month, segment = period_name.split('_Seg')\n                    segment = f\"Seg{segment}\"\n                    \n                    if month not in month_segment_data:\n                        month_segment_data[month] = {}\n                    \n                    hit_dist = results.get('hit_distribution', {})\n                    month_segment_data[month][segment] = hit_dist\n            \n            print(f\"\\n{'='*80}\")\n            print(\"ðŸ“Š DHARMA_X SEGMENTED PREDICTION RESULTS\")\n            print(f\"{'='*80}\")\n            \n            # Display results month by month\n            for month in sorted(month_segment_data.keys()):\n                segments = month_segment_data[month]\n                segment_names = sorted(segments.keys(), key=lambda x: int(x.replace('Seg', '')))\n                \n                print(f\"\\n{month}\")\n                print(\"â”€\" * 60)\n                \n                # Create header row\n                header = \"      \" + \"\".join(f\"{seg:>15}\" for seg in segment_names)\n                print(header)\n                \n                # Display each hit level\n                for hit_level in range(7):\n                    hit_label = f\"{hit_level}/6\"\n                    row_data = [f\"{hit_label:>4}  \"]\n                    \n                    for segment in segment_names:\n                        hit_data = segments.get(segment, {})\n                        count = hit_data.get(hit_level, 0)\n                        total = sum(hit_data.values()) if hit_data else 1\n                        percentage = (count / total * 100) if total > 0 else 0\n                        \n                        if count > 0:\n                            formatted_value = f\"{count} ({percentage:.2f}%)\"\n                        else:\n                            formatted_value = \"-\"\n                        \n                        row_data.append(f\"{formatted_value:>15}\")\n                    \n                    print(\"\".join(row_data))\n        \n        def display_summary_statistics(all_results):\n            \"\"\"Display summary statistics for each segment on-screen\"\"\"\n            \n            summary_data = []\n            \n            for period_name, results in all_results.items():\n                if '_Seg' in period_name:\n                    month, segment = period_name.split('_Seg')\n                    \n                    hit_dist = results.get('hit_distribution', {})\n                    total_predictions = sum(hit_dist.values())\n                    \n                    # Calculate key metrics\n                    success_3plus = sum(count for hit, count in hit_dist.items() if hit >= 3)\n                    success_4plus = sum(count for hit, count in hit_dist.items() if hit >= 4)\n                    success_5plus = sum(count for hit, count in hit_dist.items() if hit >= 5)\n                    success_6 = hit_dist.get(6, 0)\n                    \n                    success_3plus_pct = (success_3plus / total_predictions * 100) if total_predictions > 0 else 0\n                    success_4plus_pct = (success_4plus / total_predictions * 100) if total_predictions > 0 else 0\n                    success_5plus_pct = (success_5plus / total_predictions * 100) if total_predictions > 0 else 0\n                    success_6_pct = (success_6 / total_predictions * 100) if total_predictions > 0 else 0\n                    \n                    summary_data.append({\n                        'Month': month,\n                        'Segment': f\"Seg{segment}\",\n                        'Total': f\"{total_predictions:,}\",\n                        'â‰¥3/6': f\"{success_3plus} ({success_3plus_pct:.2f}%)\",\n                        'â‰¥4/6': f\"{success_4plus} ({success_4plus_pct:.2f}%)\",\n                        'â‰¥5/6': f\"{success_5plus} ({success_5plus_pct:.2f}%)\",\n                        '6/6': f\"{success_6} ({success_6_pct:.2f}%)\"\n                    })\n            \n            if summary_data:\n                print(f\"\\n{'='*80}\")\n                print(\"ðŸ“ˆ SEGMENT PERFORMANCE SUMMARY\")\n                print(f\"{'='*80}\")\n                \n                # Create formatted table\n                header = f\"{'Month':<10} {'Segment':<8} {'Total':<12} {'â‰¥3/6':<18} {'â‰¥4/6':<18} {'â‰¥5/6':<18} {'6/6':<18}\"\n                print(header)\n                print(\"â”€\" * len(header))\n                \n                for data in summary_data:\n                    row = f\"{data['Month']:<10} {data['Segment']:<8} {data['Total']:<12} {data['â‰¥3/6']:<18} {data['â‰¥4/6']:<18} {data['â‰¥5/6']:<18} {data['6/6']:<18}\"\n                    print(row)\n            else:\n                print(\"âŒ No summary statistics to display\")\n        \n        def display_comprehensive_analysis(all_results):\n            \"\"\"Display both detailed and summary results\"\"\"\n            display_segmented_results(all_results)\n            display_summary_statistics(all_results)\n            \n            print(f\"\\n{'='*80}\")\n            print(\"âœ… SEGMENTED ANALYSIS COMPLETE\")\n            print(f\"{'='*80}\")\n\n        \n        # Generate comprehensive cross-period analysis\n        print(f\"\\n{'='*80}\")\n        print(\"ðŸ“Š COMPREHENSIVE MULTI-PERIOD ANALYSIS\")\n        print(f\"{'='*80}\")\n        \n        if all_results:\n            cross_analysis = generate_cross_period_analysis(all_results, all_analytics)\n            print(f\"\\n{'='*80}\")\n            print(\"ðŸ“Š DISPLAYING SEGMENTED RESULTS\")\n            print(f\"{'='*80}\")\n            \n            display_comprehensive_analysis(all_results)\n            \n            # Save comprehensive results\n            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n            results_file = f\"dharma_comprehensive_validation_{timestamp}.json\"\n            \n            with open(results_file, 'w') as f:\n                json.dump({\n                    'validation_summary': {\n                        'periods_completed': len(all_results),\n                        'total_periods': len(VALIDATION_PERIODS),\n                        'timestamp': datetime.now().isoformat()\n                    },\n                    'period_results': all_results,\n                    'cross_period_analysis': cross_analysis\n                }, f, indent=2, default=str)\n            \n            print(f\"ðŸ“„ Comprehensive results saved: {results_file}\")\n            # Save unified analytics file\n            unified_file = unified_collector.save_unified_file()\n            print(f\"ðŸ“Š Unified analytics saved: {unified_file}\")\n\n            \n            # Display summary\n            print(\"\\nValidation Summary:\")\n            total_4plus = sum(sum(1 for hit, count in result['hit_distribution'].items() \n                                if hit >= 4 and count > 0) \n                            for result in all_results.values())\n            total_5plus = sum(sum(1 for hit, count in result['hit_distribution'].items() \n                                if hit >= 5 and count > 0) \n                            for result in all_results.values())\n            \n            print(f\"Periods completed: {len(all_results)}/{len(VALIDATION_PERIODS)}\")\n            print(f\"Cross-period â‰¥4/6 patterns: {total_4plus}\")\n            print(f\"Cross-period â‰¥5/6 patterns: {total_5plus}\")\n            \n            if cross_analysis.get('consistency_score', 0) > 0.7:\n                print(\"ðŸŽ¯ HIGH CONSISTENCY DETECTED across validation periods!\")\n            elif cross_analysis.get('consistency_score', 0) > 0.5:\n                print(\"ðŸ“Š MODERATE CONSISTENCY across validation periods\")\n            else:\n                print(\"âš ï¸  LOW CONSISTENCY - System may need refinement\")\n\n            print(f\"\\n{'='*80}\")\n            print(\"ðŸ§  EXTRACTING BREAKTHROUGH INTELLIGENCE (â‰¥4/6)\")\n            print(f\"{'='*80}\")\n            \n            try:\n                breakthrough_analysis = extract_breakthrough_intelligence(unified_file)\n                \n                if breakthrough_analysis:\n                    recommendations = generate_orchestrator_optimization_recommendations(breakthrough_analysis)\n                    \n                    # Display breakthrough intelligence\n                    print(\"\\nðŸŽ¯ BREAKTHROUGH PARAMETER ANALYSIS:\")\n                    params = breakthrough_analysis['parameter_sweet_spots']\n                    print(f\"   Î² Range: {params['beta_min']:.4f} â†’ {params['beta_max']:.4f} (Î¼={params['beta_mean']:.4f})\")\n                    print(f\"   H Range: {params['entropy_min']:.4f} â†’ {params['entropy_max']:.4f} (Î¼={params['entropy_mean']:.4f})\")\n                    \n                    print(\"\\nðŸ”¥ TOP BREAKTHROUGH EXPERT COMBINATIONS:\")\n                    for i, (combo, count) in enumerate(breakthrough_analysis['top_expert_combinations'][:5], 1):\n                        expert_list = list(combo)[:3]  # Show first 3 experts\n                        print(f\"   {i}. {expert_list}... â†’ {count} successes\")\n                    \n                    print(\"\\nâš¡ CYCLE POSITION HOTSPOTS:\")\n                    for pos, count in breakthrough_analysis['cycle_hotspots'][:5]:\n                        print(f\"   Position {pos}: {count} breakthroughs\")\n                    \n                    print(\"\\nðŸš€ ORCHESTRATOR OPTIMIZATION RECOMMENDATIONS:\")\n                    for i, rec in enumerate(recommendations, 1):\n                        print(f\"   {i}. {rec['type'].upper()}: {rec['parameter']}\")\n                        print(f\"      Target: {rec['value']}\")\n                        print(f\"      Reason: {rec['reason']}\")\n                    \n                    print(f\"\\n{'='*80}\")\n                    print(\"âœ… BREAKTHROUGH INTELLIGENCE ANALYSIS COMPLETE\")\n                    print(f\"{'='*80}\")\n                    \n            except Exception as e:\n                print(f\"âŒ Breakthrough analysis failed: {e}\")\n        \n        else:\n            print(\"âŒ No validation periods completed successfully\")\n            \n        \n        return all_results, unified_file\n        \n    except Exception as e:\n        print(f\"âŒ Critical error in validation: {e}\")\n        traceback.print_exc()\n        return None, None\n\ndef generate_cross_period_analysis(all_results, all_analytics):\n    \"\"\"Generate cross-period analysis to identify consistent patterns.\"\"\"\n    \n    if not all_results:\n        return {}\n    \n    # Collect success rates across periods\n    period_success_rates = {}\n    total_predictions_by_period = {}\n    \n    for period, results in all_results.items():\n        hit_dist = results['hit_distribution']\n        total = sum(hit_dist.values())\n        success_4plus = sum(count for hit, count in hit_dist.items() if hit >= 4)\n        success_5plus = sum(count for hit, count in hit_dist.items() if hit >= 5)\n        \n        period_success_rates[period] = {\n            '4plus_rate': success_4plus / total if total > 0 else 0,\n            '5plus_rate': success_5plus / total if total > 0 else 0,\n            'total_predictions': total\n        }\n        total_predictions_by_period[period] = total\n    \n    # Calculate consistency metrics\n    rates_4plus = [rate['4plus_rate'] for rate in period_success_rates.values()]\n    rates_5plus = [rate['5plus_rate'] for rate in period_success_rates.values()]\n    \n    consistency_score = 1.0 - (np.std(rates_4plus) if rates_4plus else 1.0)\n    \n    return {\n        'period_success_rates': period_success_rates,\n        'consistency_score': consistency_score,\n        'average_4plus_rate': np.mean(rates_4plus) if rates_4plus else 0,\n        'average_5plus_rate': np.mean(rates_5plus) if rates_5plus else 0,\n        'total_periods_analyzed': len(all_results),\n        'recommendation': (\n            \"HIGH_CONFIDENCE\" if consistency_score > 0.7 and np.mean(rates_4plus) > 0.02 else\n            \"MODERATE_CONFIDENCE\" if consistency_score > 0.5 and np.mean(rates_4plus) > 0.01 else\n            \"LOW_CONFIDENCE\"\n        )\n    }\n\n# 13. EXPERT FUNCTION STUBS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \ndef _stub(df): \n    \"\"\"Fallback expert function\"\"\"\n    return unique6([s_int(1, 39) for _ in range(6)]), 0.5\n\n\n\n# Create simplified implementations for remaining experts\nfor expert_name in expert_registry:\n    if expert_name not in globals() or not callable(globals().get(expert_name)):\n        # Create a simple expert function\n        def make_expert_func(name):\n            def expert_func(df):\n                try:\n                    if len(df) < 5:\n                        return unique6([s_int(1, 39) for _ in range(6)]), 0.3\n                    \n                    # Simple frequency-based approach with variation\n                    recent = df.tail(min(50, len(df)))\n                    all_nums = []\n                    for _, row in recent.iterrows():\n                        all_nums.extend([row[f'Number{j}'] for j in range(1, 7)])\n                    \n                    freq = Counter(all_nums)\n                    candidates = [num for num, _ in freq.most_common(20)]\n                    \n                    # Add some randomness based on expert name hash\n                    name_hash = hash(name) % 1000\n                    candidates = candidates[name_hash % min(len(candidates), 5):] + candidates[:name_hash % min(len(candidates), 5)]\n                    \n                    prediction = []\n                    for num in candidates:\n                        if num not in prediction and len(prediction) < 6:\n                            prediction.append(num)\n                    \n                    while len(prediction) < 6:\n                        candidate = s_int(1, 39)\n                        if candidate not in prediction:\n                            prediction.append(candidate)\n                    \n                    confidence = 0.4 + (name_hash % 30) / 100  # Confidence between 0.4-0.7\n                    return unique6(prediction), confidence\n                except:\n                    return unique6([s_int(1, 39) for _ in range(6)]), 0.3\n            return expert_func\n        \n        globals()[expert_registry[expert_name]['func']] = make_expert_func(expert_name)\n\n# 14. MAIN EXECUTION  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nif __name__ == \"__main__\":\n    try:\n        print(\"ðŸš€ DHARMA_X COMPREHENSIVE VALIDATION SYSTEM\")\n        print(\"=\" * 80)\n        # â”€â”€ VERIFY EXPERT FUNCTIONS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        for ex, cfg in expert_registry.items():\n            fname = cfg['func']\n            status = 'FOUND' if globals().get(fname) and globals()[fname] != _stub else 'MISSING/STUB'\n            print(f\"{fname}: {status}\")\n\n        \n        # You can modify this path to your data file\n        data_file_path = \"/kaggle/input/draw-41to44/draw_Updated.xlsx\"  # Update this path\n        \n        # Run comprehensive validation\n        results, analytics = run_comprehensive_validation(\n            df_path=data_file_path,\n            n_sets=200,  # Predictions per iteration\n            max_it_per_period=10000  # Max iterations per validation period\n        )\n        \n        if results:\n            print(\"\\nðŸŽ‰ COMPREHENSIVE VALIDATION COMPLETED SUCCESSFULLY!\")\n            print(f\"ðŸ“Š Analyzed {len(results)} validation periods\")\n            print(\"ðŸ“„ Check output files for detailed analysis\")\n            \n        else:\n            print(\"\\nâŒ VALIDATION FAILED - Check data file and configuration\")\n            \n    except Exception as e:\n        print(f\"\\nâŒ Critical system error: {e}\")\n        traceback.print_exc()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-19T10:15:33.188252Z","iopub.execute_input":"2025-09-19T10:15:33.188528Z","iopub.status.idle":"2025-09-19T11:16:03.199241Z","shell.execute_reply.started":"2025-09-19T10:15:33.188508Z","shell.execute_reply":"2025-09-19T11:16:03.197899Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\n\ndisplay(FileLink('dharma_unified_analytics_complete.json'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-19T11:16:03.200522Z","iopub.execute_input":"2025-09-19T11:16:03.200885Z","iopub.status.idle":"2025-09-19T11:16:03.210546Z","shell.execute_reply.started":"2025-09-19T11:16:03.200854Z","shell.execute_reply":"2025-09-19T11:16:03.209686Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# dharma_all_periods_4plus_comprehensive_extractor_OPTIMIZED.py\n# OPTIMIZED VERSION - Sequential file processing with memory management\n# COMPLETE VERSION - Extract â‰¥4/6 breakthroughs from ALL periods with full micro-to-macro details\n# MODIFIED: Now processes files sequentially with batch processing for 23GB+ files\n\nimport json\nimport pandas as pd\nimport numpy as np\nimport gc  # Garbage collector for memory management\nfrom datetime import datetime\nimport os\nfrom typing import List, Dict, Any, Optional\n\ndef safe_first_numeric(series, default=0):\n    \"\"\"Return first valid numeric scalar from a Series after coercion; else default.\"\"\"\n    import pandas as pd\n    if series is None:\n        return default\n    s = pd.to_numeric(series, errors='coerce')\n    idx = s.first_valid_index()\n    return int(s.loc[idx]) if idx is not None else default\n\nclass AllPeriods4plusBreakthroughExtractor:\n    \"\"\"\n    OPTIMIZED: Complete comprehensive micro-to-macro extraction for ALL PERIODS â‰¥4/6 breakthroughs\n    Now processes multiple input files SEQUENTIALLY with memory management\n    \"\"\"\n    \n    def __init__(self, analytics_json_paths, min_hit=4, batch_size=1000):\n        if isinstance(analytics_json_paths, str):\n            analytics_json_paths = [analytics_json_paths]\n            \n        print(\"ðŸš€ OPTIMIZED DHARMA analytics loader - Sequential file processing\")\n        print(f\"ðŸ“‚ Processing {len(analytics_json_paths)} files sequentially\")\n        print(f\"ðŸ”„ Batch size: {batch_size} records per batch\")\n        \n        self.analytics_json_paths = analytics_json_paths\n        self.batch_size = batch_size\n        self.min_hit = int(min_hit)\n        self.levels = [str(h) for h in range(self.min_hit, 7)]\n        \n        # Initialize tracking variables\n        self.periods_with_hits = {}\n        self.total_files_processed = 0\n        self.total_records_found = 0\n        \n        # Get file sizes for progress tracking\n        self.file_sizes = []\n        total_size_gb = 0\n        for path in analytics_json_paths:\n            try:\n                size_bytes = os.path.getsize(path)\n                size_gb = size_bytes / (1024**3)\n                self.file_sizes.append(size_gb)\n                total_size_gb += size_gb\n                print(f\"   ðŸ“ {os.path.basename(path)}: {size_gb:.2f} GB\")\n            except Exception as e:\n                print(f\"   âš ï¸  Cannot get size for {path}: {e}\")\n                self.file_sizes.append(0)\n        \n        print(f\"ðŸ“Š Total data size: {total_size_gb:.2f} GB\")\n        print(\"ðŸ” Starting sequential analysis to find â‰¥4/6 periods...\")\n        \n        # Process files sequentially to find periods with hits\n        self._analyze_files_sequentially()\n        \n        print(f\"âœ… Sequential analysis complete!\")\n        print(f\"âœ… Found {self.total_records_found} â‰¥{self.min_hit}/6 breakthrough records across {len(self.periods_with_hits)} periods\")\n        for period, level_map in self.periods_with_hits.items():\n            by_lvl = {lvl: len(level_map.get(lvl, [])) for lvl in self.levels}\n            print(f\"   ðŸ“Š {period}: {by_lvl}\")\n    \n    def _analyze_files_sequentially(self):\n        \"\"\"Analyze files one by one to find periods with â‰¥min_hit records\"\"\"\n        \n        for file_idx, path in enumerate(self.analytics_json_paths):\n            print(f\"\\nðŸ” Analyzing file {file_idx + 1}/{len(self.analytics_json_paths)}: {os.path.basename(path)}\")\n            print(f\"ðŸ“Š File size: {self.file_sizes[file_idx]:.2f} GB\")\n            \n            try:\n                # Load single file\n                print(\"   ðŸ“¥ Loading JSON data...\")\n                with open(path, 'r') as f:\n                    file_data = json.load(f)\n                \n                # Process periods in this file\n                file_periods = file_data.get('periods', {})\n                print(f\"   ðŸ” Found {len(file_periods)} periods in file\")\n                \n                processed_periods = 0\n                for period_name, period_data in file_periods.items():\n                    processed_periods += 1\n                    if processed_periods % 100 == 0:\n                        print(f\"      ðŸ“ˆ Processed {processed_periods}/{len(file_periods)} periods\")\n                    \n                    matches = period_data.get('match_counts', {})\n                    collected = []\n                    for lvl in self.levels:\n                        collected.extend(matches.get(lvl, []))\n                    \n                    if collected:\n                        # Merge with existing periods data\n                        if period_name in self.periods_with_hits:\n                            # Merge match_counts if period already exists from previous file\n                            existing_levels = self.periods_with_hits[period_name]\n                            for lvl in self.levels:\n                                new_records = matches.get(lvl, [])\n                                if new_records:\n                                    if lvl in existing_levels:\n                                        existing_levels[lvl].extend(new_records)\n                                    else:\n                                        existing_levels[lvl] = new_records\n                        else:\n                            # Add new period\n                            self.periods_with_hits[period_name] = {lvl: matches.get(lvl, []) for lvl in self.levels}\n                        \n                        # Update record count\n                        period_records = sum(len(matches.get(lvl, [])) for lvl in self.levels)\n                        self.total_records_found += period_records\n                \n                # Clear file data from memory\n                del file_data, file_periods\n                gc.collect()  # Force garbage collection\n                \n                self.total_files_processed += 1\n                print(f\"   âœ… File {file_idx + 1} processed successfully\")\n                \n            except Exception as e:\n                print(f\"   âŒ Error processing file {path}: {e}\")\n                continue\n    \n    def extract_comprehensive_details(self):\n        \"\"\"Extract ALL micro-to-macro details using BATCH PROCESSING for memory efficiency\"\"\"\n        if not self.periods_with_hits:\n            print(f\"âŒ No â‰¥{self.min_hit}/6 records found in any period\")\n            return []\n        \n        print(f\"\\nðŸ”¬ Beginning OPTIMIZED comprehensive extraction (â‰¥{self.min_hit}/6)...\")\n        print(f\"ðŸ”„ Using batch processing with batch size: {self.batch_size}\")\n        \n        comprehensive_data = []\n        overall_record_id = 1\n        total_periods = len(self.periods_with_hits)\n        processed_periods = 0\n        \n        # Process periods in batches\n        for period_name, level_map in self.periods_with_hits.items():\n            processed_periods += 1\n            total_in_period = sum(len(level_map.get(lvl, [])) for lvl in self.levels)\n            \n            print(f\"\\nðŸ“Š Processing period {processed_periods}/{total_periods}: {period_name}\")\n            print(f\"   ðŸŽ¯ Found {total_in_period} records (â‰¥{self.min_hit}/6)\")\n            \n            # Collect all records for this period\n            period_records = []\n            for lvl in self.levels:\n                records = level_map.get(lvl, [])\n                for idx, record in enumerate(records):\n                    period_records.append({\n                        'record': record,\n                        'level': lvl,\n                        'period_idx': idx + 1\n                    })\n            \n            # Process records in batches\n            batch_count = 0\n            for i in range(0, len(period_records), self.batch_size):\n                batch = period_records[i:i + self.batch_size]\n                batch_count += 1\n                batch_size_actual = len(batch)\n                \n                print(f\"      ðŸ”„ Processing batch {batch_count} ({batch_size_actual} records)\")\n                \n                # Process each record in the batch\n                batch_data = []\n                for record_info in batch:\n                    record = record_info['record']\n                    lvl = record_info['level']\n                    period_idx = record_info['period_idx']\n                    \n                    # Extract comprehensive details (same logic as original)\n                    details = self._extract_single_record_details(\n                        record, period_name, lvl, period_idx, overall_record_id\n                    )\n                    batch_data.append(details)\n                    overall_record_id += 1\n                \n                # Add batch data to comprehensive data\n                comprehensive_data.extend(batch_data)\n                \n                # Clear batch data and force garbage collection every few batches\n                del batch_data\n                if batch_count % 5 == 0:\n                    gc.collect()\n                    print(f\"         ðŸ’¾ Memory cleanup after batch {batch_count}\")\n            \n            # Clear period records\n            del period_records\n            gc.collect()\n            \n            print(f\"   âœ… Period {period_name} completed ({total_in_period} records processed)\")\n        \n        print(f\"\\nðŸŽ‰ OPTIMIZED extraction complete! Processed {len(comprehensive_data)} records\")\n        return comprehensive_data\n    \n    def _extract_single_record_details(self, record, period_name, lvl, period_idx, overall_record_id):\n        \"\"\"Extract comprehensive details for a single record (same logic as original)\"\"\"\n        \n        # Initialize details dictionary\n        details = {}\n        \n        # LEVEL 1: BASIC BREAKTHROUGH IDENTIFICATION + PERIOD TRACKING\n        details.update({\n            'overall_record_id': overall_record_id,\n            'period_name': period_name,\n            'period_record_number': period_idx,\n            'hit_level': int(lvl),\n            'validation_period': record.get('validation_period', period_name),\n            'iteration': record.get('iteration'),\n            'hit_count': record.get('hit_count', int(lvl)),\n            'timestamp': record.get('timestamp'),\n            'collection_timestamp': record.get('collection_timestamp'),\n        })\n        \n        # LEVEL 2: PREDICTION ANALYSIS (MICRO DETAILS)\n        prediction = record.get('prediction', [])\n        target = record.get('target', [])\n        matches = record.get('matches', [])\n        misses = record.get('misses', [])\n        extras = record.get('extras', [])\n        \n        details.update({\n            # Raw predictions\n            'prediction_raw': str(prediction),\n            'target_raw': str(target),\n            'matches_raw': str(matches),\n            'misses_raw': str(misses),\n            'extras_raw': str(extras),\n            \n            # Individual numbers\n            'pred_num_1': prediction[0] if len(prediction) > 0 else None,\n            'pred_num_2': prediction[1] if len(prediction) > 1 else None,\n            'pred_num_3': prediction[2] if len(prediction) > 2 else None,\n            'pred_num_4': prediction[3] if len(prediction) > 3 else None,\n            'pred_num_5': prediction[4] if len(prediction) > 4 else None,\n            'pred_num_6': prediction[5] if len(prediction) > 5 else None,\n            \n            'target_num_1': target[0] if len(target) > 0 else None,\n            'target_num_2': target[1] if len(target) > 1 else None,\n            'target_num_3': target[2] if len(target) > 2 else None,\n            'target_num_4': target[3] if len(target) > 3 else None,\n            'target_num_5': target[4] if len(target) > 4 else None,\n            'target_num_6': target[5] if len(target) > 5 else None,\n            \n            # Prediction statistics\n            'prediction_sum': sum(prediction) if prediction else 0,\n            'prediction_avg': float(np.mean(prediction)) if prediction else 0.0,\n            'prediction_std': float(np.std(prediction)) if prediction else 0.0,\n            'prediction_min': min(prediction) if prediction else 0,\n            'prediction_max': max(prediction) if prediction else 0,\n            'prediction_range': (max(prediction) - min(prediction)) if prediction else 0,\n            'prediction_sorted': str(sorted(prediction)),\n        })\n        \n        # LEVEL 3: PARAMETER ANALYSIS (BETA-ENTROPY CONVERGENCE)\n        beta = record.get('beta', 0)\n        entropy = record.get('entropy', 0)\n        \n        details.update({\n            'beta': beta,\n            'entropy': entropy,\n            'beta_entropy_ratio': record.get('beta_entropy_ratio', (beta/entropy if entropy else 0)),\n            'beta_deviation': record.get('beta_deviation', 0),\n            'entropy_deviation': record.get('entropy_deviation', 0),\n            'optimal_convergence': record.get('optimal_convergence', 0),\n            \n            # Parameter analysis\n            'beta_squared': (beta ** 2) if beta else 0,\n            'entropy_squared': (entropy ** 2) if entropy else 0,\n            'beta_entropy_product': (beta * entropy) if beta and entropy else 0,\n            'beta_entropy_sum': (beta + entropy) if beta and entropy else 0,\n            'beta_entropy_diff': abs(beta - entropy) if beta and entropy else 0,\n            \n            # Phase and cycle context\n            'phase': record.get('phase'),\n            'cycle_position': record.get('cycle_position', 0),\n            'cycle_position_normalized': (record.get('cycle_position', 0) / 75.0),\n        })\n        \n        # LEVEL 4: EXPERT SYSTEM ANALYSIS (MACRO INTELLIGENCE)\n        expert_weights = record.get('expert_weights', {})\n        active_experts = record.get('active_experts', [])\n        top_3_experts = record.get('top_3_experts', [])\n        \n        details.update({\n            'active_experts_count': len(active_experts),\n            'active_experts_list': str(active_experts),\n            'expert_weights_raw': str(expert_weights),\n            'top_3_experts_raw': str(top_3_experts),\n            'weight_entropy': record.get('weight_entropy', 0),\n            'weight_concentration': record.get('weight_concentration', 0),\n        })\n        \n        # Individual expert weights (top experts)\n        for i in range(3):\n            details[f'top_expert_{i+1}_name'] = None\n            details[f'top_expert_{i+1}_weight'] = None\n        \n        if top_3_experts and len(top_3_experts) > 0:\n            for i, expert_data in enumerate(top_3_experts[:3]):\n                if isinstance(expert_data, (list, tuple)) and len(expert_data) >= 2:\n                    details[f'top_expert_{i+1}_name'] = expert_data[0]\n                    details[f'top_expert_{i+1}_weight'] = expert_data[1]\n        \n        # Extract specific expert categories\n        oracle_experts_list = ['E30_MarkovChain_Residuals', 'E16_Chaos_Theory', 'E9_Gap_Analysis', 'Z3_XORMixnet_Reverse']\n        breakthrough_experts_list = ['E1_FrequencyAnalysis', 'Z6_Adversarial_PhaseWatcher', 'B1_RandomForest', 'F2_Quantum_ML']\n        \n        oracle_experts_present = [expert for expert in active_experts if expert in oracle_experts_list]\n        breakthrough_experts_present = [expert for expert in active_experts if expert in breakthrough_experts_list]\n        \n        details.update({\n            'oracle_experts_present': str(oracle_experts_present),\n            'oracle_experts_count': len(oracle_experts_present),\n            'breakthrough_experts_present': str(breakthrough_experts_present),\n            'breakthrough_experts_count': len(breakthrough_experts_present),\n        })\n        \n        # LEVEL 5: PATTERN SIGNATURE ANALYSIS\n        number_gaps = record.get('number_gaps', [])\n        number_clusters = record.get('number_clusters', {})\n        number_distribution = record.get('number_distribution', {})\n        \n        details.update({\n            'number_gaps_raw': str(number_gaps),\n            'number_clusters_raw': str(number_clusters),\n            'number_distribution_raw': str(number_distribution),\n            'sum_total': record.get('sum_total', sum(prediction) if prediction else 0),\n            'sum_deviation': record.get('sum_deviation', 0),\n            \n            # Gap analysis\n            'gaps_count': len(number_gaps) if number_gaps else 0,\n            'gaps_avg': float(np.mean(number_gaps)) if number_gaps else 0.0,\n            'gaps_max': max(number_gaps) if number_gaps else 0,\n            'gaps_min': min(number_gaps) if number_gaps else 0,\n            \n            # Cluster analysis\n            'cluster_count': number_clusters.get('cluster_count', 0) if number_clusters else 0,\n            'cluster_sizes': str(number_clusters.get('cluster_sizes', [])) if number_clusters else '[]',\n            \n            # Distribution analysis\n            'dist_1_10': number_distribution.get('1-10', 0) if number_distribution else 0,\n            'dist_11_20': number_distribution.get('11-20', 0) if number_distribution else 0,\n            'dist_21_30': number_distribution.get('21-30', 0) if number_distribution else 0,\n            'dist_31_39': number_distribution.get('31-39', 0) if number_distribution else 0,\n        })\n        \n        # LEVEL 6: PERFORMANCE CONTEXT\n        expert_performances = record.get('expert_performances', {})\n        \n        details.update({\n            'expert_performances_raw': str(expert_performances),\n            'performance_weighted_score': record.get('performance_weighted_score', 0),\n            'template_raw': str(record.get('template', {})),\n        })\n        \n        # Performance statistics\n        if expert_performances:\n            perf_values = [v for v in expert_performances.values() if v is not None]\n            if perf_values:\n                details.update({\n                    'expert_perf_avg': float(np.mean(perf_values)),\n                    'expert_perf_std': float(np.std(perf_values)),\n                    'expert_perf_min': float(min(perf_values)),\n                    'expert_perf_max': float(max(perf_values)),\n                })\n            else:\n                details.update({\n                    'expert_perf_avg': 0.0,\n                    'expert_perf_std': 0.0,\n                    'expert_perf_min': 0.0,\n                    'expert_perf_max': 0.0,\n                })\n        else:\n            details.update({\n                'expert_perf_avg': 0.0,\n                'expert_perf_std': 0.0,\n                'expert_perf_min': 0.0,\n                'expert_perf_max': 0.0,\n            })\n        \n        # LEVEL 7: BREAKTHROUGH SIGNATURE FINGERPRINTING\n        details.update({\n            'signature_beta_bucket': (int(beta * 100) // 5 * 5) if beta else 0,\n            'signature_entropy_bucket': (int(entropy * 10) // 5 * 5) if entropy else 0,\n            'signature_expert_combo': str(tuple(sorted(active_experts[:5]))),\n            'signature_gap_pattern': str(tuple(sorted(number_gaps[:3]))) if number_gaps else '()',\n            'signature_sum_bucket': (details['sum_total'] // 10 * 10) if details['sum_total'] else 0,\n        })\n        \n        # Create breakthrough fingerprint\n        fingerprint_data = f\"{details['signature_beta_bucket']}_{details['signature_entropy_bucket']}_{details['signature_sum_bucket']}\"\n        details['breakthrough_fingerprint'] = str(hash(fingerprint_data))\n        \n        # LEVEL 8: CONTEXTUAL METADATA\n        non_null_count = sum(1 for v in details.values() if v is not None and v != '')\n        total_fields = len(details)\n        record_completeness_score = (non_null_count / total_fields) if total_fields > 0 else 0\n        \n        details.update({\n            'extraction_timestamp': datetime.now().isoformat(),\n            'record_completeness_score': float(record_completeness_score),\n            'data_quality_flag': 'HIGH' if record_completeness_score > 0.8 else 'MEDIUM',\n            'total_fields': total_fields,\n            'non_null_fields': non_null_count\n        })\n        \n        return details\n    \n    def export_comprehensive_analysis(self, output_path=\"ALL_PERIODS_4plus_comprehensive_analysis.csv\"):\n        \"\"\"Export comprehensive analysis for all periods (â‰¥4/6) to CSV with MEMORY MANAGEMENT\"\"\"\n        \n        print(f\"\\nðŸš€ Starting OPTIMIZED comprehensive analysis export...\")\n        \n        data = self.extract_comprehensive_details()\n        if not data:\n            print(f\"âŒ No â‰¥{self.min_hit}/6 data extracted\")\n            return None\n        \n        print(f\"ðŸ“Š Creating DataFrame from {len(data)} records...\")\n        df = pd.DataFrame(data)\n        \n        print(f\"ðŸ’¾ Saving to CSV: {output_path}\")\n        df.to_csv(output_path, index=False)\n        \n        # Clear large data structures\n        del data\n        gc.collect()\n        \n        print(f\"\\n{'='*80}\")\n        print(f\"ðŸ“Š OPTIMIZED ALL-PERIODS â‰¥{self.min_hit}/6 ANALYSIS COMPLETE\")\n        print(f\"{'='*80}\")\n        print(f\"âœ… Extracted {len(df)} â‰¥{self.min_hit}/6 breakthrough records\")\n        print(f\"ðŸ“„ Comprehensive analysis saved: {output_path}\")\n        print(f\"ðŸ“‹ Total data fields per record: {len(df.columns)}\")\n        \n        # Period-wise summary\n        if len(df) > 0:\n            period_summary = df.groupby('period_name').size()\n            print(f\"\\nðŸ” PERIOD-WISE BREAKTHROUGH SUMMARY:\")\n            for period, count in period_summary.items():\n                period_records = df[df['period_name'] == period]\n                avg_beta = period_records['beta'].mean() if 'beta' in df.columns else 0\n                avg_entropy = period_records['entropy'].mean() if 'entropy' in df.columns else 0\n                iterations = period_records['iteration'].tolist() if 'iteration' in df.columns else []\n                print(f\"   {period}: {count} records (â‰¥{self.min_hit}/6) | Iterations: {iterations} | Avg Î²={avg_beta:.4f} | Avg Îµ={avg_entropy:.4f}\")\n        \n        return output_path\n    \n    def generate_breakthrough_insights(self):\n        \"\"\"Generate comprehensive insights about â‰¥min_hit breakthroughs across all periods with MEMORY MANAGEMENT\"\"\"\n        \n        print(f\"ðŸ§  Generating breakthrough insights...\")\n        data = self.extract_comprehensive_details()\n        if not data:\n            return None\n        \n        df = pd.DataFrame(data)\n        \n        # Permanent scalar extraction (no indexer objects involved)\n        first_non_null_total_fields = safe_first_numeric(df.get('total_fields'), default=0)\n        \n        by_level = df['hit_level'].value_counts().to_dict() if 'hit_level' in df.columns else {}\n        insights = {\n            \"min_hit_threshold\": int(self.min_hit),\n            \"total_4plus_breakthroughs\": int(len(data)),\n            \"by_level_counts\": by_level,\n            \"periods_with_breakthroughs\": int(len(df['period_name'].unique())),\n            \"breakthrough_by_period\": df['period_name'].value_counts().to_dict(),\n            \"breakthrough_iterations\": df.groupby('period_name')['iteration'].apply(list).to_dict(),\n        \n            \"parameter_analysis\": {\n                \"overall_avg_beta\": float(df['beta'].mean()) if 'beta' in df.columns else 0,\n                \"overall_avg_entropy\": float(df['entropy'].mean()) if 'entropy' in df.columns else 0,\n                \"beta_range\": [float(df['beta'].min()), float(df['beta'].max())] if 'beta' in df.columns else [0, 0],\n                \"entropy_range\": [float(df['entropy'].min()), float(df['entropy'].max())] if 'entropy' in df.columns else [0, 0],\n                \"optimal_beta_entropy_combinations\": df[['period_name', 'beta', 'entropy', 'iteration', 'hit_level']].to_dict('records')\n                    if set(['period_name','beta','entropy','iteration']).issubset(df.columns) else []\n            },\n        \n            \"expert_analysis\": {\n                \"most_frequent_oracle_experts\": df['oracle_experts_present'].value_counts().head().to_dict() if 'oracle_experts_present' in df.columns else {},\n                \"most_frequent_breakthrough_experts\": df['breakthrough_experts_present'].value_counts().head().to_dict() if 'breakthrough_experts_present' in df.columns else {},\n                \"avg_active_experts_per_breakthrough\": float(df['active_experts_count'].mean()) if 'active_experts_count' in df.columns else 0\n            },\n        \n            \"pattern_signatures\": {\n                \"unique_fingerprints\": len(df['breakthrough_fingerprint'].unique()) if 'breakthrough_fingerprint' in df.columns else 0,\n                \"most_common_fingerprints\": df['breakthrough_fingerprint'].value_counts().head().to_dict() if 'breakthrough_fingerprint' in df.columns else {},\n                \"signature_consistency\": (len(df['breakthrough_fingerprint'].unique()) / len(df)) if len(df) > 0 and 'breakthrough_fingerprint' in df.columns else 0\n            },\n        \n            \"temporal_analysis\": {\n                \"breakthrough_timestamps\": df['timestamp'].tolist() if 'timestamp' in df.columns else [],\n                \"phase_distribution\": df['phase'].value_counts().to_dict() if 'phase' in df.columns else {},\n                \"cycle_position_distribution\": df['cycle_position'].value_counts().to_dict() if 'cycle_position' in df.columns else {}\n            },\n        \n            \"quality_metrics\": {\n                \"avg_data_completeness\": float(df['record_completeness_score'].mean()) if 'record_completeness_score' in df.columns else 0,\n                \"high_quality_records\": int(len(df[df['data_quality_flag'] == 'HIGH'])) if 'data_quality_flag' in df.columns else 0,\n                \"total_fields_per_record\": first_non_null_total_fields\n            }\n        }\n        \n        # Clear DataFrame from memory\n        del df, data\n        gc.collect()\n        \n        # Save insights to JSON\n        with open(\"ALL_PERIODS_4plus_insights.json\", 'w') as f:\n            json.dump(insights, f, indent=2, default=str)\n        \n        print(f\"ðŸ’¡ Insights generated and saved to ALL_PERIODS_4plus_insights.json\")\n        return insights\n\ndef run_all_periods_extraction(analytics_json_paths, min_hit=4, batch_size=1000):\n    \"\"\"\n    OPTIMIZED: Main function to extract comprehensive â‰¥min_hit/6 breakthrough data from ALL periods.\n    Now uses sequential file processing and batch processing for 23GB+ files\n    \"\"\"\n    try:\n        print(f\"ðŸš€ STARTING OPTIMIZED DHARMA EXTRACTION\")\n        print(f\"âš¡ Sequential file processing with batch size: {batch_size}\")\n        \n        extractor = AllPeriods4plusBreakthroughExtractor(\n            analytics_json_paths, \n            min_hit=min_hit, \n            batch_size=batch_size\n        )\n        \n        # Extract comprehensive details\n        csv_path = extractor.export_comprehensive_analysis(\"ALL_PERIODS_4plus_comprehensive_analysis.csv\")\n        \n        # Generate insights\n        insights = extractor.generate_breakthrough_insights()\n        \n        if csv_path:\n            print(f\"\\nðŸŽ¯ OPTIMIZED ALL-PERIODS â‰¥{min_hit}/6 EXTRACTION COMPLETE!\")\n            print(f\"ðŸ“Š Comprehensive CSV: {csv_path}\")\n            print(f\"ðŸ’¡ Insights JSON: ALL_PERIODS_4plus_insights.json\")\n            print(f\"ðŸ”¬ Ready for cross-period analysis of all â‰¥{min_hit}/6 breakthroughs!\")\n            \n            if insights:\n                print(f\"\\nðŸ“ˆ KEY CROSS-PERIOD INSIGHTS:\")\n                print(f\"   Total â‰¥{min_hit}/6 breakthroughs: {insights.get('total_4plus_breakthroughs','N/A')}\")\n                print(f\"   Periods with breakthroughs: {insights.get('periods_with_breakthroughs','N/A')}\")\n                pa = insights.get('parameter_analysis', {})\n                print(f\"   Optimal Beta range: {pa.get('beta_range','N/A')}\")\n                print(f\"   Optimal Entropy range: {pa.get('entropy_range','N/A')}\")\n                ps = insights.get('pattern_signatures', {})\n                if 'signature_consistency' in ps:\n                    print(f\"   Pattern consistency: {float(ps['signature_consistency']):.2%}\")\n        \n        return csv_path, insights\n        \n    except Exception as e:\n        print(f\"âŒ Error during extraction: {e}\")\n        import traceback\n        traceback.print_exc()\n        return None, None\n\n# USAGE EXAMPLE - OPTIMIZED VERSION\nif __name__ == \"__main__\":\n    # Replace with your actual analytics JSON file paths\n    # Can be a single file or a list of files\n    analytics_files = [\n        \"dharma_unified_analytics_complete.json\"\n        \n    ]\n    \n    # OPTIMIZED: Sequential processing with configurable batch size\n    # Adjust batch_size based on available memory (lower = less memory usage)\n    csv_path, insights = run_all_periods_extraction(\n        analytics_files, \n        min_hit=4, \n        batch_size=500  # Reduced batch size for very large files\n    )\n    \n    if csv_path:\n        print(\"\\nðŸš€ OPTIMIZED EXTRACTION SUCCESSFUL!\")\n        print(\"ðŸ“‹ Files created:\")\n        print(f\"   - {csv_path}\")\n        print(\"   - ALL_PERIODS_4plus_insights.json\")\n        print(\"\\nðŸ’¡ Next steps:\")\n        print(\"   1. Load CSV into your analysis tool\")\n        print(\"   2. Filter by period_name for specific period analysis\")\n        print(\"   3. Use insights JSON for strategic optimization\")\n        print(\"\\nâš¡ Performance improvements:\")\n        print(\"   - Sequential file processing (no 23GB memory load)\")\n        print(\"   - Batch processing for memory management\")\n        print(\"   - Automatic garbage collection\")\n        print(\"   - Progress tracking and memory cleanup\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-19T11:16:03.212112Z","iopub.execute_input":"2025-09-19T11:16:03.212417Z","iopub.status.idle":"2025-09-19T11:21:56.202689Z","shell.execute_reply.started":"2025-09-19T11:16:03.212398Z","shell.execute_reply":"2025-09-19T11:21:56.201716Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# dharma_cross_period_trend_analyzer_OPTIMIZED.py\n# OPTIMIZED VERSION - Chunked CSV processing with memory management\n# Eliminates combinatorial explosions and repeated parsing bottlenecks\n\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict, Counter\nimport ast\nimport gc\nfrom typing import Dict, List, Set, Tuple, Any\nfrom statistics import mean, median, mode, stdev\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef safe_parse_list(x):\n    \"\"\"Optimized parsing with caching\"\"\"\n    if pd.isna(x):\n        return []\n    try:\n        return ast.literal_eval(str(x))\n    except:\n        return []\n\ndef parse_tuple_signature(s):\n    \"\"\"Parse tuple signature efficiently\"\"\"\n    if pd.isna(s): \n        return None\n    try:\n        t = ast.literal_eval(str(s))\n        if isinstance(t, (list, tuple)):\n            return tuple(t)\n    except Exception:\n        return None\n    return None\n\nclass OptimizedCrossPeriodAnalyzer:\n    \"\"\"\n    OPTIMIZED: Cross-period trend analyzer with chunked processing\n    Eliminates expensive combinations and repeated parsing\n    \"\"\"\n    \n    def __init__(self, csv_file_path, chunk_size=50000):\n        print(\"ðŸš€ OPTIMIZED DHARMA_X - Chunked Cross-Period Analysis\")\n        print(\"=\"*80)\n        print(\"RWYA: Universal patterns defeat single-segment anomalies\")\n        print(\"=\"*80)\n        \n        self.csv_file_path = csv_file_path\n        self.chunk_size = chunk_size\n        \n        # Pre-analyze file\n        print(f\"\\nðŸ“‚ Analyzing file structure...\")\n        self.total_rows = sum(1 for _ in open(csv_file_path)) - 1  # Subtract header\n        self.num_chunks = (self.total_rows + chunk_size - 1) // chunk_size\n        \n        print(f\"âœ… File contains {self.total_rows:,} rows\")\n        print(f\"ðŸ”„ Will process in {self.num_chunks} chunks of {chunk_size:,} rows each\")\n        \n        # Initialize accumulators\n        self.periods = set()\n        self.beta_period_success = defaultdict(set)\n        self.entropy_period_success = defaultdict(set)\n        self.expert_combo_periods = defaultdict(set)\n        self.synergy_patterns = defaultdict(lambda: defaultdict(int))\n        self.synergy_period_coverage = defaultdict(set)\n        self.cycle_period_success = defaultdict(set)\n        \n        # Precomputed mappings\n        self.beta_bin_ranges = {}\n        self.entropy_bin_ranges = {}\n        \n        # Enhanced parameter tracking for orchestrator manipulation\n        self.detailed_parameter_tracking = {\n            'beta_values': defaultdict(lambda: defaultdict(list)),\n            'entropy_values': defaultdict(lambda: defaultdict(list)),\n            'beta_entropy_combinations': defaultdict(lambda: defaultdict(list)),\n            'parameter_expert_combinations': defaultdict(lambda: defaultdict(set)),\n            'hit_level_parameters': {4: defaultdict(list), 5: defaultdict(list), 6: defaultdict(list)},\n            'parameter_performance': defaultdict(lambda: defaultdict(list))\n        }\n        \n        # Cross-period consistency tracking\n        self.cross_period_trends = {\n            'universal_parameters': defaultdict(set),\n            'parameter_combinations': defaultdict(set),\n            'hit_level_segregation': {4: defaultdict(set), 5: defaultdict(set), 6: defaultdict(set)},\n            'orchestrator_signals': defaultdict(lambda: defaultdict(dict))\n        }\n        \n    def process_chunks_sequentially(self):\n        \"\"\"Process CSV file in chunks to avoid memory overload\"\"\"\n        print(f\"\\nðŸ”„ Processing {self.num_chunks} chunks sequentially...\")\n        \n        chunk_iterator = pd.read_csv(\n            self.csv_file_path, \n            chunksize=self.chunk_size, \n            low_memory=False\n        )\n        \n        chunk_num = 0\n        total_processed = 0\n        \n        for chunk_df in chunk_iterator:\n            chunk_num += 1\n            chunk_size_actual = len(chunk_df)\n            total_processed += chunk_size_actual\n            \n            print(f\"   ðŸ“Š Processing chunk {chunk_num}/{self.num_chunks} ({chunk_size_actual:,} rows)\")\n            \n            # Preprocess chunk\n            self._preprocess_chunk(chunk_df)\n            \n            # Process each phase on this chunk\n            self._process_phase1_chunk(chunk_df)\n            self._process_phase2_chunk(chunk_df)\n            self._process_phase3_chunk(chunk_df)\n            self._process_phase4_chunk(chunk_df)\n            # NEW: Enhanced parameter tracking\n            self._process_enhanced_parameter_tracking(chunk_df)\n            \n            # Clear chunk from memory\n            del chunk_df\n            gc.collect()\n            \n            if chunk_num % 5 == 0:\n                print(f\"      ðŸ’¾ Memory cleanup after chunk {chunk_num}\")\n        \n        print(f\"âœ… Processed {total_processed:,} total rows in chunks\")\n        \n    def _preprocess_chunk(self, chunk_df):\n        \"\"\"Preprocess chunk with optimized parsing\"\"\"\n        \n        # Collect periods\n        self.periods.update(chunk_df['period_name'].unique())\n        \n        # Precompute bins for this chunk\n        chunk_df['beta_bin'] = pd.cut(chunk_df['beta'], bins=20, labels=False)\n        chunk_df['entropy_bin'] = pd.cut(chunk_df['entropy'], bins=20, labels=False)\n        \n        # Pre-parse expert lists once per chunk (expensive operation)\n        if 'active_experts_list' in chunk_df.columns:\n            chunk_df['active_experts_parsed'] = chunk_df['active_experts_list'].apply(safe_parse_list)\n        \n        # Store bin ranges for later reference\n        for bin_id in chunk_df['beta_bin'].dropna().unique():\n            if bin_id not in self.beta_bin_ranges:\n                beta_values = chunk_df[chunk_df['beta_bin'] == bin_id]['beta']\n                self.beta_bin_ranges[bin_id] = (beta_values.min(), beta_values.max())\n        \n        for bin_id in chunk_df['entropy_bin'].dropna().unique():\n            if bin_id not in self.entropy_bin_ranges:\n                entropy_values = chunk_df[chunk_df['entropy_bin'] == bin_id]['entropy']\n                self.entropy_bin_ranges[bin_id] = (entropy_values.min(), entropy_values.max())\n    \n    def _process_phase1_chunk(self, chunk_df):\n        \"\"\"Phase 1: Parameter consistency - vectorized processing\"\"\"\n        \n        # Group by period and collect successful bins\n        for period, period_data in chunk_df.groupby('period_name'):\n            # Beta bins present in this period\n            successful_betas = period_data['beta_bin'].dropna().unique()\n            for beta_bin in successful_betas:\n                self.beta_period_success[beta_bin].add(period)\n            \n            # Entropy bins present in this period\n            successful_entropies = period_data['entropy_bin'].dropna().unique()\n            for entropy_bin in successful_entropies:\n                self.entropy_period_success[entropy_bin].add(period)\n    \n    def _process_phase2_chunk(self, chunk_df):\n        \"\"\"Phase 2: Expert combinations - optimized without expensive combinations()\"\"\"\n        \n        # Use signature-based approach instead of combinations\n        if 'signature_expert_combo' in chunk_df.columns:\n            # Use pre-computed signature combinations (much faster)\n            sig_data = chunk_df[['period_name', 'signature_expert_combo']].dropna()\n            \n            for _, row in sig_data.iterrows():\n                period = row['period_name']\n                combo_sig = parse_tuple_signature(row['signature_expert_combo'])\n                \n                if combo_sig:\n                    self.expert_combo_periods[combo_sig].add(period)\n        \n        else:\n            # Fallback: use top-N experts approach (avoid combinations explosion)\n            if 'active_experts_parsed' in chunk_df.columns:\n                expert_data = chunk_df[['period_name', 'active_experts_parsed']].dropna()\n                \n                for _, row in expert_data.iterrows():\n                    period = row['period_name']\n                    experts = row['active_experts_parsed']\n                    \n                    if len(experts) >= 3:\n                        # Use top 3-5 experts as signature (avoid combinations)\n                        for size in [3, 4, 5]:\n                            if len(experts) >= size:\n                                top_experts = tuple(sorted(experts)[:size])\n                                self.expert_combo_periods[top_experts].add(period)\n    \n    def _process_phase3_chunk(self, chunk_df):\n        \"\"\"Phase 3: Parameter-Expert synergy - optimized processing\"\"\"\n        \n        # Filter valid rows first\n        valid_mask = (\n            ~chunk_df['beta_bin'].isna() & \n            ~chunk_df['entropy_bin'].isna()\n        )\n        \n        if 'active_experts_parsed' in chunk_df.columns:\n            valid_mask = valid_mask & (chunk_df['active_experts_parsed'].apply(len) >= 3)\n        \n        valid_data = chunk_df[valid_mask]\n        \n        for _, row in valid_data.iterrows():\n            period = row['period_name']\n            beta_bin = row['beta_bin'] \n            entropy_bin = row['entropy_bin']\n            \n            # Create synergy signature using top 3 experts\n            if 'active_experts_parsed' in row and len(row['active_experts_parsed']) >= 3:\n                top_3_experts = tuple(sorted(row['active_experts_parsed'])[:3])\n                synergy_key = (beta_bin, entropy_bin, top_3_experts)\n                \n                self.synergy_patterns[synergy_key][period] += 1\n                self.synergy_period_coverage[synergy_key].add(period)\n    \n    def _process_phase4_chunk(self, chunk_df):\n        \"\"\"Phase 4: Cycle position analysis - vectorized\"\"\"\n        \n        cycle_data = chunk_df[['period_name', 'cycle_position']].dropna()\n        \n        for period, period_data in cycle_data.groupby('period_name'):\n            successful_cycles = period_data['cycle_position'].unique()\n            for cycle_pos in successful_cycles:\n                self.cycle_period_success[cycle_pos].add(period)\n\n    def _process_enhanced_parameter_tracking(self, chunk_df):\n        \"\"\"Enhanced parameter tracking for orchestrator manipulation\"\"\"\n        \n        # Filter for valid hits (â‰¥4/6)\n        valid_hits = chunk_df[chunk_df['hit_level'] >= 4].copy()\n        \n        if valid_hits.empty:\n            return\n        \n        for _, row in valid_hits.iterrows():\n            period = row['period_name']\n            hit_level = int(row['hit_level'])\n            beta = row['beta']\n            entropy = row['entropy']\n            \n            # Track detailed parameter values by period and hit level\n            if not pd.isna(beta):\n                self.detailed_parameter_tracking['beta_values'][period][hit_level].append(beta)\n                self.detailed_parameter_tracking['parameter_performance'][f'beta_{beta:.4f}'][period].append(hit_level)\n                \n            if not pd.isna(entropy):\n                self.detailed_parameter_tracking['entropy_values'][period][hit_level].append(entropy)\n                self.detailed_parameter_tracking['parameter_performance'][f'entropy_{entropy:.4f}'][period].append(hit_level)\n            \n            # Track beta-entropy combinations\n            if not pd.isna(beta) and not pd.isna(entropy):\n                combo_key = f\"Î²{beta:.4f}_H{entropy:.4f}\"\n                self.detailed_parameter_tracking['beta_entropy_combinations'][period][hit_level].append(combo_key)\n                self.detailed_parameter_tracking['parameter_performance'][combo_key][period].append(hit_level)\n            \n            # Track parameter-expert combinations\n            if 'active_experts_parsed' in row and len(row['active_experts_parsed']) >= 3:\n                top_experts = tuple(sorted(row['active_experts_parsed'])[:3])\n                param_expert_key = f\"Î²{beta:.4f}_H{entropy:.4f}_{'+'.join(top_experts)}\"\n                self.detailed_parameter_tracking['parameter_expert_combinations'][period][hit_level].add(param_expert_key)\n            \n            # Hit-level segregated tracking\n            self.detailed_parameter_tracking['hit_level_parameters'][hit_level][period].append({\n                'beta': beta,\n                'entropy': entropy,\n                'beta_bin': row.get('beta_bin'),\n                'entropy_bin': row.get('entropy_bin'),\n                'cycle_position': row.get('cycle_position'),\n                'active_experts': row.get('active_experts_parsed', [])\n            })\n\n    def _analyze_cross_period_consistency(self):\n        \"\"\"Analyze parameter consistency across all periods for orchestrator manipulation\"\"\"\n        \n        periods_list = sorted(list(self.periods))\n        n_periods = len(periods_list)\n        \n        print(f\"\\nðŸ”¬ Analyzing cross-period parameter consistency across {n_periods} periods...\")\n        \n        # Analyze universal parameters (present in ALL periods)\n        universal_parameters = {}\n        \n        # Beta value consistency\n        all_period_betas = set()\n        for period in periods_list:\n            if period in self.detailed_parameter_tracking['beta_values']:\n                period_betas = []\n                for hit_level, beta_list in self.detailed_parameter_tracking['beta_values'][period].items():\n                    period_betas.extend(beta_list)\n                if period_betas:\n                    # Round to 4 decimal places for consistency\n                    period_betas_rounded = [round(b, 4) for b in period_betas]\n                    all_period_betas.update(period_betas_rounded)\n        \n        # Find betas present in ALL periods\n        universal_betas = []\n        for beta_val in all_period_betas:\n            present_in_periods = 0\n            total_hits = 0\n            hit_levels = []\n            \n            for period in periods_list:\n                period_found = False\n                if period in self.detailed_parameter_tracking['beta_values']:\n                    for hit_level, beta_list in self.detailed_parameter_tracking['beta_values'][period].items():\n                        if any(abs(b - beta_val) < 0.0001 for b in beta_list):\n                            present_in_periods += 1\n                            total_hits += len([b for b in beta_list if abs(b - beta_val) < 0.0001])\n                            hit_levels.extend([hit_level] * len([b for b in beta_list if abs(b - beta_val) < 0.0001]))\n                            period_found = True\n                            break\n            \n            if present_in_periods >= n_periods * 0.8:  # Present in 80%+ periods\n                universal_betas.append({\n                    'value': beta_val,\n                    'period_coverage': f\"{present_in_periods}/{n_periods}\",\n                    'total_occurrences': total_hits,\n                    'avg_hit_level': mean(hit_levels) if hit_levels else 0,\n                    'hit_levels': list(set(hit_levels))\n                })\n        \n        # Same analysis for entropy\n        all_period_entropies = set()\n        for period in periods_list:\n            if period in self.detailed_parameter_tracking['entropy_values']:\n                period_entropies = []\n                for hit_level, entropy_list in self.detailed_parameter_tracking['entropy_values'][period].items():\n                    period_entropies.extend(entropy_list)\n                if period_entropies:\n                    period_entropies_rounded = [round(e, 4) for e in period_entropies]\n                    all_period_entropies.update(period_entropies_rounded)\n        \n        universal_entropies = []\n        for entropy_val in all_period_entropies:\n            present_in_periods = 0\n            total_hits = 0\n            hit_levels = []\n            \n            for period in periods_list:\n                if period in self.detailed_parameter_tracking['entropy_values']:\n                    for hit_level, entropy_list in self.detailed_parameter_tracking['entropy_values'][period].items():\n                        if any(abs(e - entropy_val) < 0.0001 for e in entropy_list):\n                            present_in_periods += 1\n                            total_hits += len([e for e in entropy_list if abs(e - entropy_val) < 0.0001])\n                            hit_levels.extend([hit_level] * len([e for e in entropy_list if abs(e - entropy_val) < 0.0001]))\n                            break\n            \n            if present_in_periods >= n_periods * 0.8:\n                universal_entropies.append({\n                    'value': entropy_val,\n                    'period_coverage': f\"{present_in_periods}/{n_periods}\",\n                    'total_occurrences': total_hits,\n                    'avg_hit_level': mean(hit_levels) if hit_levels else 0,\n                    'hit_levels': list(set(hit_levels))\n                })\n        \n        return {\n            'universal_betas': sorted(universal_betas, key=lambda x: x['avg_hit_level'], reverse=True),\n            'universal_entropies': sorted(universal_entropies, key=lambda x: x['avg_hit_level'], reverse=True),\n            'n_periods': n_periods\n        }\n\n    def _analyze_hit_level_segregation(self):\n        \"\"\"Analyze parameters segregated by hit levels (4/6, 5/6, 6/6)\"\"\"\n        \n        hit_level_analysis = {}\n        periods_list = sorted(list(self.periods))\n        \n        for hit_level in [4, 5, 6]:\n            print(f\"\\nðŸ“Š Analyzing {hit_level}/6 specific parameters...\")\n            \n            # Collect all parameters for this hit level across all periods\n            level_betas = []\n            level_entropies = []\n            level_combinations = []\n            period_coverage = defaultdict(int)\n            \n            for period in periods_list:\n                if period in self.detailed_parameter_tracking['hit_level_parameters'][hit_level]:\n                    period_data = self.detailed_parameter_tracking['hit_level_parameters'][hit_level][period]\n                    \n                    for record in period_data:\n                        if not pd.isna(record['beta']):\n                            level_betas.append(record['beta'])\n                            period_coverage[f\"beta_{record['beta']:.4f}\"] += 1\n                        \n                        if not pd.isna(record['entropy']):\n                            level_entropies.append(record['entropy'])\n                            period_coverage[f\"entropy_{record['entropy']:.4f}\"] += 1\n                        \n                        if not pd.isna(record['beta']) and not pd.isna(record['entropy']):\n                            combo = f\"Î²{record['beta']:.4f}_H{record['entropy']:.4f}\"\n                            level_combinations.append(combo)\n                            period_coverage[combo] += 1\n            \n            # Statistical analysis\n            analysis = {\n                'hit_level': hit_level,\n                'total_records': len(level_betas),\n                'beta_stats': {\n                    'mean': mean(level_betas) if level_betas else 0,\n                    'median': median(level_betas) if level_betas else 0,\n                    'std': stdev(level_betas) if len(level_betas) > 1 else 0,\n                    'min': min(level_betas) if level_betas else 0,\n                    'max': max(level_betas) if level_betas else 0,\n                    'most_common': max(set(level_betas), key=level_betas.count) if level_betas else 0\n                },\n                'entropy_stats': {\n                    'mean': mean(level_entropies) if level_entropies else 0,\n                    'median': median(level_entropies) if level_entropies else 0,\n                    'std': stdev(level_entropies) if len(level_entropies) > 1 else 0,\n                    'min': min(level_entropies) if level_entropies else 0,\n                    'max': max(level_entropies) if level_entropies else 0,\n                    'most_common': max(set(level_entropies), key=level_entropies.count) if level_entropies else 0\n                },\n                'top_combinations': sorted(\n                    [(combo, count) for combo, count in Counter(level_combinations).most_common(5)],\n                    key=lambda x: x[1], reverse=True\n                ),\n                'period_consistency': {\n                    param: count for param, count in period_coverage.items() \n                    if count >= len(periods_list) * 0.5  # Present in 50%+ periods\n                }\n            }\n            \n            hit_level_analysis[hit_level] = analysis\n        \n        return hit_level_analysis\n    \n    def generate_analysis_report(self):\n        \"\"\"Generate comprehensive analysis report from accumulated data\"\"\"\n        \n        periods = sorted(list(self.periods))\n        n_periods = len(periods)\n        \n        print(f\"ðŸ“… Periods: {periods}\")\n        \n        # ================================================================\n        # PHASE 1: PARAMETER RANGE CONSISTENCY ACROSS PERIODS\n        # ================================================================\n        print(\"\\n\" + \"=\"*60)\n        print(\"ðŸ“Š PHASE 1: CROSS-PERIOD PARAMETER CONSISTENCY\")\n        print(\"=\"*60)\n        \n        # Universal beta ranges\n        universal_beta_bins = []\n        beta_thresh = int(np.ceil(0.70 * n_periods))\n        \n        print(f\"\\nðŸŽ¯ Universal beta bins (â‰¥{beta_thresh}/{n_periods} periods):\")\n        for beta_bin, periods_set in self.beta_period_success.items():\n            if len(periods_set) >= beta_thresh:\n                beta_min, beta_max = self.beta_bin_ranges.get(beta_bin, (0, 0))\n                print(f\"  Beta range [{beta_min:.4f}, {beta_max:.4f}] â†’ {len(periods_set)}/{n_periods} periods\")\n                \n                universal_beta_bins.append({\n                    'bin': beta_bin,\n                    'periods': len(periods_set),\n                    'beta_min': beta_min,\n                    'beta_max': beta_max\n                })\n        \n        # Universal entropy ranges  \n        universal_entropy_bins = []\n        entropy_thresh = int(np.ceil(0.70 * n_periods))\n        \n        print(f\"\\nðŸŽ¯ Universal entropy bins (â‰¥{entropy_thresh}/{n_periods} periods):\")\n        for entropy_bin, periods_set in self.entropy_period_success.items():\n            if len(periods_set) >= entropy_thresh:\n                entropy_min, entropy_max = self.entropy_bin_ranges.get(entropy_bin, (0, 0))\n                print(f\"  Entropy range [{entropy_min:.4f}, {entropy_max:.4f}] â†’ {len(periods_set)}/{n_periods} periods\")\n                \n                universal_entropy_bins.append({\n                    'bin': entropy_bin,\n                    'periods': len(periods_set),\n                    'entropy_min': entropy_min,\n                    'entropy_max': entropy_max\n                })\n        \n        # ================================================================\n        # PHASE 2: EXPERT COMBINATION CROSS-PERIOD CONSISTENCY\n        # ================================================================\n        print(\"\\n\" + \"=\"*60)\n        print(\"ðŸ”¥ PHASE 2: EXPERT COMBINATION CROSS-PERIOD ANALYSIS (accelerated)\")\n        print(\"=\"*60)\n        \n        universal_expert_combos = []\n        combo_thresh = int(np.ceil(0.50 * n_periods))\n        \n        print(f\"\\nðŸ† CROSS-PERIOD EXPERT COMBINATION ANALYSIS:\")\n        print(f\"\\n   UNIVERSAL EXPERT COMBINATIONS (present in â‰¥{combo_thresh}/{n_periods} periods):\")\n        \n        shown = 0\n        for combo, periods_set in sorted(self.expert_combo_periods.items(), \n                                       key=lambda x: len(x[1]), reverse=True):\n            if len(periods_set) >= combo_thresh:\n                combo_str = \", \".join(list(combo)[:4]) + (\"...\" if len(combo) > 4 else \"\")\n                print(f\"     [{combo_str}] â†’ {len(periods_set)}/{n_periods} periods\")\n                \n                universal_expert_combos.append((combo, len(periods_set)))\n                shown += 1\n                if shown >= 10:\n                    break\n        \n        # ================================================================\n        # PHASE 3: PARAMETER + EXPERT COMBINATION SYNERGY ANALYSIS  \n        # ================================================================\n        print(\"\\n\" + \"=\"*60)\n        print(\"âš¡ PHASE 3: PARAMETERâ€“EXPERT SYNERGY (multi-period)\")\n        print(\"=\"*60)\n        \n        synergy_thresh = int(np.ceil(0.40 * n_periods))\n        universal_synergies = []\n        \n        print(\"\\nðŸ’¡ UNIVERSAL SYNERGIES (Î²-bin + H-bin + signature_expert_combo):\")\n        \n        for synergy_key, period_counts in self.synergy_patterns.items():\n            periods_with_pattern = len(self.synergy_period_coverage[synergy_key])\n            \n            if periods_with_pattern >= synergy_thresh:\n                beta_bin, entropy_bin, expert_combo = synergy_key\n                \n                # Get actual parameter ranges\n                beta_min, beta_max = self.beta_bin_ranges.get(beta_bin, (0, 0))\n                entropy_min, entropy_max = self.entropy_bin_ranges.get(entropy_bin, (0, 0))\n                \n                total_records = sum(period_counts.values())\n                \n                combo_str = \", \".join(list(expert_combo)[:4]) + (\"...\" if len(expert_combo) > 4 else \"\")\n                print(f\"   Î²[{beta_min:.4f},{beta_max:.4f}] + H[{entropy_min:.4f},{entropy_max:.4f}] + [{combo_str}] â†’ {periods_with_pattern}/{n_periods} periods, {total_records} rows\")\n                \n                universal_synergies.append({\n                    'beta_range': f\"[{beta_min:.4f},{beta_max:.4f}]\",\n                    'entropy_range': f\"[{entropy_min:.4f},{entropy_max:.4f}]\",\n                    'expert_combo': expert_combo,\n                    'periods': periods_with_pattern,\n                    'total_records': total_records\n                })\n                \n                if len(universal_synergies) >= 8:\n                    break\n        \n        # ================================================================\n        # PHASE 4: TEMPORAL CYCLE UNIVERSALS\n        # ================================================================\n        print(\"\\n\" + \"=\"*60)\n        print(\"â° PHASE 4: UNIVERSAL CYCLE POSITIONS\")\n        print(\"=\"*60)\n        \n        cycle_thresh = int(np.ceil(0.60 * n_periods))\n        universal_cycles = []\n        \n        print(f\"\\nðŸ”„ Cycle positions present in â‰¥{cycle_thresh}/{n_periods} periods:\")\n        \n        shown = 0\n        for cycle_pos, periods_set in sorted(self.cycle_period_success.items(),\n                                           key=lambda x: len(x[1]), reverse=True):\n            if len(periods_set) >= cycle_thresh:\n                print(f\"   Position {int(cycle_pos):2d} â†’ {len(periods_set)}/{n_periods} periods\")\n                \n                universal_cycles.append({\n                    'cycle_position': int(cycle_pos),\n                    'periods': len(periods_set)\n                })\n                \n                shown += 1\n                if shown >= 10:\n                    break\n\n        # ================================================================\n        # PHASE 5A: UNIVERSAL PARAMETER TRENDS FOR ORCHESTRATOR\n        # ================================================================\n        print(\"\\n\" + \"=\"*60)\n        print(\"ðŸŽ¯ PHASE 5A: UNIVERSAL PARAMETER TRENDS FOR ORCHESTRATOR\")\n        print(\"=\"*60)\n        \n        universal_analysis = self._analyze_cross_period_consistency()\n        \n        print(f\"\\nðŸ”¥ UNIVERSAL BETA VALUES (consistent across {universal_analysis['n_periods']} periods):\")\n        for i, beta_info in enumerate(universal_analysis['universal_betas'][:5], 1):\n            print(f\"   {i}. Î² = {beta_info['value']:.4f}\")\n            print(f\"      Coverage: {beta_info['period_coverage']} periods\")\n            print(f\"      Performance: {beta_info['avg_hit_level']:.2f}/6 average\")\n            print(f\"      Hit levels achieved: {beta_info['hit_levels']}\")\n            print(f\"      Total occurrences: {beta_info['total_occurrences']}\")\n            print()\n        \n        print(f\"\\nðŸ”¥ UNIVERSAL ENTROPY VALUES (consistent across {universal_analysis['n_periods']} periods):\")\n        for i, entropy_info in enumerate(universal_analysis['universal_entropies'][:5], 1):\n            print(f\"   {i}. H = {entropy_info['value']:.4f}\")\n            print(f\"      Coverage: {entropy_info['period_coverage']} periods\")\n            print(f\"      Performance: {entropy_info['avg_hit_level']:.2f}/6 average\")\n            print(f\"      Hit levels achieved: {entropy_info['hit_levels']}\")\n            print(f\"      Total occurrences: {entropy_info['total_occurrences']}\")\n            print()\n\n        # ================================================================\n        # PHASE 5B: HIT-LEVEL SEGREGATED PARAMETER ANALYSIS\n        # ================================================================\n        print(\"\\n\" + \"=\"*60)\n        print(\"ðŸ† PHASE 5B: HIT-LEVEL SEGREGATED PARAMETER ANALYSIS\")\n        print(\"=\"*60)\n        \n        hit_level_analysis = self._analyze_hit_level_segregation()\n        \n        for hit_level in [4, 5, 6]:\n            if hit_level in hit_level_analysis:\n                analysis = hit_level_analysis[hit_level]\n                print(f\"\\nðŸŽ¯ {hit_level}/6 SPECIFIC PARAMETER ANALYSIS:\")\n                print(f\"   Total {hit_level}/6 records: {analysis['total_records']:,}\")\n                \n                print(f\"\\n   ðŸ“Š OPTIMAL BETA FOR {hit_level}/6:\")\n                beta_stats = analysis['beta_stats']\n                print(f\"      Most successful: Î² = {beta_stats['most_common']:.4f}\")\n                print(f\"      Range: [{beta_stats['min']:.4f}, {beta_stats['max']:.4f}]\")\n                print(f\"      Average: {beta_stats['mean']:.4f} Â± {beta_stats['std']:.4f}\")\n                print(f\"      Median: {beta_stats['median']:.4f}\")\n                \n                print(f\"\\n   ðŸ“Š OPTIMAL ENTROPY FOR {hit_level}/6:\")\n                entropy_stats = analysis['entropy_stats']\n                print(f\"      Most successful: H = {entropy_stats['most_common']:.4f}\")\n                print(f\"      Range: [{entropy_stats['min']:.4f}, {entropy_stats['max']:.4f}]\")\n                print(f\"      Average: {entropy_stats['mean']:.4f} Â± {entropy_stats['std']:.4f}\")\n                print(f\"      Median: {entropy_stats['median']:.4f}\")\n                \n                print(f\"\\n   ðŸ”¥ TOP PARAMETER COMBINATIONS FOR {hit_level}/6:\")\n                for j, (combo, count) in enumerate(analysis['top_combinations'][:3], 1):\n                    print(f\"      {j}. {combo} â†’ {count} occurrences\")\n                \n                print(f\"\\n   ðŸŒ CROSS-PERIOD CONSISTENT PARAMETERS FOR {hit_level}/6:\")\n                for param, period_count in sorted(analysis['period_consistency'].items(), \n                                                key=lambda x: x[1], reverse=True)[:5]:\n                    print(f\"      {param} â†’ Present in {period_count} periods\")\n                print()\n\n        # ================================================================\n        # PHASE 5C: ORCHESTRATOR MANIPULATION RECOMMENDATIONS\n        # ================================================================\n        print(\"\\n\" + \"=\"*60)\n        print(\"ðŸš€ PHASE 5C: ORCHESTRATOR MANIPULATION RECOMMENDATIONS\")\n        print(\"=\"*60)\n        \n        print(f\"\\nðŸŽ¯ IMMEDIATE ORCHESTRATOR SETTINGS (Universal Parameters):\")\n        \n        if universal_analysis['universal_betas']:\n            top_beta = universal_analysis['universal_betas'][0]\n            print(f\"\\n   PRIMARY BETA LOCK:\")\n            print(f\"      orchestrator.set_beta({top_beta['value']:.4f})  # Verified across {top_beta['period_coverage']} periods\")\n            print(f\"      # Performance: {top_beta['avg_hit_level']:.2f}/6 average, {top_beta['total_occurrences']} total hits\")\n        \n        if universal_analysis['universal_entropies']:\n            top_entropy = universal_analysis['universal_entropies'][0]\n            print(f\"\\n   PRIMARY ENTROPY LOCK:\")\n            print(f\"      orchestrator.set_entropy({top_entropy['value']:.4f})  # Verified across {top_entropy['period_coverage']} periods\")\n            print(f\"      # Performance: {top_entropy['avg_hit_level']:.2f}/6 average, {top_entropy['total_occurrences']} total hits\")\n        \n        print(f\"\\nðŸŽ¯ HIT-LEVEL SPECIFIC ORCHESTRATOR SETTINGS:\")\n        \n        for hit_level in [4, 5, 6]:\n            if hit_level in hit_level_analysis:\n                analysis = hit_level_analysis[hit_level]\n                print(f\"\\n   FOR TARGETING {hit_level}/6 SPECIFICALLY:\")\n                print(f\"      orchestrator.set_beta({analysis['beta_stats']['most_common']:.4f})  # Most successful for {hit_level}/6\")\n                print(f\"      orchestrator.set_entropy({analysis['entropy_stats']['most_common']:.4f})  # Most successful for {hit_level}/6\")\n                \n                if analysis['top_combinations']:\n                    top_combo = analysis['top_combinations'][0][0]\n                    parts = top_combo.split('_H')\n                    if len(parts) == 2:\n                        beta_part = parts[0].replace('Î²', '')\n                        entropy_part = parts[1]\n                        print(f\"      # Combined: Î²={beta_part}, H={entropy_part} â†’ {analysis['top_combinations'][0][1]} occurrences\")\n        \n        print(f\"\\nðŸŽ¯ BALANCED ORCHESTRATOR STRATEGY:\")\n        print(f\"      â€¢ Use universal parameters for consistent â‰¥4/6 performance\")\n        print(f\"      â€¢ Switch to hit-level specific parameters for targeted improvements\")\n        print(f\"      â€¢ Monitor cross-period consistency before making permanent changes\")\n        print(f\"      â€¢ Combine with expert rotation for maximum effectiveness\")\n\n        # ================================================================\n        # PHASE 5D: BALANCED UNIVERSAL RECOMMENDATIONS\n        # ================================================================\n        print(\"\\n\" + \"=\"*60)\n        print(\"ðŸ† PHASE 5D: BALANCED UNIVERSAL RECOMMENDATIONS\")\n        print(\"=\"*60)\n        \n        # Parameter locks\n        if universal_beta_bins:\n            top_beta = max(universal_beta_bins, key=lambda x: x['periods'])\n            print(f\"\\nðŸŽ¯ Parameter locks (multi-period): Beta [{top_beta['beta_min']:.4f}, {top_beta['beta_max']:.4f}]\")\n        \n        if universal_entropy_bins:\n            top_entropy = max(universal_entropy_bins, key=lambda x: x['periods'])\n            print(f\"ðŸŽ¯ Parameter locks (multi-period): Entropy [{top_entropy['entropy_min']:.4f}, {top_entropy['entropy_max']:.4f}]\")\n        \n        # Expert strategy\n        print(\"\\nðŸŽ¯ Expert strategy:\")\n        if universal_expert_combos:\n            combo, coverage = universal_expert_combos[0]\n            combo_str = \", \".join(list(combo)[:4]) + (\"...\" if len(combo) > 4 else \"\")\n            print(f\"   Prioritize signature combos like [{combo_str}] seen in {coverage}/{n_periods} periods.\")\n        \n        # Cycle strategy\n        print(\"\\nðŸŽ¯ Cycle strategy:\")\n        if universal_cycles:\n            best_cycle = universal_cycles[0]\n            print(f\"   Rotate universal positions led by {best_cycle['cycle_position']} (coverage {best_cycle['periods']}/{n_periods} periods).\")\n        \n        print(\"\\nðŸŽ¯ Balance strategy:\")\n        print(\"   â€¢ 60% universal parameters, 25% exploration, 15% breakthrough attempts.\")\n        \n        print(\"\\n\" + \"=\"*60)\n        print(\"âœ… OPTIMIZED DHARMA_X CROSS-PERIOD TREND ANALYSIS COMPLETE\")\n        print(\"=\"*60)\n\ndef dharma_cross_period_trend_analyzer_optimized(csv_file_path, chunk_size=50000):\n    \"\"\"\n    OPTIMIZED: Cross-period trend analyzer with chunked processing\n    Eliminates memory issues and combinatorial explosions\n    \"\"\"\n    try:\n        print(\"ðŸš€ STARTING OPTIMIZED CROSS-PERIOD ANALYSIS\")\n        print(\"âš¡ Chunked processing + vectorized operations\")\n        \n        analyzer = OptimizedCrossPeriodAnalyzer(csv_file_path, chunk_size)\n        \n        # Process file in chunks\n        analyzer.process_chunks_sequentially()\n        \n        # Generate comprehensive report\n        analyzer.generate_analysis_report()\n        \n        # Clean up\n        del analyzer\n        gc.collect()\n        \n        return True\n        \n    except Exception as e:\n        print(f\"âŒ Error during optimized analysis: {e}\")\n        import traceback\n        traceback.print_exc()\n        return False\n\n# Preserve original function name for compatibility\ndef dharma_cross_period_trend_analyzer(csv_file_path):\n    \"\"\"\n    OPTIMIZED wrapper - maintains original function signature\n    \"\"\"\n    return dharma_cross_period_trend_analyzer_optimized(csv_file_path, chunk_size=50000)\n\n# USAGE EXAMPLE - OPTIMIZED VERSION\nif __name__ == \"__main__\":\n    # Your CSV file\n    csv_file = \"ALL_PERIODS_4plus_comprehensive_analysis.csv\"\n    \n    print(\"ðŸš€ INITIALIZING OPTIMIZED DHARMA_X CROSS-PERIOD TREND ANALYZER\")\n    print(\"RWYA - Universal patterns defeat single-segment anomalies\")\n    print(\"True intelligence comes from consistency across all conditions\")\n    \n    # Use optimized version with chunked processing\n    success = dharma_cross_period_trend_analyzer_optimized(\n        csv_file, \n        chunk_size=30000  # Adjust based on available memory\n    )\n    \n    if success:\n        print(\"\\nðŸš€ OPTIMIZED ANALYSIS SUCCESSFUL!\")\n        print(\"âš¡ Performance improvements:\")\n        print(\"   - Chunked CSV processing (no memory overload)\")\n        print(\"   - Eliminated expensive combinations() operations\")\n        print(\"   - Pre-computed parsing and vectorized operations\")\n        print(\"   - Progressive result accumulation\")\n        print(\"   - Same comprehensive 6-phase analysis as original!\")\n        print(\"   - ENHANCED: Universal parameter trends for orchestrator manipulation\")\n        print(\"   - ENHANCED: Hit-level segregated analysis (4/6, 5/6, 6/6)\")\n        print(\"   - ENHANCED: Direct orchestrator.set_beta() and orchestrator.set_entropy() recommendations\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-19T11:21:56.204094Z","iopub.execute_input":"2025-09-19T11:21:56.204391Z","iopub.status.idle":"2025-09-19T11:23:41.620617Z","shell.execute_reply.started":"2025-09-19T11:21:56.204371Z","shell.execute_reply":"2025-09-19T11:23:41.619845Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}